{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {
        "d8b23dca-1c7e-4beb-926c-d7e2a30635a9": {
          "type": "Synapse.DataFrame",
          "sync_state": {
            "table": {
              "rows": [
                {
                  "0": "LI-001-A",
                  "1": "SKU-100-A",
                  "2": "Blue Widget",
                  "3": "3",
                  "4": "19.99",
                  "5": {
                    "CreatedAt": "2025-08-29T10:20:30Z",
                    "UpdatedAt": "2025-08-30T09:00:00Z",
                    "Flags": {
                      "Flag": [
                        {
                          "_name": "fragile",
                          "_value": "true"
                        },
                        {
                          "_name": "gift_wrap",
                          "_value": "false"
                        }
                      ]
                    }
                  },
                  "6": {
                    "Tax": [
                      {
                        "Code": "GST",
                        "Amount": "1.999",
                        "TaxBreakup": {
                          "Break": [
                            {
                              "Type": "percentage",
                              "Rate": "10"
                            }
                          ]
                        }
                      },
                      {
                        "Code": "ENV",
                        "Amount": "0.500",
                        "TaxBreakup": {
                          "Break": [
                            {
                              "Type": "fixed",
                              "Rate": "0.5"
                            }
                          ]
                        }
                      }
                    ]
                  },
                  "7": {
                    "Attribute": [
                      {
                        "_name": "Invoice Number",
                        "_code": "INVNO"
                      },
                      {
                        "_name": "Color",
                        "_code": "CLR"
                      }
                    ]
                  },
                  "8": {
                    "Boxes": {
                      "Box": [
                        {
                          "_id": "BX-1",
                          "Weight": "2.4",
                          "Dimensions": {
                            "Length": "20",
                            "Width": "15",
                            "Height": "10"
                          },
                          "Labels": {
                            "Label": [
                              "TOP",
                              "FRAGILE"
                            ]
                          }
                        },
                        {
                          "_id": "BX-2",
                          "Weight": "1.8",
                          "Dimensions": {
                            "Length": "18",
                            "Width": "12",
                            "Height": "8"
                          }
                        }
                      ]
                    }
                  },
                  "9": {
                    "Serial": [
                      "SN-A1",
                      "SN-A2"
                    ]
                  },
                  "10": "abfss://ds-india@demodsindia.dfs.core.windows.net/sample_invoices.xml",
                  "11": "2025-09-06 14:31:48.236298"
                },
                {
                  "0": "LI-002-A",
                  "1": "SKU-200-A",
                  "2": "Green Widget",
                  "3": "1",
                  "4": "49.50",
                  "5": {
                    "CreatedAt": "2025-08-29T10:25:00Z",
                    "UpdatedAt": "2025-08-30T09:05:00Z"
                  },
                  "6": {
                    "Tax": [
                      {
                        "Code": "GST",
                        "Amount": "4.95"
                      }
                    ]
                  },
                  "7": {
                    "Attribute": [
                      {
                        "_name": "Invoice Number",
                        "_code": "INVNO"
                      },
                      {
                        "_name": "WarrantyMonths",
                        "_code": "WRN"
                      }
                    ]
                  },
                  "10": "abfss://ds-india@demodsindia.dfs.core.windows.net/sample_invoices.xml",
                  "11": "2025-09-06 14:31:48.236298"
                },
                {
                  "0": "LI-003-B",
                  "1": "SKU-300-B",
                  "2": "Red Widget",
                  "3": "5",
                  "4": "9.99",
                  "6": {
                    "Tax": [
                      {
                        "Code": "GST",
                        "Amount": "4.995"
                      }
                    ]
                  },
                  "8": {
                    "Boxes": {
                      "Box": [
                        {
                          "_id": "BX-10",
                          "Weight": "3.1",
                          "Dimensions": {
                            "Length": "25",
                            "Width": "20",
                            "Height": "12"
                          }
                        }
                      ]
                    }
                  },
                  "10": "abfss://ds-india@demodsindia.dfs.core.windows.net/sample_invoices.xml",
                  "11": "2025-09-06 14:31:48.236298"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "_id",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "ItemId",
                  "type": "string"
                },
                {
                  "key": "2",
                  "name": "Description",
                  "type": "string"
                },
                {
                  "key": "3",
                  "name": "Quantity",
                  "type": "string"
                },
                {
                  "key": "4",
                  "name": "Price",
                  "type": "string"
                },
                {
                  "key": "5",
                  "name": "Meta",
                  "type": "StructType(StructField(CreatedAt,StringType,true),StructField(UpdatedAt,StringType,true),StructField(Flags,StructType(StructField(Flag,ArrayType(StructType(StructField(_name,StringType,true),StructField(_value,StringType,true)),true),true)),true))"
                },
                {
                  "key": "6",
                  "name": "Taxes",
                  "type": "StructType(StructField(Tax,ArrayType(StructType(StructField(Code,StringType,true),StructField(Amount,StringType,true),StructField(TaxBreakup,StructType(StructField(Break,ArrayType(StructType(StructField(Type,StringType,true),StructField(Rate,StringType,true)),true),true)),true)),true),true))"
                },
                {
                  "key": "7",
                  "name": "Attributes",
                  "type": "StructType(StructField(Attribute,ArrayType(StructType(StructField(_name,StringType,true),StructField(_code,StringType,true),StructField(_,StringType,true)),true),true))"
                },
                {
                  "key": "8",
                  "name": "Packaging",
                  "type": "StructType(StructField(Boxes,StructType(StructField(Box,ArrayType(StructType(StructField(_id,StringType,true),StructField(Weight,StringType,true),StructField(Dimensions,StructType(StructField(Length,StringType,true),StructField(Width,StringType,true),StructField(Height,StringType,true)),true),StructField(Labels,StructType(StructField(Label,ArrayType(StringType,true),true)),true)),true),true)),true))"
                },
                {
                  "key": "9",
                  "name": "Serials",
                  "type": "StructType(StructField(Serial,ArrayType(StringType,true),true))"
                },
                {
                  "key": "10",
                  "name": "_ingest_file",
                  "type": "string"
                },
                {
                  "key": "11",
                  "name": "_ingest_ts",
                  "type": "timestamp"
                }
              ],
              "truncated": false
            },
            "isSummary": false,
            "language": "scala",
            "wranglerEntryContext": {
              "dataframeType": "pyspark"
            }
          },
          "persist_state": {
            "view": {
              "type": "details",
              "tableOptions": {},
              "chartOptions": {
                "chartType": "bar",
                "aggregationType": "count",
                "categoryFieldKeys": [
                  "0"
                ],
                "seriesFieldKeys": [
                  "0"
                ],
                "isStacked": false
              }
            }
          }
        },
        "251166c6-0105-4e77-84dd-ec6b5892021e": {
          "type": "Synapse.DataFrame",
          "sync_state": {
            "table": {
              "rows": [
                {
                  "0": "SKU-100-A",
                  "1": "LI-001-A",
                  "2": "GST",
                  "3": "1.999",
                  "4": [
                    {
                      "Type": "percentage",
                      "Rate": "10"
                    }
                  ],
                  "5": "abfss://ds-india@demodsindia.dfs.core.windows.net/sample_invoices.xml",
                  "6": "2025-09-06 15:04:22.052964"
                },
                {
                  "0": "SKU-100-A",
                  "1": "LI-001-A",
                  "2": "ENV",
                  "3": "0.500",
                  "4": [
                    {
                      "Type": "fixed",
                      "Rate": "0.5"
                    }
                  ],
                  "5": "abfss://ds-india@demodsindia.dfs.core.windows.net/sample_invoices.xml",
                  "6": "2025-09-06 15:04:22.052964"
                },
                {
                  "0": "SKU-200-A",
                  "1": "LI-002-A",
                  "2": "GST",
                  "3": "4.95",
                  "5": "abfss://ds-india@demodsindia.dfs.core.windows.net/sample_invoices.xml",
                  "6": "2025-09-06 15:04:22.052964"
                },
                {
                  "0": "SKU-300-B",
                  "1": "LI-003-B",
                  "2": "GST",
                  "3": "4.995",
                  "5": "abfss://ds-india@demodsindia.dfs.core.windows.net/sample_invoices.xml",
                  "6": "2025-09-06 15:04:22.052964"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "ItemId",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "_id",
                  "type": "string"
                },
                {
                  "key": "2",
                  "name": "item_taxes_Code",
                  "type": "string"
                },
                {
                  "key": "3",
                  "name": "item_taxes_Amount",
                  "type": "string"
                },
                {
                  "key": "4",
                  "name": "item_taxes_TaxBreakup__Break",
                  "type": "ArrayType(StructType(StructField(Type,StringType,true),StructField(Rate,StringType,true)),true)"
                },
                {
                  "key": "5",
                  "name": "_ingest_file",
                  "type": "string"
                },
                {
                  "key": "6",
                  "name": "_ingest_ts",
                  "type": "timestamp"
                }
              ],
              "truncated": false
            },
            "isSummary": false,
            "language": "scala",
            "wranglerEntryContext": {
              "dataframeType": "pyspark"
            }
          },
          "persist_state": {
            "view": {
              "type": "details",
              "tableOptions": {},
              "chartOptions": {
                "chartType": "bar",
                "aggregationType": "count",
                "categoryFieldKeys": [
                  "0"
                ],
                "seriesFieldKeys": [
                  "0"
                ],
                "isStacked": false
              }
            }
          }
        },
        "1d2e2246-07e0-43b7-8ff9-a13e5ebe6a93": {
          "type": "Synapse.DataFrame",
          "sync_state": {
            "table": {
              "rows": [
                {
                  "0": "INV-1001",
                  "1": "2025-08-30",
                  "2": "Acme Pty Ltd",
                  "3": "123 George St",
                  "4": "Sydney",
                  "5": "NSW",
                  "6": "2000",
                  "7": "AU",
                  "8": "LI-001-A",
                  "9": "SKU-100-A",
                  "10": "Blue Widget",
                  "11": "3",
                  "12": "19.99",
                  "13": "GST",
                  "14": "1.999",
                  "15": "2",
                  "16": "2",
                  "17": "invoices_v1",
                  "18": "2025",
                  "19": "09",
                  "20": "07",
                  "21": "2025-09-07 12:43:33.129329",
                  "22": "abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/invoices.xml"
                },
                {
                  "0": "INV-1001",
                  "1": "2025-08-30",
                  "2": "Acme Pty Ltd",
                  "3": "123 George St",
                  "4": "Sydney",
                  "5": "NSW",
                  "6": "2000",
                  "7": "AU",
                  "8": "LI-001-A",
                  "9": "SKU-100-A",
                  "10": "Blue Widget",
                  "11": "3",
                  "12": "19.99",
                  "13": "ENV",
                  "14": "0.500",
                  "15": "2",
                  "16": "2",
                  "17": "invoices_v1",
                  "18": "2025",
                  "19": "09",
                  "20": "07",
                  "21": "2025-09-07 12:43:33.129329",
                  "22": "abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/invoices.xml"
                },
                {
                  "0": "INV-1001",
                  "1": "2025-08-30",
                  "2": "Acme Pty Ltd",
                  "3": "123 George St",
                  "4": "Sydney",
                  "5": "NSW",
                  "6": "2000",
                  "7": "AU",
                  "8": "LI-002-A",
                  "9": "SKU-200-A",
                  "10": "Green Widget",
                  "11": "1",
                  "12": "49.50",
                  "13": "GST",
                  "14": "4.95",
                  "15": "-1",
                  "16": "-1",
                  "17": "invoices_v1",
                  "18": "2025",
                  "19": "09",
                  "20": "07",
                  "21": "2025-09-07 12:43:33.129329",
                  "22": "abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/invoices.xml"
                },
                {
                  "0": "INV-1002",
                  "1": "2025-08-31",
                  "2": "Globex AU",
                  "3": "77 Market Rd",
                  "4": "Melbourne",
                  "5": "VIC",
                  "6": "3000",
                  "7": "AU",
                  "8": "LI-003-B",
                  "9": "SKU-300-B",
                  "10": "Red Widget",
                  "11": "5",
                  "12": "9.99",
                  "13": "GST",
                  "14": "4.995",
                  "15": "-1",
                  "16": "1",
                  "17": "invoices_v1",
                  "18": "2025",
                  "19": "09",
                  "20": "07",
                  "21": "2025-09-07 12:43:33.129329",
                  "22": "abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/invoices.xml"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "invoice_number",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "invoice_date",
                  "type": "date"
                },
                {
                  "key": "2",
                  "name": "buyer_name",
                  "type": "string"
                },
                {
                  "key": "3",
                  "name": "addr_line1",
                  "type": "string"
                },
                {
                  "key": "4",
                  "name": "addr_city",
                  "type": "string"
                },
                {
                  "key": "5",
                  "name": "addr_state",
                  "type": "string"
                },
                {
                  "key": "6",
                  "name": "addr_postcode",
                  "type": "string"
                },
                {
                  "key": "7",
                  "name": "addr_country",
                  "type": "string"
                },
                {
                  "key": "8",
                  "name": "line_id",
                  "type": "string"
                },
                {
                  "key": "9",
                  "name": "item_id",
                  "type": "string"
                },
                {
                  "key": "10",
                  "name": "description",
                  "type": "string"
                },
                {
                  "key": "11",
                  "name": "qty",
                  "type": "string"
                },
                {
                  "key": "12",
                  "name": "price",
                  "type": "string"
                },
                {
                  "key": "13",
                  "name": "tax_code",
                  "type": "string"
                },
                {
                  "key": "14",
                  "name": "tax_amount",
                  "type": "string"
                },
                {
                  "key": "15",
                  "name": "serial_count",
                  "type": "int"
                },
                {
                  "key": "16",
                  "name": "box_count",
                  "type": "int"
                },
                {
                  "key": "17",
                  "name": "source",
                  "type": "string"
                },
                {
                  "key": "18",
                  "name": "year",
                  "type": "string"
                },
                {
                  "key": "19",
                  "name": "month",
                  "type": "string"
                },
                {
                  "key": "20",
                  "name": "day",
                  "type": "string"
                },
                {
                  "key": "21",
                  "name": "ingestion_timestamp",
                  "type": "timestamp"
                },
                {
                  "key": "22",
                  "name": "file_path",
                  "type": "string"
                }
              ],
              "truncated": false
            },
            "isSummary": false,
            "language": "scala",
            "wranglerEntryContext": {
              "dataframeType": "pyspark"
            }
          },
          "persist_state": {
            "view": {
              "type": "details",
              "tableOptions": {},
              "chartOptions": {
                "chartType": "bar",
                "aggregationType": "sum",
                "categoryFieldKeys": [
                  "0"
                ],
                "seriesFieldKeys": [
                  "15"
                ],
                "isStacked": false
              }
            }
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 2,
              "statement_ids": [
                2
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2025-09-06T14:20:57.3508052Z",
              "session_start_time": "2025-09-06T14:20:57.3522309Z",
              "execution_start_time": "2025-09-06T14:27:07.9914928Z",
              "execution_finish_time": "2025-09-06T14:27:08.3466884Z",
              "parent_msg_id": "5713a3fd-452a-4c45-9469-fc3e83e5b11b"
            },
            "text/plain": "StatementMeta(demospark, 4, 2, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 5,
              "statement_ids": [
                5
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2025-09-06T14:28:17.2737685Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-06T14:28:17.2756511Z",
              "execution_finish_time": "2025-09-06T14:28:17.5739713Z",
              "parent_msg_id": "8c2564c4-0da1-4a43-8c7a-29b4fde9542e"
            },
            "text/plain": "StatementMeta(demospark, 4, 5, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "INPUT_PATH = \"abfss://ds-india@demodsindia.dfs.core.windows.net/sample_invoices.xml\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 6,
              "statement_ids": [
                6
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2025-09-06T14:28:18.8751516Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-06T14:28:18.8771022Z",
              "execution_finish_time": "2025-09-06T14:28:19.1673806Z",
              "parent_msg_id": "7f8dccdd-d166-4fa5-9593-cff6e4979695"
            },
            "text/plain": "StatementMeta(demospark, 4, 6, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# ---- USER INPUTS ----\n",
        "ROW_TAG = \"LineItem\"   # anchor at the deep repeating node\n",
        "INPUT_PATH = \"abfss://ds-india@demodsindia.dfs.core.windows.net/sample_invoices.xml\"\n",
        "CATALOG_DB = \"demo_xml\"\n",
        "BRONZE_TABLE = \"lineitem_bronze\"\n",
        "GOLD_PREFIX  = \"norm_\"\n",
        "PARTITION_COLS = []  # e.g. [\"invoice_year\",\"invoice_month\",\"invoice_day\"]\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 7,
              "statement_ids": [
                7
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2025-09-06T14:28:20.7202307Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-06T14:28:20.722563Z",
              "execution_finish_time": "2025-09-06T14:28:21.026204Z",
              "parent_msg_id": "e12bc4b4-f1d8-4b5b-b31c-dc0e64d8d85f"
            },
            "text/plain": "StatementMeta(demospark, 4, 7, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "line_item_schema = StructType([\n",
        "    StructField(\"_id\", StringType()),              # attribute id -> _id (attributePrefix=\"_\")\n",
        "    StructField(\"ItemId\", StringType()),\n",
        "    StructField(\"Description\", StringType()),\n",
        "    StructField(\"Quantity\", StringType()),\n",
        "    StructField(\"Price\", StringType()),\n",
        "    StructField(\"Meta\", StructType([\n",
        "        StructField(\"CreatedAt\", StringType()),\n",
        "        StructField(\"UpdatedAt\", StringType()),\n",
        "        StructField(\"Flags\", StructType([\n",
        "            StructField(\"Flag\", ArrayType(StructType([\n",
        "                StructField(\"_name\", StringType()),\n",
        "                StructField(\"_value\", StringType())\n",
        "            ])))\n",
        "        ]))\n",
        "    ])),\n",
        "    StructField(\"Taxes\", StructType([\n",
        "        StructField(\"Tax\", ArrayType(StructType([\n",
        "            StructField(\"Code\", StringType()),\n",
        "            StructField(\"Amount\", StringType()),\n",
        "            StructField(\"TaxBreakup\", StructType([\n",
        "                StructField(\"Break\", ArrayType(StructType([\n",
        "                    StructField(\"Type\", StringType()),\n",
        "                    StructField(\"Rate\", StringType())\n",
        "                ])))\n",
        "            ]))\n",
        "        ])))\n",
        "    ])),\n",
        "    StructField(\"Attributes\", StructType([\n",
        "        StructField(\"Attribute\", ArrayType(StructType([\n",
        "            StructField(\"_name\", StringType()),\n",
        "            StructField(\"_code\", StringType()),\n",
        "            StructField(\"_\", StringType())   # element text captured via valueTag -> \"_\"\n",
        "        ])))\n",
        "    ])),\n",
        "    StructField(\"Packaging\", StructType([\n",
        "        StructField(\"Boxes\", StructType([\n",
        "            StructField(\"Box\", ArrayType(StructType([\n",
        "                StructField(\"_id\", StringType()),\n",
        "                StructField(\"Weight\", StringType()),\n",
        "                StructField(\"Dimensions\", StructType([\n",
        "                    StructField(\"Length\", StringType()),\n",
        "                    StructField(\"Width\", StringType()),\n",
        "                    StructField(\"Height\", StringType()),\n",
        "                ])),\n",
        "                StructField(\"Labels\", StructType([\n",
        "                    StructField(\"Label\", ArrayType(StringType()))\n",
        "                ]))\n",
        "            ])))\n",
        "        ]))\n",
        "    ])),\n",
        "    StructField(\"Serials\", StructType([\n",
        "        StructField(\"Serial\", ArrayType(StringType()))\n",
        "    ]))\n",
        "])\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 9,
              "statement_ids": [
                9
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2025-09-06T14:31:39.1267846Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-06T14:31:39.1287143Z",
              "execution_finish_time": "2025-09-06T14:32:08.7930151Z",
              "parent_msg_id": "a1febeff-e9df-41e0-b718-24b75d024362"
            },
            "text/plain": "StatementMeta(demospark, 4, 9, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n |-- _id: string (nullable = true)\n |-- ItemId: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: string (nullable = true)\n |-- Price: string (nullable = true)\n |-- Meta: struct (nullable = true)\n |    |-- CreatedAt: string (nullable = true)\n |    |-- UpdatedAt: string (nullable = true)\n |    |-- Flags: struct (nullable = true)\n |    |    |-- Flag: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- _name: string (nullable = true)\n |    |    |    |    |-- _value: string (nullable = true)\n |-- Taxes: struct (nullable = true)\n |    |-- Tax: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- Code: string (nullable = true)\n |    |    |    |-- Amount: string (nullable = true)\n |    |    |    |-- TaxBreakup: struct (nullable = true)\n |    |    |    |    |-- Break: array (nullable = true)\n |    |    |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |    |    |-- Type: string (nullable = true)\n |    |    |    |    |    |    |-- Rate: string (nullable = true)\n |-- Attributes: struct (nullable = true)\n |    |-- Attribute: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- _name: string (nullable = true)\n |    |    |    |-- _code: string (nullable = true)\n |    |    |    |-- _: string (nullable = true)\n |-- Packaging: struct (nullable = true)\n |    |-- Boxes: struct (nullable = true)\n |    |    |-- Box: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- _id: string (nullable = true)\n |    |    |    |    |-- Weight: string (nullable = true)\n |    |    |    |    |-- Dimensions: struct (nullable = true)\n |    |    |    |    |    |-- Length: string (nullable = true)\n |    |    |    |    |    |-- Width: string (nullable = true)\n |    |    |    |    |    |-- Height: string (nullable = true)\n |    |    |    |    |-- Labels: struct (nullable = true)\n |    |    |    |    |    |-- Label: array (nullable = true)\n |    |    |    |    |    |    |-- element: string (containsNull = true)\n |-- Serials: struct (nullable = true)\n |    |-- Serial: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |-- _ingest_file: string (nullable = false)\n |-- _ingest_ts: timestamp (nullable = false)\n\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "d8b23dca-1c7e-4beb-926c-d7e2a30635a9",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, d8b23dca-1c7e-4beb-926c-d7e2a30635a9)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "df_raw = (\n",
        "    spark.read\n",
        "    .format(\"xml\")\n",
        "    .option(\"rowTag\", ROW_TAG)\n",
        "    .option(\"attributePrefix\", \"_\")       # keep attributes as _attr\n",
        "    .option(\"valueTag\", \"_VALUE\")         # text node column name (must differ)\n",
        "    .option(\"treatEmptyValuesAsNulls\", \"true\")\n",
        "    .schema(line_item_schema)             # ensure schema matches names below\n",
        "    .load(INPUT_PATH)\n",
        "    .withColumn(\"_ingest_file\", F.input_file_name())\n",
        "    .withColumn(\"_ingest_ts\", F.current_timestamp())\n",
        ")\n",
        "\n",
        "df_raw.printSchema()\n",
        "display(df_raw.limit(10))                 # <- you had df, use df_raw\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 10,
              "statement_ids": [
                10
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2025-09-06T14:34:04.9594336Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-06T14:34:04.9613435Z",
              "execution_finish_time": "2025-09-06T14:34:05.2523542Z",
              "parent_msg_id": "c9d10c3b-1c76-41a8-a2a2-3ad61d57fe54"
            },
            "text/plain": "StatementMeta(demospark, 4, 10, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "def flatten_structs(df):\n",
        "    \"\"\"Recursively flattens struct columns; leaves arrays/maps for explicit handling.\"\"\"\n",
        "    while True:\n",
        "        struct_cols = [ (c, t) for c,t in df.dtypes if t.startswith(\"struct\") ]\n",
        "        if not struct_cols:\n",
        "            return df\n",
        "        select_exprs = []\n",
        "        for c in df.columns:\n",
        "            if df.schema[c].dataType.__class__ == StructType:\n",
        "                for f in df.schema[c].dataType.fields:\n",
        "                    select_exprs.append(F.col(f\"{c}.{f.name}\").alias(f\"{c}__{f.name}\"))\n",
        "            else:\n",
        "                select_exprs.append(F.col(c))\n",
        "        df = df.select(*select_exprs)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 11,
              "statement_ids": [
                11
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2025-09-06T14:34:13.5975012Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-06T14:34:13.5994229Z",
              "execution_finish_time": "2025-09-06T14:34:13.9121106Z",
              "parent_msg_id": "bd16a3f6-db99-4116-a8d5-ecfb6dabd62d"
            },
            "text/plain": "StatementMeta(demospark, 4, 11, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Keep keys to join back (LineItem scope)\n",
        "PARENT_KEYS = [\"ItemId\", \"_id\"]  # item id + lineitem attribute id\n",
        "\n",
        "# Map array paths (relative to LineItem) to output suffixes\n",
        "NORMALIZE_PLAN = {\n",
        "    \"Taxes.Tax\"                      : \"item_taxes\",\n",
        "    \"Taxes.Tax.TaxBreakup.Break\"     : \"item_tax_break\",\n",
        "    \"Attributes.Attribute\"           : \"item_attributes\",\n",
        "    \"Packaging.Boxes.Box\"            : \"item_boxes\",\n",
        "    \"Packaging.Boxes.Box.Labels.Label\": \"item_box_labels\",\n",
        "    \"Serials.Serial\"                 : \"item_serials\"\n",
        "}\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 18,
              "statement_ids": [
                18
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2025-09-06T15:05:42.0767238Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-06T15:05:42.0785739Z",
              "execution_finish_time": "2025-09-06T15:06:07.3906683Z",
              "parent_msg_id": "05ecf4be-0bf2-494a-8664-a83a2ae87114"
            },
            "text/plain": "StatementMeta(demospark, 4, 18, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"explode(Taxes)\" due to data type mismatch: Parameter 1 requires the (\"ARRAY\" or \"MAP\") type, however \"Taxes\" has the type \"STRUCT<Tax: ARRAY<STRUCT<Code: STRING, Amount: STRING, TaxBreakup: STRUCT<Break: ARRAY<STRUCT<Type: STRING, Rate: STRING>>>>>>\".;\n'Project [_id#637, ItemId#638, Description#639, Quantity#640, Price#641, Meta#642, Taxes#643, Attributes#644, Packaging#645, Serials#646, _ingest_file#647, _ingest_ts#648, generatorouter(explode(Taxes#643)) AS elem0#1542]\n+- SubqueryAlias spark_catalog.demo_xml.lineitem_bronze\n   +- Relation spark_catalog.demo_xml.lineitem_bronze[_id#637,ItemId#638,Description#639,Quantity#640,Price#641,Meta#642,Taxes#643,Attributes#644,Packaging#645,Serials#646,_ingest_file#647,_ingest_ts#648] parquet\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[37], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m     running_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(parts[:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     31\u001b[0m     alias \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melem\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 32\u001b[0m     cur \u001b[38;5;241m=\u001b[39m cur\u001b[38;5;241m.\u001b[39mwithColumn(alias, F\u001b[38;5;241m.\u001b[39mexplode_outer(F\u001b[38;5;241m.\u001b[39mcol(running_col)))\n\u001b[1;32m     33\u001b[0m     alias_chain\u001b[38;5;241m.\u001b[39mappend(alias)\n\u001b[1;32m     35\u001b[0m leaf \u001b[38;5;241m=\u001b[39m alias_chain[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:5174\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   5170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   5171\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5172\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   5173\u001b[0m     )\n\u001b[0;32m-> 5174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mwithColumn(colName, col\u001b[38;5;241m.\u001b[39m_jc), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
            "File \u001b[0;32m~/cluster-env/env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"explode(Taxes)\" due to data type mismatch: Parameter 1 requires the (\"ARRAY\" or \"MAP\") type, however \"Taxes\" has the type \"STRUCT<Tax: ARRAY<STRUCT<Code: STRING, Amount: STRING, TaxBreakup: STRUCT<Break: ARRAY<STRUCT<Type: STRING, Rate: STRING>>>>>>\".;\n'Project [_id#637, ItemId#638, Description#639, Quantity#640, Price#641, Meta#642, Taxes#643, Attributes#644, Packaging#645, Serials#646, _ingest_file#647, _ingest_ts#648, generatorouter(explode(Taxes#643)) AS elem0#1542]\n+- SubqueryAlias spark_catalog.demo_xml.lineitem_bronze\n   +- Relation spark_catalog.demo_xml.lineitem_bronze[_id#637,ItemId#638,Description#639,Quantity#640,Price#641,Meta#642,Taxes#643,Attributes#644,Packaging#645,Serials#646,_ingest_file#647,_ingest_ts#648] parquet\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "bronze = spark.table(f\"{CATALOG_DB}.{BRONZE_TABLE}\")\n",
        "\n",
        "def explode_path(df, path, alias):\n",
        "    parts = path.split(\".\")\n",
        "    cur = df\n",
        "    parent_alias = None\n",
        "    for i, p in enumerate(parts):\n",
        "        current = \".\".join(parts[:i+1])\n",
        "        a = f\"{alias}_{i}\"\n",
        "        cur = cur.withColumn(a, F.explode_outer(F.col(current)))\n",
        "        parent_alias = a\n",
        "    return cur, parent_alias\n",
        "\n",
        "# Base (scalar) table for the LineItem itself (without arrays)\n",
        "base_scalar = flatten_structs(bronze.drop(*[c for c,t in bronze.dtypes if t.startswith(\"array\") or t.startswith(\"struct<array\")]))\n",
        "base_name = f\"{CATALOG_DB}.{GOLD_PREFIX}lineitem\"\n",
        "base_scalar.write.mode(\"overwrite\").format(\"delta\").saveAsTable(base_name)\n",
        "\n",
        "# Child tables\n",
        "for path, suffix in NORMALIZE_PLAN.items():\n",
        "    df = bronze\n",
        "    parts = path.split(\".\")\n",
        "    cur = df\n",
        "    alias_chain = []\n",
        "    ref = \"ROOT\"\n",
        "\n",
        "    # Walk and explode at each array level\n",
        "    running_col = parts[0]\n",
        "    for i, p in enumerate(parts):\n",
        "        running_col = \".\".join(parts[:i+1])\n",
        "        alias = f\"elem{i}\"\n",
        "        cur = cur.withColumn(alias, F.explode_outer(F.col(running_col)))\n",
        "        alias_chain.append(alias)\n",
        "\n",
        "    leaf = alias_chain[-1]\n",
        "\n",
        "    # Select parent keys + leaf columns + lineage\n",
        "    leaf_fields = [F.col(f\"{leaf}.{f.name}\").alias(f.name) for f in cur.select(leaf).schema[0].dataType.fields] \\\n",
        "                  if hasattr(cur.select(leaf).schema[0].dataType, \"fields\") else [F.col(leaf).alias(\"value\")]\n",
        "\n",
        "    out = (\n",
        "        cur.select(*[F.col(k) for k in PARENT_KEYS], *leaf_fields, \"_ingest_file\", \"_ingest_ts\")\n",
        "    )\n",
        "    out = flatten_structs(out)\n",
        "\n",
        "    name = f\"{CATALOG_DB}.{GOLD_PREFIX}{suffix}\"\n",
        "    writer = out.write.mode(\"overwrite\").format(\"delta\")\n",
        "    if PARTITION_COLS:\n",
        "        writer = writer.partitionBy(*PARTITION_COLS)\n",
        "    writer.saveAsTable(name)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 16,
              "statement_ids": [
                16
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2025-09-06T14:49:03.3369752Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-06T14:49:03.3389223Z",
              "execution_finish_time": "2025-09-06T14:49:04.0135615Z",
              "parent_msg_id": "6146211e-abd1-4f98-bd7f-9bfb1fc7e8af"
            },
            "text/plain": "StatementMeta(demospark, 4, 16, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n|namespace|\n+---------+\n|default  |\n+---------+\n\n+------------------+\n|current_database()|\n+------------------+\n|           default|\n+------------------+\n\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "spark.sql(\"SHOW DATABASES\").show(truncate=False)\n",
        "\n",
        "spark.sql(\"SELECT current_database()\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 17,
              "statement_ids": [
                17
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2025-09-06T15:04:17.4847823Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-06T15:04:17.4866878Z",
              "execution_finish_time": "2025-09-06T15:04:53.2802334Z",
              "parent_msg_id": "a1a0c128-a21f-4b50-b9e9-3aa51e1fff3b"
            },
            "text/plain": "StatementMeta(demospark, 4, 17, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'display'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[35], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 3) Read it back\u001b[39;00m\n\u001b[1;32m     12\u001b[0m bronze \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mtable(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCATALOG_DB\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBRONZE_TABLE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m bronze\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mdisplay()\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:3127\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3094\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   3095\u001b[0m \n\u001b[1;32m   3096\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3124\u001b[0m \u001b[38;5;124;03m+---+\u001b[39;00m\n\u001b[1;32m   3125\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 3127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   3128\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[1;32m   3129\u001b[0m     )\n\u001b[1;32m   3130\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   3131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'display'"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "# Set names (adjust if you already have them)\n",
        "CATALOG_DB = \"demo_xml\"\n",
        "BRONZE_TABLE = \"lineitem_bronze\"\n",
        "\n",
        "# 1) Make sure the DB exists\n",
        "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {CATALOG_DB}\")\n",
        "\n",
        "# 2) Write bronze\n",
        "df_raw.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{CATALOG_DB}.{BRONZE_TABLE}\")\n",
        "\n",
        "# 3) Read it back\n",
        "bronze = spark.table(f\"{CATALOG_DB}.{BRONZE_TABLE}\")\n",
        "bronze.limit(5).display()\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 20,
              "statement_ids": [
                20
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2025-09-06T15:17:48.7629559Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-06T15:17:48.7648263Z",
              "execution_finish_time": "2025-09-06T15:17:49.0286602Z",
              "parent_msg_id": "1766738a-3ee7-4bfd-a05b-f7110781554d"
            },
            "text/plain": "StatementMeta(demospark, 4, 20, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n |-- _id: string (nullable = true)\n |-- ItemId: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: string (nullable = true)\n |-- Price: string (nullable = true)\n |-- Meta: struct (nullable = true)\n |    |-- CreatedAt: string (nullable = true)\n |    |-- UpdatedAt: string (nullable = true)\n |    |-- Flags: struct (nullable = true)\n |    |    |-- Flag: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- _name: string (nullable = true)\n |    |    |    |    |-- _value: string (nullable = true)\n |-- Taxes: struct (nullable = true)\n |    |-- Tax: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- Code: string (nullable = true)\n |    |    |    |-- Amount: string (nullable = true)\n |    |    |    |-- TaxBreakup: struct (nullable = true)\n |    |    |    |    |-- Break: array (nullable = true)\n |    |    |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |    |    |-- Type: string (nullable = true)\n |    |    |    |    |    |    |-- Rate: string (nullable = true)\n |-- Attributes: struct (nullable = true)\n |    |-- Attribute: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- _name: string (nullable = true)\n |    |    |    |-- _code: string (nullable = true)\n |    |    |    |-- _: string (nullable = true)\n |-- Packaging: struct (nullable = true)\n |    |-- Boxes: struct (nullable = true)\n |    |    |-- Box: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- _id: string (nullable = true)\n |    |    |    |    |-- Weight: string (nullable = true)\n |    |    |    |    |-- Dimensions: struct (nullable = true)\n |    |    |    |    |    |-- Length: string (nullable = true)\n |    |    |    |    |    |-- Width: string (nullable = true)\n |    |    |    |    |    |-- Height: string (nullable = true)\n |    |    |    |    |-- Labels: struct (nullable = true)\n |    |    |    |    |    |-- Label: array (nullable = true)\n |    |    |    |    |    |    |-- element: string (containsNull = true)\n |-- Serials: struct (nullable = true)\n |    |-- Serial: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |-- _ingest_file: string (nullable = true)\n |-- _ingest_ts: timestamp (nullable = true)\n\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dtype_at' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[41], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Example: verify specific nodes\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTaxes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTaxes.Tax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTaxes.Tax.TaxBreakup\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTaxes.Tax.TaxBreakup.Break\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(p, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m->\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype_at(bronze\u001b[38;5;241m.\u001b[39mschema, p))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dtype_at' is not defined"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "bronze.printSchema()\n",
        "\n",
        "# Example: verify specific nodes\n",
        "for p in [\"Taxes\", \"Taxes.Tax\", \"Taxes.Tax.TaxBreakup\", \"Taxes.Tax.TaxBreakup.Break\"]:\n",
        "    print(p, \"->\", dtype_at(bronze.schema, p))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 3,
              "statement_ids": [
                3
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "6",
              "normalized_state": "finished",
              "queued_time": "2025-09-07T06:28:10.8501322Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-07T06:28:10.8521842Z",
              "execution_finish_time": "2025-09-07T06:28:11.1735318Z",
              "parent_msg_id": "597beeec-0e8c-48c3-8b9b-606850cba8b2"
            },
            "text/plain": "StatementMeta(demospark, 6, 3, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from functools import reduce\n",
        "from pyspark.sql.types import StructType, ArrayType, MapType\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "def dtype_at(schema: StructType, path: str):\n",
        "    \"\"\"\n",
        "    Return the DataType at a dotted path in a (nested) schema, or None if not found.\n",
        "    \"\"\"\n",
        "    tokens = path.split(\".\")\n",
        "    cur = schema\n",
        "    for tk in tokens:\n",
        "        if isinstance(cur, StructType):\n",
        "            fld = next((f for f in cur.fields if f.name == tk), None)\n",
        "            if fld is None:\n",
        "                return None\n",
        "            cur = fld.dataType\n",
        "        elif isinstance(cur, ArrayType):\n",
        "            # step into the element type; tk should address a field inside it (if struct)\n",
        "            cur = cur.elementType\n",
        "            if isinstance(cur, StructType):\n",
        "                fld = next((f for f in cur.fields if f.name == tk), None)\n",
        "                if fld is None:\n",
        "                    return None\n",
        "                cur = fld.dataType\n",
        "            else:\n",
        "                # primitive array  tk shouldnt exist here\n",
        "                return None\n",
        "        else:\n",
        "            return None\n",
        "    return cur\n",
        "\n",
        "def explode_arrays_along(df, path: str, base_alias=\"elem\"):\n",
        "    \"\"\"\n",
        "    Walk a dotted path and explode only where the prefix is an ARRAY.\n",
        "    Supports multiple arrays along the path, e.g. A.B[] . C . D[] .\n",
        "    Returns (df_with_explodes, leaf_colref, alias_chain)\n",
        "    \"\"\"\n",
        "    parts = path.split(\".\")\n",
        "    orig_schema = df.schema\n",
        "    cur = df\n",
        "    alias_chain = []\n",
        "    last_array_i = -1\n",
        "    current_alias = None\n",
        "\n",
        "    for i in range(len(parts)):\n",
        "        prefix = \".\".join(parts[:i+1])\n",
        "        dt = dtype_at(orig_schema, prefix)\n",
        "\n",
        "        # Build the *effective* column reference for this level:\n",
        "        if current_alias is None:\n",
        "            effective = prefix         # still referencing original nested column\n",
        "        else:\n",
        "            # continue inside the struct produced by last explode\n",
        "            rest = \".\".join(parts[last_array_i+1:i+1])\n",
        "            effective = f\"{current_alias}.{rest}\" if rest else current_alias\n",
        "\n",
        "        # Only explode when this prefix is an array\n",
        "        if isinstance(dt, ArrayType):\n",
        "            current_alias = f\"{base_alias}{len(alias_chain)}\"\n",
        "            cur = cur.withColumn(current_alias, F.explode_outer(F.col(effective)))\n",
        "            alias_chain.append(current_alias)\n",
        "            last_array_i = i\n",
        "\n",
        "    # Determine the leaf column reference\n",
        "    if current_alias is None:\n",
        "        # No arrays on the path  leaf is the original (possibly struct) path\n",
        "        leaf = \".\".join(parts)\n",
        "    else:\n",
        "        # Leaf is the last alias produced by explode\n",
        "        leaf = current_alias\n",
        "\n",
        "    return cur, leaf, alias_chain\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 4,
              "statement_ids": [
                4
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "6",
              "normalized_state": "finished",
              "queued_time": "2025-09-07T06:45:30.0209693Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-07T06:45:30.0228048Z",
              "execution_finish_time": "2025-09-07T06:45:31.2571234Z",
              "parent_msg_id": "fedf6b3a-c846-4e27-9c06-9155fde15569"
            },
            "text/plain": "StatementMeta(demospark, 6, 4, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'NORMALIZE_PLAN' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StructType\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functions \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path, suffix \u001b[38;5;129;01min\u001b[39;00m NORMALIZE_PLAN\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# explode only arrays along the path\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     cur, leaf, alias_chain \u001b[38;5;241m=\u001b[39m explode_arrays_along(bronze, path, base_alias\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melem\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# determine leaf dtype\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'NORMALIZE_PLAN' is not defined"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "from pyspark.sql.types import StructType\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "for path, suffix in NORMALIZE_PLAN.items():\n",
        "    # explode only arrays along the path\n",
        "    cur, leaf, alias_chain = explode_arrays_along(bronze, path, base_alias=\"elem\")\n",
        "\n",
        "    # determine leaf dtype\n",
        "    leaf_dtype = cur.select(F.col(leaf).alias(leaf)).schema[0].dataType\n",
        "\n",
        "    # prefix child columns to avoid collisions with parent keys (e.g., _id)\n",
        "    if isinstance(leaf_dtype, StructType):\n",
        "        leaf_fields = [\n",
        "            F.col(f\"{leaf}.{f.name}\").alias(f\"{suffix}_{f.name}\") for f in leaf_dtype.fields\n",
        "        ]\n",
        "    else:\n",
        "        leaf_fields = [F.col(leaf).alias(f\"{suffix}_value\")]\n",
        "\n",
        "    # build the row\n",
        "    out = cur.select(\n",
        "        *[F.col(k) for k in PARENT_KEYS],   # keep parent keys as-is\n",
        "        *leaf_fields,\n",
        "        F.col(\"_ingest_file\"),\n",
        "        F.col(\"_ingest_ts\")\n",
        "    )\n",
        "\n",
        "    # now flatten safely; names are unique at top-level\n",
        "    out = flatten_structs(out)\n",
        "\n",
        "    name = f\"{CATALOG_DB}.{GOLD_PREFIX}{suffix}\"\n",
        "    w = out.write.format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"overwriteSchema\", \"true\")    # <- reset schema (and partitioning if changed)\n",
        "# only specify partitionBy on first creation; otherwise you can omit it if unchanged\n",
        "    if PARTITION_COLS:\n",
        "        w = w.partitionBy(*PARTITION_COLS)\n",
        "    w.saveAsTable(name)\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 2,
              "statement_ids": [
                2
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "5",
              "normalized_state": "finished",
              "queued_time": "2025-09-06T16:09:48.1668726Z",
              "session_start_time": "2025-09-06T16:09:48.1685012Z",
              "execution_start_time": "2025-09-06T16:15:13.6564953Z",
              "execution_finish_time": "2025-09-06T16:15:14.9519625Z",
              "parent_msg_id": "db5ad0cf-7f9a-45ae-8f25-6f69c57ba48d"
            },
            "text/plain": "StatementMeta(demospark, 5, 2, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'CATALOG_DB' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bronze \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mtable(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCATALOG_DB\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBRONZE_TABLE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m bronze\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[1;32m      3\u001b[0m display(bronze\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m50\u001b[39m))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CATALOG_DB' is not defined"
          ]
        }
      ],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "bronze = spark.table(f\"{CATALOG_DB}.{BRONZE_TABLE}\")\n",
        "bronze.printSchema()\n",
        "display(bronze.limit(50))          # nice grid in Synapse\n",
        "# or: bronze.show(50, truncate=False)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 28,
              "statement_ids": [
                28
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2025-09-06T15:30:30.407686Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-06T15:30:30.4095059Z",
              "execution_finish_time": "2025-09-06T15:30:33.5543372Z",
              "parent_msg_id": "a4754edb-0354-4d02-8ed7-b30cbf3f3cc0"
            },
            "text/plain": "StatementMeta(demospark, 4, 28, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n |-- ItemId: string (nullable = true)\n |-- _id: string (nullable = true)\n |-- item_taxes_Code: string (nullable = true)\n |-- item_taxes_Amount: string (nullable = true)\n |-- item_taxes_TaxBreakup__Break: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- Type: string (nullable = true)\n |    |    |-- Rate: string (nullable = true)\n |-- _ingest_file: string (nullable = true)\n |-- _ingest_ts: timestamp (nullable = true)\n\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "251166c6-0105-4e77-84dd-ec6b5892021e",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 251166c6-0105-4e77-84dd-ec6b5892021e)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# example for one child table\n",
        "tbl = f\"{CATALOG_DB}.{GOLD_PREFIX}item_taxes\"   # replace with your actual suffix\n",
        "spark.table(tbl).printSchema()\n",
        "display(spark.table(tbl).limit(50))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 24,
              "statement_ids": [
                24
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2025-09-06T15:20:06.571888Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-06T15:20:06.5737431Z",
              "execution_finish_time": "2025-09-06T15:20:06.8310366Z",
              "parent_msg_id": "7b0a6fc1-a2fe-479a-9bbf-67863b45aa75"
            },
            "text/plain": "StatementMeta(demospark, 4, 24, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n |-- _id: string (nullable = true)\n |-- ItemId: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: string (nullable = true)\n |-- Price: string (nullable = true)\n |-- Meta: struct (nullable = true)\n |    |-- CreatedAt: string (nullable = true)\n |    |-- UpdatedAt: string (nullable = true)\n |    |-- Flags: struct (nullable = true)\n |    |    |-- Flag: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- _name: string (nullable = true)\n |    |    |    |    |-- _value: string (nullable = true)\n |-- Taxes: struct (nullable = true)\n |    |-- Tax: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- Code: string (nullable = true)\n |    |    |    |-- Amount: string (nullable = true)\n |    |    |    |-- TaxBreakup: struct (nullable = true)\n |    |    |    |    |-- Break: array (nullable = true)\n |    |    |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |    |    |-- Type: string (nullable = true)\n |    |    |    |    |    |    |-- Rate: string (nullable = true)\n |-- Attributes: struct (nullable = true)\n |    |-- Attribute: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- _name: string (nullable = true)\n |    |    |    |-- _code: string (nullable = true)\n |    |    |    |-- _: string (nullable = true)\n |-- Packaging: struct (nullable = true)\n |    |-- Boxes: struct (nullable = true)\n |    |    |-- Box: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- _id: string (nullable = true)\n |    |    |    |    |-- Weight: string (nullable = true)\n |    |    |    |    |-- Dimensions: struct (nullable = true)\n |    |    |    |    |    |-- Length: string (nullable = true)\n |    |    |    |    |    |-- Width: string (nullable = true)\n |    |    |    |    |    |-- Height: string (nullable = true)\n |    |    |    |    |-- Labels: struct (nullable = true)\n |    |    |    |    |    |-- Label: array (nullable = true)\n |    |    |    |    |    |    |-- element: string (containsNull = true)\n |-- Serials: struct (nullable = true)\n |    |-- Serial: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |-- _ingest_file: string (nullable = true)\n |-- _ingest_ts: timestamp (nullable = true)\n\nTaxes -> StructType([StructField('Tax', ArrayType(StructType([StructField('Code', StringType(), True), StructField('Amount', StringType(), True), StructField('TaxBreakup', StructType([StructField('Break', ArrayType(StructType([StructField('Type', StringType(), True), StructField('Rate', StringType(), True)]), True), True)]), True)]), True), True)])\nTaxes.Tax -> ArrayType(StructType([StructField('Code', StringType(), True), StructField('Amount', StringType(), True), StructField('TaxBreakup', StructType([StructField('Break', ArrayType(StructType([StructField('Type', StringType(), True), StructField('Rate', StringType(), True)]), True), True)]), True)]), True)\nTaxes.Tax.TaxBreakup -> StructType([StructField('Break', ArrayType(StructType([StructField('Type', StringType(), True), StructField('Rate', StringType(), True)]), True), True)])\nTaxes.Tax.TaxBreakup.Break -> ArrayType(StructType([StructField('Type', StringType(), True), StructField('Rate', StringType(), True)]), True)\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "bronze.printSchema()\n",
        "\n",
        "# Example: verify specific nodes\n",
        "for p in [\"Taxes\", \"Taxes.Tax\", \"Taxes.Tax.TaxBreakup\", \"Taxes.Tax.TaxBreakup.Break\"]:\n",
        "    print(p, \"->\", dtype_at(bronze.schema, p))\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 9,
              "statement_ids": [
                9
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "6",
              "normalized_state": "finished",
              "queued_time": "2025-09-07T07:24:59.6007714Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-07T07:24:59.6028389Z",
              "execution_finish_time": "2025-09-07T07:25:00.2701813Z",
              "parent_msg_id": "4e500839-2baa-47e6-b2ab-5d4f3abb8bd3"
            },
            "text/plain": "StatementMeta(demospark, 6, 9, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to copy raw XML: \nBronze layer processing failed: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 151\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Execute in Synapse notebook\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m main()\n",
            "Cell \u001b[0;32mIn[19], line 132\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m set_spark_config()\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Copy raw XML\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m copy_raw_xml(landing_path, xml_raw_path)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Parse and flatten XML\u001b[39;00m\n\u001b[1;32m    135\u001b[0m valid_structured_df, corrupt_df \u001b[38;5;241m=\u001b[39m parse_and_flatten_xml(landing_path, xsd_path, SCHEMA)\n",
            "Cell \u001b[0;32mIn[19], line 56\u001b[0m, in \u001b[0;36mcopy_raw_xml\u001b[0;34m(landing_path, xml_raw_path)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     df_raw \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mtext(landing_path)\n\u001b[0;32m---> 56\u001b[0m     df_raw \u001b[38;5;241m=\u001b[39m df_raw\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m, lit(CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_name\u001b[39m\u001b[38;5;124m\"\u001b[39m])) \\\n\u001b[1;32m     57\u001b[0m                    \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m, lit(CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m])) \\\n\u001b[1;32m     58\u001b[0m                    \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m, lit(CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m     59\u001b[0m     df_raw\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;241m.\u001b[39mpartitionBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;241m.\u001b[39msave(xml_raw_path)\n\u001b[1;32m     62\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw XML copied to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxml_raw_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py:174\u001b[0m, in \u001b[0;36mtry_remote_functions.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/functions.py:193\u001b[0m, in \u001b[0;36mlit\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlit\u001b[39m\u001b[38;5;124m\"\u001b[39m, col)\u001b[38;5;241m.\u001b[39mastype(dt)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;28mstr\u001b[39m(col))\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlit\u001b[39m\u001b[38;5;124m\"\u001b[39m, col)\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/functions.py:95\u001b[0m, in \u001b[0;36m_invoke_function\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_invoke_function\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[1;32m     91\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m    Invokes JVM function identified by name with args\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m    and wraps the result with :class:`~pyspark.sql.Column`.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     jf \u001b[38;5;241m=\u001b[39m _get_jvm_function(name, SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Column(jf(\u001b[38;5;241m*\u001b[39margs))\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "metadata": {},
      "source": [
        "# Import required libraries\n",
        "from pyspark.sql.functions import lit, current_timestamp, input_file_name, explode, col\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, TimestampType\n",
        "from pyspark.sql.dataframe import DataFrame\n",
        "import logging\n",
        "\n",
        "# Configure logging for Synapse monitoring\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    \"container\": \"ds-india\",\n",
        "    \"storage_account\": \"demodsindia\",\n",
        "    \"source_name\": \"source_name\",  # Replace with actual source (e.g., \"hr_system\")\n",
        "    \"year\": \"2025\",\n",
        "    \"month\": \"09\",\n",
        "    \"day\": \"07\",\n",
        "    \"landing_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/sample.xml\",\n",
        "    \"xml_raw_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/bronze/history/xml_raw/source={source_name}/\",\n",
        "    \"xml_structured_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/bronze/history/xml_structured/\",\n",
        "    \"corrupt_records_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/bronze/corrupt_records/source={source_name}/\",\n",
        "    \"xsd_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/sample.xsd\",\n",
        "    \"spark_partitions\": 16,\n",
        "    \"row_tag\": \"record\"\n",
        "}\n",
        "\n",
        "# Define explicit schema for XML parsing\n",
        "SCHEMA = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"details\", StructType([\n",
        "        StructField(\"name\", StringType(), True),\n",
        "        StructField(\"department\", StringType(), True),\n",
        "        StructField(\"roles\", StructType([\n",
        "            StructField(\"role\", ArrayType(StringType()), True)\n",
        "        ]), True)\n",
        "    ]), True),\n",
        "    StructField(\"_corrupt_record\", StringType(), True)\n",
        "])\n",
        "\n",
        "def set_spark_config(partitions: int = CONFIG[\"spark_partitions\"]) -> None:\n",
        "    \"\"\"Configure Spark settings for Synapse notebook.\"\"\"\n",
        "    try:\n",
        "        spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "        spark.conf.set(\"spark.sql.shuffle.partitions\", str(partitions))\n",
        "        spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
        "        logger.info(\"Spark configurations set successfully.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to set Spark configurations: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def copy_raw_xml(landing_path: str, xml_raw_path: str) -> None:\n",
        "    \"\"\"Copy raw XML from Landing to History/xml_raw in Synapse.\"\"\"\n",
        "    try:\n",
        "        df_raw = spark.read.text(landing_path)\n",
        "        df_raw = df_raw.withColumn(\"source\", lit(CONFIG[\"source_name\"])) \\\n",
        "                       .withColumn(\"year\", lit(CONFIG[\"year\"])) \\\n",
        "                       .withColumn(\"month\", lit(CONFIG[\"month\"]))\n",
        "        df_raw.write.mode(\"append\") \\\n",
        "            .partitionBy(\"source\", \"year\", \"month\") \\\n",
        "            .save(xml_raw_path)\n",
        "        logger.info(f\"Raw XML copied to {xml_raw_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to copy raw XML: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def parse_and_flatten_xml(landing_path: str, xsd_path: str, schema: StructType) -> tuple:\n",
        "    \"\"\"Parse XML, apply XSD validation, explode/flatten, and return structured and corrupt DataFrames.\"\"\"\n",
        "    try:\n",
        "        df_xml = spark.read.format(\"com.databricks.spark.xml\") \\\n",
        "            .option(\"rowTag\", CONFIG[\"row_tag\"]) \\\n",
        "            .option(\"mode\", \"PERMISSIVE\") \\\n",
        "            .option(\"rowValidationXSDPath\", xsd_path) \\\n",
        "            .schema(schema) \\\n",
        "            .load(landing_path)\n",
        "\n",
        "        df_structured = df_xml.select(\n",
        "            col(\"id\"),\n",
        "            col(\"details.name\").alias(\"name\"),\n",
        "            col(\"details.department\").alias(\"department\"),\n",
        "            explode(col(\"details.roles.role\")).alias(\"role\"),\n",
        "            col(\"_corrupt_record\")\n",
        "        ).withColumn(\"source\", lit(CONFIG[\"source_name\"])) \\\n",
        "         .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
        "         .withColumn(\"file_path\", input_file_name())\n",
        "\n",
        "        corrupt_df = df_structured.filter(df_structured[\"_corrupt_record\"].isNotNull())\n",
        "        valid_structured_df = df_structured.filter(df_structured[\"_corrupt_record\"].isNull()) \\\n",
        "            .select(\"id\", \"name\", \"department\", \"role\", \"source\", \"ingestion_timestamp\", \"file_path\")\n",
        "\n",
        "        logger.info(\"XML parsed and flattened successfully.\")\n",
        "        return valid_structured_df, corrupt_df\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to parse and flatten XML: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def save_structured_data(df: DataFrame, output_path: str) -> None:\n",
        "    \"\"\"Save structured DataFrame as Parquet in Synapse.\"\"\"\n",
        "    try:\n",
        "        df.write.mode(\"append\") \\\n",
        "            .partitionBy(\"source\", \"year\", \"month\") \\\n",
        "            .parquet(output_path)\n",
        "        logger.info(f\"Structured data saved to {output_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to save structured data: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def save_corrupt_records(df: DataFrame, corrupt_path: str) -> None:\n",
        "    \"\"\"Save corrupt records as Parquet in Synapse.\"\"\"\n",
        "    try:\n",
        "        df.write.mode(\"overwrite\") \\\n",
        "            .parquet(corrupt_path)\n",
        "        logger.info(f\"Corrupt records saved to {corrupt_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to save corrupt records: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to process Bronze layer in Synapse notebook.\"\"\"\n",
        "    try:\n",
        "        # Format paths\n",
        "        landing_path = CONFIG[\"landing_path\"].format(**CONFIG)\n",
        "        xml_raw_path = CONFIG[\"xml_raw_path\"].format(**CONFIG)\n",
        "        xml_structured_path = CONFIG[\"xml_structured_path\"].format(**CONFIG)\n",
        "        corrupt_records_path = CONFIG[\"corrupt_records_path\"].format(**CONFIG)\n",
        "        xsd_path = CONFIG[\"xsd_path\"].format(**CONFIG)\n",
        "\n",
        "        # Configure Spark settings\n",
        "        set_spark_config()\n",
        "\n",
        "        # Copy raw XML\n",
        "        copy_raw_xml(landing_path, xml_raw_path)\n",
        "\n",
        "        # Parse and flatten XML\n",
        "        valid_structured_df, corrupt_df = parse_and_flatten_xml(landing_path, xsd_path, SCHEMA)\n",
        "\n",
        "        # Save structured data and corrupt records\n",
        "        save_structured_data(valid_structured_df, xml_structured_path)\n",
        "        save_corrupt_records(corrupt_df, corrupt_records_path)\n",
        "\n",
        "        # Display sample for verification in Synapse notebook\n",
        "        valid_structured_df.show(5, truncate=False)\n",
        "        corrupt_df.show(5, truncate=False)\n",
        "\n",
        "        logger.info(\"Bronze layer processing completed successfully.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Bronze layer processing failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute in Synapse notebook\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 4,
              "statement_ids": [
                4
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "7",
              "normalized_state": "finished",
              "queued_time": "2025-09-07T08:14:13.5525516Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-07T08:14:13.554641Z",
              "execution_finish_time": "2025-09-07T08:14:14.7736818Z",
              "parent_msg_id": "19329e9b-280c-4b99-83a7-34027f1630aa"
            },
            "text/plain": "StatementMeta(demospark, 7, 4, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `_corrupt_record` cannot be resolved. Did you mean one of the following? [`role`, `details`, `id`].;\n'Project [id#30, details#31.name AS name#34, details#31.department AS department#35, role#40, '_corrupt_record]\n+- Generate explode(details#31.roles.role), false, [role#40]\n   +- Relation [id#30,details#31] XmlRelation(com.databricks.spark.xml.DefaultSource$$Lambda$3742/0x000071b87719eea0@f523ee7,Some(abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml),Map(path -> abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml, rowvalidationxsdpath -> abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xsd, mode -> PERMISSIVE, rowtag -> record, columnnameofcorruptrecord -> _corrupt_record),StructType(StructField(id,IntegerType,true),StructField(details,StructType(StructField(name,StringType,true),StructField(department,StringType,true),StructField(roles,StructType(StructField(role,ArrayType(StringType,true),true)),true)),true)))\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 127\u001b[0m\n\u001b[1;32m    123\u001b[0m     corrupt_df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    125\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBronze layer processing completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 127\u001b[0m main()\n",
            "Cell \u001b[0;32mIn[9], line 115\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m set_spark_config()\n\u001b[1;32m    114\u001b[0m copy_raw_xml_file(landing_path, xml_raw_path)\n\u001b[0;32m--> 115\u001b[0m valid_structured_df, corrupt_df \u001b[38;5;241m=\u001b[39m parse_and_flatten_xml(landing_path, xsd_path, SCHEMA)\n\u001b[1;32m    116\u001b[0m save_structured_data(valid_structured_df, xml_structured_path)\n\u001b[1;32m    117\u001b[0m save_corrupt_records(corrupt_df, corrupt_records_path)\n",
            "Cell \u001b[0;32mIn[9], line 67\u001b[0m, in \u001b[0;36mparse_and_flatten_xml\u001b[0;34m(landing_path, xsd_path, schema)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_and_flatten_xml\u001b[39m(landing_path: \u001b[38;5;28mstr\u001b[39m, xsd_path: \u001b[38;5;28mstr\u001b[39m, schema: StructType):\n\u001b[1;32m     59\u001b[0m     df_xml \u001b[38;5;241m=\u001b[39m (spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcom.databricks.spark.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m               \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrowTag\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow_tag\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     61\u001b[0m               \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPERMISSIVE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m               \u001b[38;5;241m.\u001b[39mschema(schema)\n\u001b[1;32m     65\u001b[0m               \u001b[38;5;241m.\u001b[39mload(landing_path))\n\u001b[0;32m---> 67\u001b[0m     df_structured \u001b[38;5;241m=\u001b[39m (df_xml\u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m     68\u001b[0m             col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     69\u001b[0m             col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetails.name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     70\u001b[0m             col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetails.department\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepartment\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     71\u001b[0m             explode(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetails.roles.role\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     72\u001b[0m             col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_corrupt_record\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m         )\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m, lit(CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m, lit(CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m, lit(CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m\"\u001b[39m, lit(CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mingestion_timestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, current_timestamp())\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_file_name())\n\u001b[1;32m     80\u001b[0m     )\n\u001b[1;32m     82\u001b[0m     corrupt_df \u001b[38;5;241m=\u001b[39m df_structured\u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_corrupt_record\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misNotNull())\n\u001b[1;32m     83\u001b[0m     valid_structured_df \u001b[38;5;241m=\u001b[39m (df_structured\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_corrupt_record\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misNull())\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_corrupt_record\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:3227\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3184\u001b[0m \n\u001b[1;32m   3185\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3226\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3227\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jcols(\u001b[38;5;241m*\u001b[39mcols))\n\u001b[1;32m   3228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
            "File \u001b[0;32m~/cluster-env/env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `_corrupt_record` cannot be resolved. Did you mean one of the following? [`role`, `details`, `id`].;\n'Project [id#30, details#31.name AS name#34, details#31.department AS department#35, role#40, '_corrupt_record]\n+- Generate explode(details#31.roles.role), false, [role#40]\n   +- Relation [id#30,details#31] XmlRelation(com.databricks.spark.xml.DefaultSource$$Lambda$3742/0x000071b87719eea0@f523ee7,Some(abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml),Map(path -> abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml, rowvalidationxsdpath -> abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xsd, mode -> PERMISSIVE, rowtag -> record, columnnameofcorruptrecord -> _corrupt_record),StructType(StructField(id,IntegerType,true),StructField(details,StructType(StructField(name,StringType,true),StructField(department,StringType,true),StructField(roles,StructType(StructField(role,ArrayType(StringType,true),true)),true)),true)))\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import lit, current_timestamp, input_file_name, explode, col\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
        "from pyspark.sql.dataframe import DataFrame\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "CONFIG = {\n",
        "    \"container\": \"ds-india\",\n",
        "    \"storage_account\": \"demodsindia\",\n",
        "    \"source_name\": \"source_name\",\n",
        "    \"year\": \"2025\",\n",
        "    \"month\": \"09\",\n",
        "    \"day\": \"07\",\n",
        "    \"landing_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/sample.xml\",\n",
        "    \"xml_raw_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/bronze/history/xml_raw/source={source_name}/\",\n",
        "    \"xml_structured_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/bronze/history/xml_structured/\",\n",
        "    \"corrupt_records_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/bronze/corrupt_records/source={source_name}/\",\n",
        "    \"xsd_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/sample.xsd\",\n",
        "    \"spark_partitions\": 16,\n",
        "    \"row_tag\": \"record\"\n",
        "}\n",
        "\n",
        "SCHEMA = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"details\", StructType([\n",
        "        StructField(\"name\", StringType(), True),\n",
        "        StructField(\"department\", StringType(), True),\n",
        "        StructField(\"roles\", StructType([\n",
        "            StructField(\"role\", ArrayType(StringType()), True)\n",
        "        ]), True)\n",
        "    ]), True)\n",
        "    # NOTE: do NOT include _corrupt_record in the schema; let spark-xml add it\n",
        "])\n",
        "\n",
        "def set_spark_config(partitions: int = CONFIG[\"spark_partitions\"]) -> None:\n",
        "    spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "    spark.conf.set(\"spark.sql.shuffle.partitions\", str(partitions))\n",
        "    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")  # harmless if you keep Parquet\n",
        "    logger.info(\"Spark configurations set.\")\n",
        "\n",
        "def copy_raw_xml_file(landing_path: str, xml_raw_path: str) -> None:\n",
        "    # Byte-for-byte copy; keeps filename. Works in Synapse.\n",
        "    from notebookutils import mssparkutils\n",
        "    target_dir = xml_raw_path.format(**CONFIG)\n",
        "    mssparkutils.fs.mkdirs(target_dir)\n",
        "    # Add date partitions to the path to match your strategy\n",
        "    dated = f\"{target_dir}year={CONFIG['year']}/month={CONFIG['month']}/day={CONFIG['day']}/\"\n",
        "    mssparkutils.fs.mkdirs(dated)\n",
        "    file_name = landing_path.split(\"/\")[-1]\n",
        "    dest = dated + file_name\n",
        "    ok = mssparkutils.fs.cp(landing_path, dest, recurse=False)\n",
        "    if not ok:\n",
        "        raise RuntimeError(f\"Failed to copy {landing_path} to {dest}\")\n",
        "    logger.info(f\"Raw XML copied to {dest}\")\n",
        "\n",
        "def parse_and_flatten_xml(landing_path: str, xsd_path: str, schema: StructType):\n",
        "    df_xml = (spark.read.format(\"com.databricks.spark.xml\")\n",
        "              .option(\"rowTag\", CONFIG[\"row_tag\"])\n",
        "              .option(\"mode\", \"PERMISSIVE\")\n",
        "              .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
        "              .option(\"rowValidationXSDPath\", xsd_path)\n",
        "              .schema(schema)\n",
        "              .load(landing_path))\n",
        "\n",
        "    df_structured = (df_xml.select(\n",
        "            col(\"id\"),\n",
        "            col(\"details.name\").alias(\"name\"),\n",
        "            col(\"details.department\").alias(\"department\"),\n",
        "            explode(col(\"details.roles.role\")).alias(\"role\"),\n",
        "            col(\"_corrupt_record\")\n",
        "        )\n",
        "        .withColumn(\"source\", lit(CONFIG[\"source_name\"]))\n",
        "        .withColumn(\"year\", lit(CONFIG[\"year\"]))\n",
        "        .withColumn(\"month\", lit(CONFIG[\"month\"]))\n",
        "        .withColumn(\"day\", lit(CONFIG[\"day\"]))\n",
        "        .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
        "        .withColumn(\"file_path\", input_file_name())\n",
        "    )\n",
        "\n",
        "    corrupt_df = df_structured.filter(col(\"_corrupt_record\").isNotNull())\n",
        "    valid_structured_df = (df_structured\n",
        "        .filter(col(\"_corrupt_record\").isNull())\n",
        "        .drop(\"_corrupt_record\"))\n",
        "\n",
        "    logger.info(\"XML parsed and flattened.\")\n",
        "    return valid_structured_df, corrupt_df\n",
        "\n",
        "def save_structured_data(df: DataFrame, output_path: str) -> None:\n",
        "    (df.write.mode(\"append\")\n",
        "       .partitionBy(\"source\", \"year\", \"month\", \"day\")\n",
        "       .parquet(output_path))\n",
        "    logger.info(f\"Structured data saved to {output_path}\")\n",
        "\n",
        "def save_corrupt_records(df: DataFrame, corrupt_path: str) -> None:\n",
        "    if df.rdd.isEmpty():\n",
        "        logger.info(\"No corrupt records to save.\")\n",
        "        return\n",
        "    # Keep history; dont overwrite the entire folder.\n",
        "    (df.write.mode(\"append\")\n",
        "       .partitionBy(\"source\", \"year\", \"month\", \"day\")\n",
        "       .parquet(corrupt_path))\n",
        "    logger.info(f\"Corrupt records saved to {corrupt_path}\")\n",
        "\n",
        "def main():\n",
        "    landing_path = CONFIG[\"landing_path\"].format(**CONFIG)\n",
        "    xml_raw_path = CONFIG[\"xml_raw_path\"].format(**CONFIG)\n",
        "    xml_structured_path = CONFIG[\"xml_structured_path\"].format(**CONFIG)\n",
        "    corrupt_records_path = CONFIG[\"corrupt_records_path\"].format(**CONFIG)\n",
        "    xsd_path = CONFIG[\"xsd_path\"].format(**CONFIG)\n",
        "\n",
        "    set_spark_config()\n",
        "    copy_raw_xml_file(landing_path, xml_raw_path)\n",
        "    valid_structured_df, corrupt_df = parse_and_flatten_xml(landing_path, xsd_path, SCHEMA)\n",
        "    save_structured_data(valid_structured_df, xml_structured_path)\n",
        "    save_corrupt_records(corrupt_df, corrupt_records_path)\n",
        "\n",
        "    # Quick peek\n",
        "    logger.info(\"Sample structured rows:\")\n",
        "    valid_structured_df.show(5, truncate=False)\n",
        "    logger.info(\"Sample corrupt rows (if any):\")\n",
        "    corrupt_df.show(5, truncate=False)\n",
        "\n",
        "    logger.info(\"Bronze layer processing completed.\")\n",
        "\n",
        "main()\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 5,
              "statement_ids": [
                5
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "6",
              "normalized_state": "finished",
              "queued_time": "2025-09-07T06:56:18.5638825Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-07T06:56:18.5657516Z",
              "execution_finish_time": "2025-09-07T06:56:21.6156597Z",
              "parent_msg_id": "56a0c836-3aaf-443e-8277-438a325aacaf"
            },
            "text/plain": "StatementMeta(demospark, 6, 5, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o4223.text.\n: org.apache.hadoop.fs.FileAlreadyExistsException: Operation failed: \"This endpoint does not support BlobStorageEvents or SoftDelete. Please disable these account features if you would like to use this endpoint.\", 409, HEAD, https://storageaccount.dfs.core.windows.net/container/?upn=false&action=getAccessControl&timeout=90\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.checkException(AzureBlobFileSystem.java:1439)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:652)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:640)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1759)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.exists(AzureBlobFileSystem.java:1236)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:759)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:757)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\nCaused by: Operation failed: \"This endpoint does not support BlobStorageEvents or SoftDelete. Please disable these account features if you would like to use this endpoint.\", 409, HEAD, https://storageaccount.dfs.core.windows.net/container/?upn=false&action=getAccessControl&timeout=90\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute(AbfsRestOperation.java:231)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0(AbfsRestOperation.java:191)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:189)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAclStatus(AbfsClient.java:1000)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAclStatus(AbfsClient.java:981)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getIsNamespaceEnabled(AzureBlobFileSystemStore.java:422)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:1037)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:650)\n\t... 19 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 25\u001b[0m\n\u001b[1;32m     12\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[1;32m     13\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     14\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetails\u001b[39m\u001b[38;5;124m\"\u001b[39m, StructType([\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_corrupt_record\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m ])\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Copy raw XML to xml_raw\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m df_raw \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mtext(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabfss://container@storageaccount.dfs.core.windows.net/bronze/landing/xml_data/source=source_name/year=2025/month=09/day=07/sample.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m df_raw\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;241m.\u001b[39mpartitionBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabfss://container@storageaccount.dfs.core.windows.net/bronze/history/xml_raw/source=source_name/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Parse, explode, flatten, and store as Parquet in xml_structured\u001b[39;00m\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:615\u001b[0m, in \u001b[0;36mDataFrameReader.text\u001b[0;34m(self, paths, wholetext, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter)\u001b[0m\n\u001b[1;32m    613\u001b[0m     paths \u001b[38;5;241m=\u001b[39m [paths]\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 615\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mtext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(paths)))\n",
            "File \u001b[0;32m~/cluster-env/env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[0;32m~/cluster-env/env/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4223.text.\n: org.apache.hadoop.fs.FileAlreadyExistsException: Operation failed: \"This endpoint does not support BlobStorageEvents or SoftDelete. Please disable these account features if you would like to use this endpoint.\", 409, HEAD, https://storageaccount.dfs.core.windows.net/container/?upn=false&action=getAccessControl&timeout=90\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.checkException(AzureBlobFileSystem.java:1439)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:652)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:640)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1759)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.exists(AzureBlobFileSystem.java:1236)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:759)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:757)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\nCaused by: Operation failed: \"This endpoint does not support BlobStorageEvents or SoftDelete. Please disable these account features if you would like to use this endpoint.\", 409, HEAD, https://storageaccount.dfs.core.windows.net/container/?upn=false&action=getAccessControl&timeout=90\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute(AbfsRestOperation.java:231)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0(AbfsRestOperation.java:191)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:189)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAclStatus(AbfsClient.java:1000)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAclStatus(AbfsClient.java:981)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getIsNamespaceEnabled(AzureBlobFileSystemStore.java:422)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:1037)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:650)\n\t... 19 more\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit, current_timestamp, input_file_name, explode, col\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"XML to Bronze History Zone\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"16\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define explicit schema for XML parsing\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"details\", StructType([\n",
        "        StructField(\"name\", StringType(), True),\n",
        "        StructField(\"department\", StringType(), True),\n",
        "        StructField(\"roles\", StructType([\n",
        "            StructField(\"role\", ArrayType(StringType()), True)\n",
        "        ]), True)\n",
        "    ]), True),\n",
        "    StructField(\"_corrupt_record\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Copy raw XML to xml_raw\n",
        "df_raw = spark.read.text(\"abfss://container@storageaccount.dfs.core.windows.net/bronze/landing/xml_data/source=source_name/year=2025/month=09/day=07/sample.xml\")\n",
        "df_raw.write.mode(\"append\") \\\n",
        "    .partitionBy(\"source\", \"year\", \"month\") \\\n",
        "    .save(\"abfss://container@storageaccount.dfs.core.windows.net/bronze/history/xml_raw/source=source_name/\")\n",
        "\n",
        "# Parse, explode, flatten, and store as Parquet in xml_structured\n",
        "df_xml = spark.read.format(\"xml\") \\\n",
        "    .option(\"rowTag\", \"record\") \\\n",
        "    .option(\"mode\", \"PERMISSIVE\") \\\n",
        "    .option(\"rowValidationXSDPath\", \"abfss://container@storageaccount.dfs.core.windows.net/bronze/schemas/source_name.xsd\") \\\n",
        "    .schema(schema) \\\n",
        "    .load(\"abfss://container@storageaccount.dfs.core.windows.net/bronze/landing/xml_data/source=source_name/year=2025/month=09/day=07/sample.xml\")\n",
        "\n",
        "df_structured = df_xml.select(\n",
        "    col(\"id\"),\n",
        "    col(\"details.name\").alias(\"name\"),\n",
        "    col(\"details.department\").alias(\"department\"),\n",
        "    explode(col(\"details.roles.role\")).alias(\"role\"),\n",
        "    col(\"_corrupt_record\")\n",
        ").withColumn(\"source\", lit(\"source_name\")) \\\n",
        " .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
        " .withColumn(\"file_path\", input_file_name())\n",
        "\n",
        "# Handle malformed records\n",
        "corrupt_df = df_structured.filter(df_structured[\"_corrupt_record\"].isNotNull())\n",
        "corrupt_df.write.mode(\"overwrite\") \\\n",
        "    .parquet(\"abfss://container@storageaccount.dfs.core.windows.net/bronze/corrupt_records/source=source_name/\")\n",
        "\n",
        "# Store valid structured data\n",
        "valid_structured_df = df_structured.filter(df_structured[\"_corrupt_record\"].isNull()) \\\n",
        "    .select(\"id\", \"name\", \"department\", \"role\", \"source\", \"ingestion_timestamp\", \"file_path\")\n",
        "valid_structured_df.write.mode(\"append\") \\\n",
        "    .partitionBy(\"source\", \"year\", \"month\") \\\n",
        "    .parquet(\"abfss://container@storageaccount.dfs.core.windows.net/bronze/history/xml_structured/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 3,
              "statement_ids": [
                3
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "7",
              "normalized_state": "finished",
              "queued_time": "2025-09-07T08:11:39.869541Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-07T08:11:39.8713678Z",
              "execution_finish_time": "2025-09-07T08:11:40.5662463Z",
              "parent_msg_id": "962e84a0-0329-445b-a67a-25e915046dbd"
            },
            "text/plain": "StatementMeta(demospark, 7, 3, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Schema ===\nroot\n |-- details: struct (nullable = true)\n |    |-- department: string (nullable = true)\n |    |-- name: string (nullable = true)\n |    |-- roles: struct (nullable = true)\n |    |    |-- role: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |-- id: string (nullable = true)\n\n\n=== Sample rows ===\n+----------------------------------------+---+\n|details                                 |id |\n+----------------------------------------+---+\n|{Sales, John Doe, {[Manager, Analyst]}} |1  |\n|{Marketing, Jane Smith, {[Coordinator]}}|2  |\n|{IT, Bob, {[Engineer]}}                 |abc|\n+----------------------------------------+---+\n\n\n=== Corrupt rows (if any) ===\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `_corrupt_record` cannot be resolved. Did you mean one of the following? [`details`, `id`].;\n'Filter isnotnull('_corrupt_record)\n+- Relation [details#15,id#16] XmlRelation(com.databricks.spark.xml.DefaultSource$$Lambda$3742/0x000071b87719eea0@5a8155d4,Some(abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml),Map(rowtag -> record, mode -> PERMISSIVE, columnnameofcorruptrecord -> _corrupt_record, path -> abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml),null)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m10\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Corrupt rows (if any) ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m df\u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_corrupt_record\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misNotNull())\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:3329\u001b[0m, in \u001b[0;36mDataFrame.filter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   3327\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mfilter(condition)\n\u001b[1;32m   3328\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(condition, Column):\n\u001b[0;32m-> 3329\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mfilter(condition\u001b[38;5;241m.\u001b[39m_jc)\n\u001b[1;32m   3330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   3332\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN_OR_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3333\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(condition)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   3334\u001b[0m     )\n",
            "File \u001b[0;32m~/cluster-env/env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `_corrupt_record` cannot be resolved. Did you mean one of the following? [`details`, `id`].;\n'Filter isnotnull('_corrupt_record)\n+- Relation [details#15,id#16] XmlRelation(com.databricks.spark.xml.DefaultSource$$Lambda$3742/0x000071b87719eea0@5a8155d4,Some(abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml),Map(rowtag -> record, mode -> PERMISSIVE, columnnameofcorruptrecord -> _corrupt_record, path -> abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml),null)\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "# --- MVP: Read XML and preview ---------------------------------------------\n",
        "# Requires spark-xml on the pool (e.g. com.databricks:spark-xml_2.12:0.18.0)\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "CONFIG = {\n",
        "    \"container\": \"ds-india\",\n",
        "    \"storage_account\": \"demodsindia\",\n",
        "    \"landing_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/sample.xml\",\n",
        "    \"row_tag\": \"record\",   # <-- change to your top-level repeating element\n",
        "    # Optional: if you have XSD, set this path; otherwise leave as None\n",
        "    \"xsd_path\": None  # e.g. \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/sample.xsd\"\n",
        "}\n",
        "\n",
        "landing_path = CONFIG[\"landing_path\"].format(**CONFIG)\n",
        "\n",
        "reader = (\n",
        "    spark.read.format(\"com.databricks.spark.xml\")\n",
        "         .option(\"rowTag\", CONFIG[\"row_tag\"])\n",
        "         .option(\"mode\", \"PERMISSIVE\")                       # don't fail the whole file\n",
        "         .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")  # capture bad rows\n",
        ")\n",
        "\n",
        "# If you *do* have an XSD, uncomment the next line\n",
        "# if CONFIG[\"xsd_path\"]:\n",
        "#     reader = reader.option(\"rowValidationXSDPath\", CONFIG[\"xsd_path\"].format(**CONFIG))\n",
        "\n",
        "df = reader.load(landing_path)\n",
        "\n",
        "print(\"=== Schema ===\")\n",
        "df.printSchema()\n",
        "\n",
        "print(\"\\n=== Sample rows ===\")\n",
        "df.show(10, truncate=False)\n",
        "\n",
        "print(\"\\n=== Corrupt rows (if any) ===\")\n",
        "df.filter(col(\"_corrupt_record\").isNotNull()).show(5, truncate=False)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 5,
              "statement_ids": [
                5
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "7",
              "normalized_state": "finished",
              "queued_time": "2025-09-07T08:15:46.5894332Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-07T08:15:46.5912347Z",
              "execution_finish_time": "2025-09-07T08:15:53.7047676Z",
              "parent_msg_id": "bef8fbe7-c9c3-41c2-a20a-2a1bc44e43cd"
            },
            "text/plain": "StatementMeta(demospark, 7, 5, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Schema ===\nroot\n |-- details: struct (nullable = true)\n |    |-- department: string (nullable = true)\n |    |-- name: string (nullable = true)\n |    |-- roles: struct (nullable = true)\n |    |    |-- role: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |-- id: string (nullable = true)\n |-- _corrupt_record: string (nullable = true)\n\n\n=== Sample rows ===\n+----------------------------------------+---+---------------+\n|details                                 |id |_corrupt_record|\n+----------------------------------------+---+---------------+\n|{Sales, John Doe, {[Manager, Analyst]}} |1  |NULL           |\n|{Marketing, Jane Smith, {[Coordinator]}}|2  |NULL           |\n|{IT, Bob, {[Engineer]}}                 |abc|NULL           |\n+----------------------------------------+---+---------------+\n\n\n=== Corrupt rows (if any) ===\n+-------+---+---------------+\n|details|id |_corrupt_record|\n+-------+---+---------------+\n+-------+---+---------------+\n\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import col, lit\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "reader = (\n",
        "    spark.read.format(\"com.databricks.spark.xml\")\n",
        "         .option(\"rowTag\", \"record\")\n",
        "         .option(\"mode\", \"PERMISSIVE\")\n",
        "         .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
        ")\n",
        "\n",
        "df = reader.load(\"abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml\")\n",
        "\n",
        "# Ensure the column exists so downstream code is stable\n",
        "if \"_corrupt_record\" not in df.columns:\n",
        "    df = df.withColumn(\"_corrupt_record\", lit(None).cast(StringType()))\n",
        "\n",
        "print(\"=== Schema ===\")\n",
        "df.printSchema()\n",
        "\n",
        "print(\"\\n=== Sample rows ===\")\n",
        "df.show(10, truncate=False)\n",
        "\n",
        "print(\"\\n=== Corrupt rows (if any) ===\")\n",
        "df.filter(col(\"_corrupt_record\").isNotNull()).show(5, truncate=False)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 2,
              "statement_ids": [
                2
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "9",
              "normalized_state": "finished",
              "queued_time": "2025-09-07T09:30:18.6557824Z",
              "session_start_time": "2025-09-07T09:30:18.6578681Z",
              "execution_start_time": "2025-09-07T09:37:58.0730018Z",
              "execution_finish_time": "2025-09-07T09:38:29.7606951Z",
              "parent_msg_id": "7f8d4db9-4d1a-4d54-aee7-a307f7590618"
            },
            "text/plain": "StatementMeta(demospark, 9, 2, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o4313.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (vm-2b885156 executor 2): java.util.concurrent.ExecutionException: org.xml.sax.SAXParseException; schema_reference.4: Failed to read schema document 'file:/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1757237607632_0001/container_1757237607632_0001_01_000003/./abfss:/ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xsd', because 1) could not find the document; 2) the document could not be read; 3) the root element of the document is not <xsd:schema>.\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)\n\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\n\tat com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)\n\tat com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n\tat com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)\n\tat com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)\n\tat com.databricks.spark.xml.util.ValidatorUtil$.getSchema(ValidatorUtil.scala:54)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.$anonfun$parse$2(StaxXmlParser.scala:47)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.$anonfun$parse$1(StaxXmlParser.scala:47)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:865)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:865)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:368)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:332)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:368)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:332)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:368)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:332)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:368)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:332)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:636)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:639)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.xml.sax.SAXParseException; schema_reference.4: Failed to read schema document 'file:/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1757237607632_0001/container_1757237607632_0001_01_000003/./abfss:/ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xsd', because 1) could not find the document; 2) the document could not be read; 3) the root element of the document is not <xsd:schema>.\n\tat java.xml/com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:204)\n\tat java.xml/com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.error(ErrorHandlerWrapper.java:135)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:396)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:306)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.traversers.XSDHandler.reportSchemaErr(XSDHandler.java:4258)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.traversers.XSDHandler.reportSchemaError(XSDHandler.java:4241)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.traversers.XSDHandler.getSchemaDocument1(XSDHandler.java:2532)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.traversers.XSDHandler.getSchemaDocument(XSDHandler.java:2239)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.traversers.XSDHandler.parseSchema(XSDHandler.java:589)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaLoader.loadSchema(XMLSchemaLoader.java:618)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaLoader.loadGrammar(XMLSchemaLoader.java:577)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaLoader.loadGrammar(XMLSchemaLoader.java:543)\n\tat java.xml/com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory.newSchema(XMLSchemaFactory.java:281)\n\tat java.xml/javax.xml.validation.SchemaFactory.newSchema(SchemaFactory.java:612)\n\tat java.xml/javax.xml.validation.SchemaFactory.newSchema(SchemaFactory.java:628)\n\tat com.databricks.spark.xml.util.ValidatorUtil$$anon$1.load(ValidatorUtil.scala:44)\n\tat com.databricks.spark.xml.util.ValidatorUtil$$anon$1.load(ValidatorUtil.scala:35)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\t... 34 more\nCaused by: java.io.FileNotFoundException: /mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1757237607632_0001/container_1757237607632_0001_01_000003/./abfss:/ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xsd (No such file or directory)\n\tat java.base/java.io.FileInputStream.open0(Native Method)\n\tat java.base/java.io.FileInputStream.open(FileInputStream.java:216)\n\tat java.base/java.io.FileInputStream.<init>(FileInputStream.java:157)\n\tat java.base/java.io.FileInputStream.<init>(FileInputStream.java:111)\n\tat java.base/sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:86)\n\tat java.base/sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:195)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.XMLEntityManager.setupCurrentEntity(XMLEntityManager.java:653)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.XMLVersionDetector.determineDocVersion(XMLVersionDetector.java:150)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.opti.SchemaParsingConfig.parse(SchemaParsingConfig.java:593)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.opti.SchemaParsingConfig.parse(SchemaParsingConfig.java:696)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.opti.SchemaDOMParser.parse(SchemaDOMParser.java:530)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.traversers.XSDHandler.getSchemaDocument(XSDHandler.java:2227)\n\t... 45 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3055)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2991)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2990)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2990)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1294)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1294)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1294)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3262)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3193)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3182)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1028)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2568)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:361)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:322)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:358)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:230)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:191)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:214)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:220)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:961)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:214)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:202)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:33)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:33)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:33)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:202)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:186)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:180)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:262)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:905)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:413)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:380)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:242)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:838)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.util.concurrent.ExecutionException: org.xml.sax.SAXParseException; schema_reference.4: Failed to read schema document 'file:/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1757237607632_0001/container_1757237607632_0001_01_000003/./abfss:/ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xsd', because 1) could not find the document; 2) the document could not be read; 3) the root element of the document is not <xsd:schema>.\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)\n\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\n\tat com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)\n\tat com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n\tat com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)\n\tat com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)\n\tat com.databricks.spark.xml.util.ValidatorUtil$.getSchema(ValidatorUtil.scala:54)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.$anonfun$parse$2(StaxXmlParser.scala:47)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.$anonfun$parse$1(StaxXmlParser.scala:47)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:865)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:865)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:368)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:332)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:368)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:332)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:368)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:332)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:368)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:332)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:636)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:639)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: org.xml.sax.SAXParseException; schema_reference.4: Failed to read schema document 'file:/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1757237607632_0001/container_1757237607632_0001_01_000003/./abfss:/ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xsd', because 1) could not find the document; 2) the document could not be read; 3) the root element of the document is not <xsd:schema>.\n\tat java.xml/com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:204)\n\tat java.xml/com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.error(ErrorHandlerWrapper.java:135)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:396)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:306)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.traversers.XSDHandler.reportSchemaErr(XSDHandler.java:4258)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.traversers.XSDHandler.reportSchemaError(XSDHandler.java:4241)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.traversers.XSDHandler.getSchemaDocument1(XSDHandler.java:2532)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.traversers.XSDHandler.getSchemaDocument(XSDHandler.java:2239)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.traversers.XSDHandler.parseSchema(XSDHandler.java:589)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaLoader.loadSchema(XMLSchemaLoader.java:618)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaLoader.loadGrammar(XMLSchemaLoader.java:577)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaLoader.loadGrammar(XMLSchemaLoader.java:543)\n\tat java.xml/com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory.newSchema(XMLSchemaFactory.java:281)\n\tat java.xml/javax.xml.validation.SchemaFactory.newSchema(SchemaFactory.java:612)\n\tat java.xml/javax.xml.validation.SchemaFactory.newSchema(SchemaFactory.java:628)\n\tat com.databricks.spark.xml.util.ValidatorUtil$$anon$1.load(ValidatorUtil.scala:44)\n\tat com.databricks.spark.xml.util.ValidatorUtil$$anon$1.load(ValidatorUtil.scala:35)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\t... 34 more\nCaused by: java.io.FileNotFoundException: /mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1757237607632_0001/container_1757237607632_0001_01_000003/./abfss:/ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xsd (No such file or directory)\n\tat java.base/java.io.FileInputStream.open0(Native Method)\n\tat java.base/java.io.FileInputStream.open(FileInputStream.java:216)\n\tat java.base/java.io.FileInputStream.<init>(FileInputStream.java:157)\n\tat java.base/java.io.FileInputStream.<init>(FileInputStream.java:111)\n\tat java.base/sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:86)\n\tat java.base/sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:195)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.XMLEntityManager.setupCurrentEntity(XMLEntityManager.java:653)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.XMLVersionDetector.determineDocVersion(XMLVersionDetector.java:150)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.opti.SchemaParsingConfig.parse(SchemaParsingConfig.java:593)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.opti.SchemaParsingConfig.parse(SchemaParsingConfig.java:696)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.opti.SchemaDOMParser.parse(SchemaDOMParser.java:530)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.traversers.XSDHandler.getSchemaDocument(XSDHandler.java:2227)\n\t... 45 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 152\u001b[0m\n\u001b[1;32m    148\u001b[0m     corrupt_df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    150\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBronze processing complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 152\u001b[0m main()\n",
            "Cell \u001b[0;32mIn[5], line 142\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m copy_raw_xml_file(landing_path, xml_raw_path)\n\u001b[1;32m    141\u001b[0m valid_structured_df, corrupt_df \u001b[38;5;241m=\u001b[39m parse_and_flatten_xml(landing_path, xsd_path, SCHEMA)\n\u001b[0;32m--> 142\u001b[0m save_structured_data(valid_structured_df, xml_structured_path)\n\u001b[1;32m    143\u001b[0m save_corrupt_records(corrupt_df, corrupt_records_path)\n\u001b[1;32m    145\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreview structured:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[5], line 118\u001b[0m, in \u001b[0;36msave_structured_data\u001b[0;34m(df, output_path)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_structured_data\u001b[39m(df: DataFrame, output_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     (df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    117\u001b[0m        \u001b[38;5;241m.\u001b[39mpartitionBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 118\u001b[0m        \u001b[38;5;241m.\u001b[39mparquet(output_path))\n\u001b[1;32m    119\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStructured  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mparquet(path)\n",
            "File \u001b[0;32m~/cluster-env/env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[0;32m~/cluster-env/env/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4313.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (vm-2b885156 executor 2): java.util.concurrent.ExecutionException: org.xml.sax.SAXParseException; schema_reference.4: Failed to read schema document 'file:/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1757237607632_0001/container_1757237607632_0001_01_000003/./abfss:/ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xsd', because 1) could not find the document; 2) the document could not be read; 3) the root element of the document is not <xsd:schema>.\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)\n\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\n\tat com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)\n\tat com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n\tat com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)\n\tat com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)\n\tat com.databricks.spark.xml.util.ValidatorUtil$.getSchema(ValidatorUtil.scala:54)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.$anonfun$parse$2(StaxXmlParser.scala:47)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.$anonfun$parse$1(StaxXmlParser.scala:47)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:865)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:865)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:368)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:332)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:368)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:332)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:368)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:332)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:368)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:332)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:636)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:639)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.xml.sax.SAXParseException; schema_reference.4: Failed to read schema document 'file:/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1757237607632_0001/container_1757237607632_0001_01_000003/./abfss:/ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xsd', because 1) could not find the document; 2) the document could not be read; 3) the root element of the document is not <xsd:schema>.\n\tat java.xml/com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:204)\n\tat java.xml/com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.error(ErrorHandlerWrapper.java:135)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:396)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:306)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.traversers.XSDHandler.reportSchemaErr(XSDHandler.java:4258)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.traversers.XSDHandler.reportSchemaError(XSDHandler.java:4241)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.traversers.XSDHandler.getSchemaDocument1(XSDHandler.java:2532)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.traversers.XSDHandler.getSchemaDocument(XSDHandler.java:2239)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.traversers.XSDHandler.parseSchema(XSDHandler.java:589)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaLoader.loadSchema(XMLSchemaLoader.java:618)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaLoader.loadGrammar(XMLSchemaLoader.java:577)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaLoader.loadGrammar(XMLSchemaLoader.java:543)\n\tat java.xml/com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory.newSchema(XMLSchemaFactory.java:281)\n\tat java.xml/javax.xml.validation.SchemaFactory.newSchema(SchemaFactory.java:612)\n\tat java.xml/javax.xml.validation.SchemaFactory.newSchema(SchemaFactory.java:628)\n\tat com.databricks.spark.xml.util.ValidatorUtil$$anon$1.load(ValidatorUtil.scala:44)\n\tat com.databricks.spark.xml.util.ValidatorUtil$$anon$1.load(ValidatorUtil.scala:35)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\t... 34 more\nCaused by: java.io.FileNotFoundException: /mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1757237607632_0001/container_1757237607632_0001_01_000003/./abfss:/ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xsd (No such file or directory)\n\tat java.base/java.io.FileInputStream.open0(Native Method)\n\tat java.base/java.io.FileInputStream.open(FileInputStream.java:216)\n\tat java.base/java.io.FileInputStream.<init>(FileInputStream.java:157)\n\tat java.base/java.io.FileInputStream.<init>(FileInputStream.java:111)\n\tat java.base/sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:86)\n\tat java.base/sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:195)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.XMLEntityManager.setupCurrentEntity(XMLEntityManager.java:653)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.XMLVersionDetector.determineDocVersion(XMLVersionDetector.java:150)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.opti.SchemaParsingConfig.parse(SchemaParsingConfig.java:593)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.opti.SchemaParsingConfig.parse(SchemaParsingConfig.java:696)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.opti.SchemaDOMParser.parse(SchemaDOMParser.java:530)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.traversers.XSDHandler.getSchemaDocument(XSDHandler.java:2227)\n\t... 45 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3055)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2991)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2990)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2990)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1294)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1294)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1294)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3262)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3193)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3182)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1028)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2568)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:361)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:322)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:358)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:230)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:191)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:214)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:220)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:961)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:214)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:202)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:33)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:33)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:33)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:202)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:186)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:180)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:262)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:905)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:413)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:380)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:242)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:838)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.util.concurrent.ExecutionException: org.xml.sax.SAXParseException; schema_reference.4: Failed to read schema document 'file:/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1757237607632_0001/container_1757237607632_0001_01_000003/./abfss:/ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xsd', because 1) could not find the document; 2) the document could not be read; 3) the root element of the document is not <xsd:schema>.\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)\n\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\n\tat com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)\n\tat com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n\tat com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)\n\tat com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)\n\tat com.databricks.spark.xml.util.ValidatorUtil$.getSchema(ValidatorUtil.scala:54)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.$anonfun$parse$2(StaxXmlParser.scala:47)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.$anonfun$parse$1(StaxXmlParser.scala:47)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:865)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:865)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:368)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:332)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:368)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:332)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:368)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:332)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:368)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:332)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:636)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:639)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: org.xml.sax.SAXParseException; schema_reference.4: Failed to read schema document 'file:/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1757237607632_0001/container_1757237607632_0001_01_000003/./abfss:/ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xsd', because 1) could not find the document; 2) the document could not be read; 3) the root element of the document is not <xsd:schema>.\n\tat java.xml/com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:204)\n\tat java.xml/com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.error(ErrorHandlerWrapper.java:135)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:396)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:306)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.traversers.XSDHandler.reportSchemaErr(XSDHandler.java:4258)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.traversers.XSDHandler.reportSchemaError(XSDHandler.java:4241)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.traversers.XSDHandler.getSchemaDocument1(XSDHandler.java:2532)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.traversers.XSDHandler.getSchemaDocument(XSDHandler.java:2239)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.traversers.XSDHandler.parseSchema(XSDHandler.java:589)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaLoader.loadSchema(XMLSchemaLoader.java:618)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaLoader.loadGrammar(XMLSchemaLoader.java:577)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaLoader.loadGrammar(XMLSchemaLoader.java:543)\n\tat java.xml/com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory.newSchema(XMLSchemaFactory.java:281)\n\tat java.xml/javax.xml.validation.SchemaFactory.newSchema(SchemaFactory.java:612)\n\tat java.xml/javax.xml.validation.SchemaFactory.newSchema(SchemaFactory.java:628)\n\tat com.databricks.spark.xml.util.ValidatorUtil$$anon$1.load(ValidatorUtil.scala:44)\n\tat com.databricks.spark.xml.util.ValidatorUtil$$anon$1.load(ValidatorUtil.scala:35)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\t... 34 more\nCaused by: java.io.FileNotFoundException: /mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1757237607632_0001/container_1757237607632_0001_01_000003/./abfss:/ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xsd (No such file or directory)\n\tat java.base/java.io.FileInputStream.open0(Native Method)\n\tat java.base/java.io.FileInputStream.open(FileInputStream.java:216)\n\tat java.base/java.io.FileInputStream.<init>(FileInputStream.java:157)\n\tat java.base/java.io.FileInputStream.<init>(FileInputStream.java:111)\n\tat java.base/sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:86)\n\tat java.base/sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:195)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.XMLEntityManager.setupCurrentEntity(XMLEntityManager.java:653)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.XMLVersionDetector.determineDocVersion(XMLVersionDetector.java:150)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.opti.SchemaParsingConfig.parse(SchemaParsingConfig.java:593)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.opti.SchemaParsingConfig.parse(SchemaParsingConfig.java:696)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.opti.SchemaDOMParser.parse(SchemaDOMParser.java:530)\n\tat java.xml/com.sun.org.apache.xerces.internal.impl.xs.traversers.XSDHandler.getSchemaDocument(XSDHandler.java:2227)\n\t... 45 more\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "# === XML  Bronze (history)  complete script =================================\n",
        "# Requires spark-xml on pool: com.databricks:spark-xml_2.12:0.18.0 (or compatible)\n",
        "\n",
        "from pyspark.sql.functions import (\n",
        "    lit, current_timestamp, input_file_name, explode, col, when, array\n",
        ")\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
        "from pyspark.sql.dataframe import DataFrame\n",
        "import logging\n",
        "\n",
        "# ---------------- Config ----------------\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(\"xml-bronze\")\n",
        "\n",
        "CONFIG = {\n",
        "    \"container\": \"ds-india\",\n",
        "    \"storage_account\": \"demodsindia\",\n",
        "    \"source_name\": \"source_name\",\n",
        "    \"year\": \"2025\",\n",
        "    \"month\": \"09\",\n",
        "    \"day\": \"07\",\n",
        "    \"landing_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/sample.xml\",\n",
        "    \"xml_raw_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/bronze/history/xml_raw/source={source_name}/\",\n",
        "    \"xml_structured_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/bronze/history/xml_structured/\",\n",
        "    \"corrupt_records_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/bronze/corrupt_records/source={source_name}/\",\n",
        "    # Set to None to skip validation\n",
        "    \"xsd_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/sample.xsd\",\n",
        "    \"spark_partitions\": 16,\n",
        "    \"row_tag\": \"record\"\n",
        "}\n",
        "\n",
        "SCHEMA = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"details\", StructType([\n",
        "        StructField(\"name\", StringType(), True),\n",
        "        StructField(\"department\", StringType(), True),\n",
        "        StructField(\"roles\", StructType([\n",
        "            StructField(\"role\", ArrayType(StringType()), True)  # may be scalar or array at runtime\n",
        "        ]), True)\n",
        "    ]), True)\n",
        "    # Do NOT include _corrupt_record; spark-xml adds it when needed.\n",
        "])\n",
        "\n",
        "# ---------------- Helpers ----------------\n",
        "def set_spark_config(partitions: int = CONFIG[\"spark_partitions\"]) -> None:\n",
        "    spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "    spark.conf.set(\"spark.sql.shuffle.partitions\", str(partitions))\n",
        "    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
        "    logger.info(\"Spark configs set.\")\n",
        "\n",
        "def _import_mssparkutils():\n",
        "    try:\n",
        "        from notebookutils import mssparkutils  # Synapse\n",
        "        return mssparkutils\n",
        "    except Exception:\n",
        "        import mssparkutils  # fallback\n",
        "        return mssparkutils\n",
        "\n",
        "def copy_raw_xml_file(landing_path: str, xml_raw_path: str) -> None:\n",
        "    mssparkutils = _import_mssparkutils()\n",
        "    target_dir = xml_raw_path.format(**CONFIG)\n",
        "    dated = f\"{target_dir}year={CONFIG['year']}/month={CONFIG['month']}/day={CONFIG['day']}/\"\n",
        "    mssparkutils.fs.mkdirs(dated)\n",
        "    file_name = landing_path.rsplit(\"/\", 1)[-1]\n",
        "    dest = dated + file_name\n",
        "    if not mssparkutils.fs.cp(landing_path, dest, recurse=False):\n",
        "        raise RuntimeError(f\"Failed to copy {landing_path}  {dest}\")\n",
        "    logger.info(f\"Raw XML copied to {dest}\")\n",
        "\n",
        "def parse_and_flatten_xml(landing_path: str, xsd_path: str | None, schema: StructType):\n",
        "    # Build reader\n",
        "    reader = (spark.read.format(\"com.databricks.spark.xml\")\n",
        "              .option(\"rowTag\", CONFIG[\"row_tag\"])\n",
        "              .option(\"mode\", \"PERMISSIVE\")\n",
        "              .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
        "              .schema(schema))\n",
        "    if xsd_path:\n",
        "        reader = reader.option(\"rowValidationXSDPath\", xsd_path)\n",
        "\n",
        "    df_xml = reader.load(landing_path)\n",
        "\n",
        "    # Ensure _corrupt_record exists even if none are corrupt\n",
        "    if \"_corrupt_record\" not in df_xml.columns:\n",
        "        df_xml = df_xml.withColumn(\"_corrupt_record\", lit(None).cast(StringType()))\n",
        "\n",
        "    # Normalize roles to array<string> without dtype introspection\n",
        "    roles_col = col(\"details.roles.role\")\n",
        "    roles_as_array = roles_col.cast(ArrayType(StringType()))\n",
        "    roles_norm = (when(roles_as_array.isNotNull(), roles_as_array)              # already array\n",
        "                  .when(roles_col.isNull(), array().cast(ArrayType(StringType())))  # null  empty array\n",
        "                  .otherwise(array(roles_col.cast(StringType()))))              # scalar  wrap\n",
        "\n",
        "    df_structured = (\n",
        "        df_xml.select(\n",
        "            col(\"id\"),\n",
        "            col(\"details.name\").alias(\"name\"),\n",
        "            col(\"details.department\").alias(\"department\"),\n",
        "            explode(roles_norm).alias(\"role\"),\n",
        "            col(\"_corrupt_record\")\n",
        "        )\n",
        "        .withColumn(\"source\", lit(CONFIG[\"source_name\"]))\n",
        "        .withColumn(\"year\", lit(CONFIG[\"year\"]))\n",
        "        .withColumn(\"month\", lit(CONFIG[\"month\"]))\n",
        "        .withColumn(\"day\", lit(CONFIG[\"day\"]))\n",
        "        .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
        "        .withColumn(\"file_path\", input_file_name())\n",
        "    )\n",
        "\n",
        "    corrupt_df = df_structured.filter(col(\"_corrupt_record\").isNotNull())\n",
        "    valid_structured_df = df_structured.filter(col(\"_corrupt_record\").isNull()).drop(\"_corrupt_record\")\n",
        "\n",
        "    logger.info(\"XML parsed + flattened.\")\n",
        "    return valid_structured_df, corrupt_df\n",
        "\n",
        "def save_structured_data(df: DataFrame, output_path: str) -> None:\n",
        "    (df.write.mode(\"append\")\n",
        "       .partitionBy(\"source\", \"year\", \"month\", \"day\")\n",
        "       .parquet(output_path))\n",
        "    logger.info(f\"Structured  {output_path}\")\n",
        "\n",
        "def save_corrupt_records(df: DataFrame, corrupt_path: str) -> None:\n",
        "    if df.rdd.isEmpty():\n",
        "        logger.info(\"No corrupt records.\")\n",
        "        return\n",
        "    (df.write.mode(\"append\")\n",
        "       .partitionBy(\"source\", \"year\", \"month\", \"day\")\n",
        "       .parquet(corrupt_path))\n",
        "    logger.info(f\"Corrupt  {corrupt_path}\")\n",
        "\n",
        "# ---------------- Main ----------------\n",
        "def main():\n",
        "    set_spark_config()\n",
        "\n",
        "    landing_path = CONFIG[\"landing_path\"].format(**CONFIG)\n",
        "    xml_raw_path = CONFIG[\"xml_raw_path\"].format(**CONFIG)\n",
        "    xml_structured_path = CONFIG[\"xml_structured_path\"].format(**CONFIG)\n",
        "    corrupt_records_path = CONFIG[\"corrupt_records_path\"].format(**CONFIG)\n",
        "    xsd_path = CONFIG[\"xsd_path\"].format(**CONFIG) if CONFIG.get(\"xsd_path\") else None\n",
        "\n",
        "    copy_raw_xml_file(landing_path, xml_raw_path)\n",
        "    valid_structured_df, corrupt_df = parse_and_flatten_xml(landing_path, xsd_path, SCHEMA)\n",
        "    save_structured_data(valid_structured_df, xml_structured_path)\n",
        "    save_corrupt_records(corrupt_df, corrupt_records_path)\n",
        "\n",
        "    logger.info(\"Preview structured:\")\n",
        "    valid_structured_df.show(5, truncate=False)\n",
        "    logger.info(\"Preview corrupt (if any):\")\n",
        "    corrupt_df.show(5, truncate=False)\n",
        "\n",
        "    logger.info(\"Bronze processing complete.\")\n",
        "\n",
        "main()\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 3,
              "statement_ids": [
                3
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "9",
              "normalized_state": "finished",
              "queued_time": "2025-09-07T09:43:08.3967495Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-07T09:43:08.3987335Z",
              "execution_finish_time": "2025-09-07T09:43:21.3597001Z",
              "parent_msg_id": "668b6a93-16d9-4622-a384-e14aa72637f3"
            },
            "text/plain": "StatementMeta(demospark, 9, 3, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Structured sample ===\n+----+----------+----------+-----------+-----------+----+-----+---+--------------------------+---------------------------------------------------------------------+\n|id  |name      |department|role       |source     |year|month|day|ingestion_timestamp       |file_path                                                            |\n+----+----------+----------+-----------+-----------+----+-----+---+--------------------------+---------------------------------------------------------------------+\n|1   |John Doe  |Sales     |Manager    |source_name|2025|09   |07 |2025-09-07 09:43:08.742535|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml|\n|1   |John Doe  |Sales     |Analyst    |source_name|2025|09   |07 |2025-09-07 09:43:08.742535|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml|\n|2   |Jane Smith|Marketing |Coordinator|source_name|2025|09   |07 |2025-09-07 09:43:08.742535|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml|\n|NULL|Bob       |IT        |Engineer   |source_name|2025|09   |07 |2025-09-07 09:43:08.742535|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml|\n+----+----------+----------+-----------+-----------+----+-----+---+--------------------------+---------------------------------------------------------------------+\n\n=== Corrupt sample (if any) ===\n+---+----+----------+----+---------------+------+----+-----+---+-------------------+---------+\n|id |name|department|role|_corrupt_record|source|year|month|day|ingestion_timestamp|file_path|\n+---+----+----------+----+---------------+------+----+-----+---+-------------------+---------+\n+---+----+----------+----+---------------+------+----+-----+---+-------------------+---------+\n\nWrote structured rows to: abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/bronze/history/xml_structured/\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "# --- Bronze MVP: read XML -> flatten -> show -> write (Parquet) --------------\n",
        "# Requires spark-xml on the pool: com.databricks:spark-xml_2.12:0.18.0 (or compatible)\n",
        "\n",
        "from pyspark.sql.functions import lit, current_timestamp, input_file_name, explode, col, when, array\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
        "\n",
        "CONFIG = {\n",
        "    \"container\": \"ds-india\",\n",
        "    \"storage_account\": \"demodsindia\",\n",
        "    \"source_name\": \"source_name\",\n",
        "    \"year\": \"2025\",\n",
        "    \"month\": \"09\",\n",
        "    \"day\": \"07\",\n",
        "    \"landing_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/sample.xml\",\n",
        "    \"xml_structured_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/bronze/history/xml_structured/\",\n",
        "    \"row_tag\": \"record\"   # change to your repeating element if different\n",
        "}\n",
        "\n",
        "# Minimal schema (no _corrupt_record here)\n",
        "SCHEMA = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"details\", StructType([\n",
        "        StructField(\"name\", StringType(), True),\n",
        "        StructField(\"department\", StringType(), True),\n",
        "        StructField(\"roles\", StructType([\n",
        "            StructField(\"role\", ArrayType(StringType()), True)  # may arrive as scalar or array\n",
        "        ]), True)\n",
        "    ]), True)\n",
        "])\n",
        "\n",
        "# Optional: a couple of sane Spark settings for small jobs\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"16\")\n",
        "\n",
        "# 1) Read XML (PERMISSIVE, no XSD)\n",
        "landing_path = CONFIG[\"landing_path\"].format(**CONFIG)\n",
        "df_xml = (spark.read.format(\"com.databricks.spark.xml\")\n",
        "          .option(\"rowTag\", CONFIG[\"row_tag\"])\n",
        "          .option(\"mode\", \"PERMISSIVE\")\n",
        "          .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
        "          .schema(SCHEMA)\n",
        "          .load(landing_path))\n",
        "\n",
        "# Ensure _corrupt_record exists even if no corrupt rows were produced\n",
        "if \"_corrupt_record\" not in df_xml.columns:\n",
        "    from pyspark.sql.types import StringType\n",
        "    df_xml = df_xml.withColumn(\"_corrupt_record\", lit(None).cast(StringType()))\n",
        "\n",
        "# Normalize roles.role  always array<string>\n",
        "roles_col = col(\"details.roles.role\")\n",
        "roles_as_array = roles_col.cast(ArrayType(StringType()))\n",
        "roles_norm = (when(roles_as_array.isNotNull(), roles_as_array)\n",
        "              .when(roles_col.isNull(), array().cast(ArrayType(StringType())))\n",
        "              .otherwise(array(roles_col.cast(StringType()))))\n",
        "\n",
        "# 2) Flatten\n",
        "df_structured = (\n",
        "    df_xml.select(\n",
        "        col(\"id\"),\n",
        "        col(\"details.name\").alias(\"name\"),\n",
        "        col(\"details.department\").alias(\"department\"),\n",
        "        explode(roles_norm).alias(\"role\"),\n",
        "        col(\"_corrupt_record\")\n",
        "    )\n",
        "    .withColumn(\"source\", lit(CONFIG[\"source_name\"]))\n",
        "    .withColumn(\"year\", lit(CONFIG[\"year\"]))\n",
        "    .withColumn(\"month\", lit(CONFIG[\"month\"]))\n",
        "    .withColumn(\"day\", lit(CONFIG[\"day\"]))\n",
        "    .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
        "    .withColumn(\"file_path\", input_file_name())\n",
        ")\n",
        "\n",
        "# Split (optional for MVP): keep both to inspect quality quickly\n",
        "df_valid   = df_structured.filter(col(\"_corrupt_record\").isNull()).drop(\"_corrupt_record\")\n",
        "df_corrupt = df_structured.filter(col(\"_corrupt_record\").isNotNull())\n",
        "\n",
        "# 3) Preview\n",
        "print(\"=== Structured sample ===\")\n",
        "df_valid.show(10, truncate=False)\n",
        "print(\"=== Corrupt sample (if any) ===\")\n",
        "df_corrupt.show(5, truncate=False)\n",
        "\n",
        "# 4) Write structured to Parquet (partitioned)\n",
        "out_path = CONFIG[\"xml_structured_path\"].format(**CONFIG)\n",
        "(df_valid.write.mode(\"append\")\n",
        "   .partitionBy(\"source\", \"year\", \"month\", \"day\")\n",
        "   .parquet(out_path))\n",
        "\n",
        "print(f\"Wrote structured rows to: {out_path}\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 3,
              "statement_ids": [
                3
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "10",
              "normalized_state": "finished",
              "queued_time": "2025-09-07T10:35:36.0793395Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-07T10:35:36.0814356Z",
              "execution_finish_time": "2025-09-07T10:36:20.2080133Z",
              "parent_msg_id": "d8654511-edcf-4b69-bfa3-63c96e6beb39"
            },
            "text/plain": "StatementMeta(demospark, 10, 3, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== MODE: PERMISSIVE (valid structure, id='abc') =====\n+----+----------+-----------+---------------+\n|id  |name      |role       |_corrupt_record|\n+----+----------+-----------+---------------+\n|1   |John Doe  |Manager    |NULL           |\n|1   |John Doe  |Analyst    |NULL           |\n|2   |Jane Smith|Coordinator|NULL           |\n|NULL|Bob       |Engineer   |NULL           |\n+----+----------+-----------+---------------+\n\nRows total: 3 | id NULL: 1 | _corrupt_record rows: 0\n\n===== MODE: DROPMALFORMED (valid structure, id='abc') =====\n+---+----------+-----------+---------------+\n|id |name      |role       |_corrupt_record|\n+---+----------+-----------+---------------+\n|1  |John Doe  |Manager    |NULL           |\n|1  |John Doe  |Analyst    |NULL           |\n|2  |Jane Smith|Coordinator|NULL           |\n+---+----------+-----------+---------------+\n\nRows total: 3 | id NULL: 0 | _corrupt_record rows: 0\n\n===== MODE: FAILFAST (valid structure, id='abc') =====\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o4505.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 4 times, most recent failure: Lost task 0.3 in stage 18.0 (TID 15) (vm-94408436 executor 1): java.lang.IllegalArgumentException: Malformed line in FAILFAST mode\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.failedRecord(StaxXmlParser.scala:106)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.doParseColumn(StaxXmlParser.scala:88)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.$anonfun$parse$3(StaxXmlParser.scala:49)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:431)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:900)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:900)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:368)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:332)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:636)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:639)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.NumberFormatException: For input string: \"abc\"\n\tat java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67)\n\tat java.base/java.lang.Integer.parseInt(Integer.java:668)\n\tat java.base/java.lang.Integer.parseInt(Integer.java:786)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:310)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:310)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat com.databricks.spark.xml.util.TypeCast$.castTo(TypeCast.scala:55)\n\tat com.databricks.spark.xml.util.TypeCast$.signSafeToInt(TypeCast.scala:303)\n\tat com.databricks.spark.xml.util.TypeCast$.convertTo(TypeCast.scala:192)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.convertField(StaxXmlParser.scala:192)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.convertObject(StaxXmlParser.scala:334)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.doParseColumn(StaxXmlParser.scala:82)\n\t... 24 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3055)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2991)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2990)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2990)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1294)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1294)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1294)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3262)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3193)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3182)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1028)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2568)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2589)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2608)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:573)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:526)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4358)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3327)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4348)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:815)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4346)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:220)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:961)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4346)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3327)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3550)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:293)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:328)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.IllegalArgumentException: Malformed line in FAILFAST mode\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.failedRecord(StaxXmlParser.scala:106)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.doParseColumn(StaxXmlParser.scala:88)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.$anonfun$parse$3(StaxXmlParser.scala:49)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:431)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:900)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:900)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:368)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:332)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:636)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:639)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: java.lang.NumberFormatException: For input string: \"abc\"\n\tat java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67)\n\tat java.base/java.lang.Integer.parseInt(Integer.java:668)\n\tat java.base/java.lang.Integer.parseInt(Integer.java:786)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:310)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:310)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat com.databricks.spark.xml.util.TypeCast$.castTo(TypeCast.scala:55)\n\tat com.databricks.spark.xml.util.TypeCast$.signSafeToInt(TypeCast.scala:303)\n\tat com.databricks.spark.xml.util.TypeCast$.convertTo(TypeCast.scala:192)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.convertField(StaxXmlParser.scala:192)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.convertObject(StaxXmlParser.scala:334)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.doParseColumn(StaxXmlParser.scala:82)\n\t... 24 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 118\u001b[0m\n\u001b[1;32m    116\u001b[0m df \u001b[38;5;241m=\u001b[39m read_with_mode(SAMPLE_XML_PATH, mode)\n\u001b[1;32m    117\u001b[0m df_flat \u001b[38;5;241m=\u001b[39m flatten(df)\n\u001b[0;32m--> 118\u001b[0m df_flat\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_corrupt_record\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    120\u001b[0m total \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m    121\u001b[0m null_id \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwhere(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misNull())\u001b[38;5;241m.\u001b[39mcount()\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_string(n, truncate, vertical))\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:976\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    969\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    970\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    973\u001b[0m         },\n\u001b[1;32m    974\u001b[0m     )\n\u001b[0;32m--> 976\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, int_truncate, vertical)\n",
            "File \u001b[0;32m~/cluster-env/env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[0;32m~/cluster-env/env/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4505.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 4 times, most recent failure: Lost task 0.3 in stage 18.0 (TID 15) (vm-94408436 executor 1): java.lang.IllegalArgumentException: Malformed line in FAILFAST mode\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.failedRecord(StaxXmlParser.scala:106)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.doParseColumn(StaxXmlParser.scala:88)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.$anonfun$parse$3(StaxXmlParser.scala:49)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:431)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:900)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:900)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:368)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:332)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:636)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:639)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.NumberFormatException: For input string: \"abc\"\n\tat java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67)\n\tat java.base/java.lang.Integer.parseInt(Integer.java:668)\n\tat java.base/java.lang.Integer.parseInt(Integer.java:786)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:310)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:310)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat com.databricks.spark.xml.util.TypeCast$.castTo(TypeCast.scala:55)\n\tat com.databricks.spark.xml.util.TypeCast$.signSafeToInt(TypeCast.scala:303)\n\tat com.databricks.spark.xml.util.TypeCast$.convertTo(TypeCast.scala:192)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.convertField(StaxXmlParser.scala:192)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.convertObject(StaxXmlParser.scala:334)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.doParseColumn(StaxXmlParser.scala:82)\n\t... 24 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3055)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2991)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2990)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2990)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1294)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1294)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1294)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3262)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3193)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3182)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1028)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2568)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2589)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2608)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:573)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:526)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4358)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3327)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4348)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:815)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4346)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:220)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:961)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4346)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3327)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3550)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:293)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:328)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.IllegalArgumentException: Malformed line in FAILFAST mode\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.failedRecord(StaxXmlParser.scala:106)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.doParseColumn(StaxXmlParser.scala:88)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.$anonfun$parse$3(StaxXmlParser.scala:49)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:431)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:900)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:900)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:368)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:332)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:636)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:639)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: java.lang.NumberFormatException: For input string: \"abc\"\n\tat java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67)\n\tat java.base/java.lang.Integer.parseInt(Integer.java:668)\n\tat java.base/java.lang.Integer.parseInt(Integer.java:786)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:310)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:310)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat com.databricks.spark.xml.util.TypeCast$.castTo(TypeCast.scala:55)\n\tat com.databricks.spark.xml.util.TypeCast$.signSafeToInt(TypeCast.scala:303)\n\tat com.databricks.spark.xml.util.TypeCast$.convertTo(TypeCast.scala:192)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.convertField(StaxXmlParser.scala:192)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.convertObject(StaxXmlParser.scala:334)\n\tat com.databricks.spark.xml.parsers.StaxXmlParser$.doParseColumn(StaxXmlParser.scala:82)\n\t... 24 more\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "# --- Compare spark-xml modes: PERMISSIVE vs DROPMALFORMED vs FAILFAST (fixed) --\n",
        "# Requires spark-xml (e.g., com.databricks:spark-xml_2.12:0.18.0)\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
        "from pyspark.sql.functions import col, explode, when, array, lit\n",
        "import uuid\n",
        "\n",
        "# 0) Paths & helpers\n",
        "CONTAINER = \"ds-india\"\n",
        "ACCOUNT   = \"demodsindia\"\n",
        "BASE      = f\"abfss://{CONTAINER}@{ACCOUNT}.dfs.core.windows.net/SEMIDATA/_tmp_mode_demo/\"\n",
        "SAMPLE_XML_PATH     = BASE + f\"valid_{uuid.uuid4().hex}.xml\"\n",
        "MALFORMED_XML_PATH  = BASE + f\"malformed_{uuid.uuid4().hex}.xml\"   # optional demo\n",
        "\n",
        "def put_text(path: str, text: str):\n",
        "    try:\n",
        "        from notebookutils import mssparkutils\n",
        "    except Exception:\n",
        "        import mssparkutils\n",
        "    mssparkutils.fs.mkdirs(BASE)\n",
        "    mssparkutils.fs.put(path, text, True)\n",
        "\n",
        "# 1) Valid-structure XML (has id=\"abc\")\n",
        "sample_xml = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<records>\n",
        "    <record>\n",
        "        <id>1</id>\n",
        "        <details>\n",
        "            <name>John Doe</name>\n",
        "            <department>Sales</department>\n",
        "            <roles>\n",
        "                <role>Manager</role>\n",
        "                <role>Analyst</role>\n",
        "            </roles>\n",
        "        </details>\n",
        "    </record>\n",
        "    <record>\n",
        "        <id>2</id>\n",
        "        <details>\n",
        "            <name>Jane Smith</name>\n",
        "            <department>Marketing</department>\n",
        "            <roles>\n",
        "                <role>Coordinator</role>\n",
        "            </roles>\n",
        "        </details>\n",
        "    </record>\n",
        "    <record>\n",
        "        <id>abc</id>\n",
        "        <details>\n",
        "            <name>Bob</name>\n",
        "            <department>IT</department>\n",
        "            <roles>\n",
        "                <role>Engineer</role>\n",
        "            </roles>\n",
        "        </details>\n",
        "    </record>\n",
        "</records>\n",
        "\"\"\"\n",
        "put_text(SAMPLE_XML_PATH, sample_xml)\n",
        "\n",
        "# 2) Malformed XML (optional: broken closing tag to demo structural errors)\n",
        "malformed_xml = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<records>\n",
        "  <record>\n",
        "    <id>10</id>\n",
        "    <details><name>Alice</name></details>\n",
        "  </record>\n",
        "  <record>\n",
        "    <id>11  <!-- MISSING closing tag on purpose -->\n",
        "    <details><name>Broken</name></details>\n",
        "  </record>\n",
        "</records>\n",
        "\"\"\"\n",
        "put_text(MALFORMED_XML_PATH, malformed_xml)\n",
        "\n",
        "# 3) Schema: declare id as IntegerType to test type mismatch\n",
        "SCHEMA = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),  # \"abc\" -> becomes NULL (not corrupt)\n",
        "    StructField(\"details\", StructType([\n",
        "        StructField(\"name\", StringType(), True),\n",
        "        StructField(\"department\", StringType(), True),\n",
        "        StructField(\"roles\", StructType([\n",
        "            StructField(\"role\", ArrayType(StringType()), True)\n",
        "        ]), True)\n",
        "    ]), True)\n",
        "])\n",
        "\n",
        "def read_with_mode(path: str, mode: str):\n",
        "    df = (spark.read.format(\"com.databricks.spark.xml\")\n",
        "            .option(\"rowTag\", \"record\")\n",
        "            .option(\"mode\", mode)  # PERMISSIVE | DROPMALFORMED | FAILFAST\n",
        "            .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
        "            .schema(SCHEMA)\n",
        "            .load(path))\n",
        "    # <-- Guard: add _corrupt_record if the source didn't create it\n",
        "    if \"_corrupt_record\" not in df.columns:\n",
        "        df = df.withColumn(\"_corrupt_record\", lit(None).cast(StringType()))\n",
        "    return df\n",
        "\n",
        "def flatten(df):\n",
        "    roles_col = col(\"details.roles.role\")\n",
        "    roles_as_array = roles_col.cast(ArrayType(StringType()))\n",
        "    roles_norm = (when(roles_as_array.isNotNull(), roles_as_array)\n",
        "                  .when(roles_col.isNull(), array().cast(ArrayType(StringType())))\n",
        "                  .otherwise(array(roles_col.cast(StringType()))))\n",
        "    return (df.select(\n",
        "                col(\"id\"),\n",
        "                col(\"details.name\").alias(\"name\"),\n",
        "                explode(roles_norm).alias(\"role\"),\n",
        "                col(\"_corrupt_record\"))\n",
        "            )\n",
        "\n",
        "# 4) Read valid-structure XML in each mode\n",
        "for mode in [\"PERMISSIVE\", \"DROPMALFORMED\", \"FAILFAST\"]:\n",
        "    print(f\"\\n===== MODE: {mode} (valid structure, id='abc') =====\")\n",
        "    df = read_with_mode(SAMPLE_XML_PATH, mode)\n",
        "    df_flat = flatten(df)\n",
        "    df_flat.select(\"id\",\"name\",\"role\",\"_corrupt_record\").show(truncate=False)\n",
        "\n",
        "    total = df.count()\n",
        "    null_id = df.where(col(\"id\").isNull()).count()\n",
        "    corrupt_rows = df.where(col(\"_corrupt_record\").isNotNull()).count()\n",
        "    print(f\"Rows total: {total} | id NULL: {null_id} | _corrupt_record rows: {corrupt_rows}\")\n",
        "\n",
        "# 5) Structural errors demo with MALFORMED XML\n",
        "print(\"\\n===== Structural errors demo with MALFORMED XML =====\")\n",
        "for mode in [\"PERMISSIVE\", \"DROPMALFORMED\", \"FAILFAST\"]:\n",
        "    print(f\"\\n--- MODE: {mode} (malformed structure) ---\")\n",
        "    try:\n",
        "        df_bad = read_with_mode(MALFORMED_XML_PATH, mode)\n",
        "        df_bad.select(\"id\",\"_corrupt_record\").show(truncate=False)\n",
        "        print(\"Read succeeded.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Read failed with {mode}: {type(e).__name__}: {str(e)[:300]} ...\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 4,
              "statement_ids": [
                4
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "10",
              "normalized_state": "finished",
              "queued_time": "2025-09-07T10:48:05.641887Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-07T10:48:05.6443127Z",
              "execution_finish_time": "2025-09-07T10:48:35.1784784Z",
              "parent_msg_id": "e88814c4-a043-4514-9489-fb502f1c6301"
            },
            "text/plain": "StatementMeta(demospark, 10, 4, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== PERMISSIVE_VALIDATE ===\nValid sample:\n+----------+----------+-----------+-----------+----+-----+---+--------------------------+---------------------------------------------------------------------+---+\n|name      |department|role       |source     |year|month|day|ingestion_timestamp       |file_path                                                            |id |\n+----------+----------+-----------+-----------+----+-----+---+--------------------------+---------------------------------------------------------------------+---+\n|John Doe  |Sales     |Manager    |source_name|2025|09   |07 |2025-09-07 10:48:06.020513|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml|1  |\n|John Doe  |Sales     |Analyst    |source_name|2025|09   |07 |2025-09-07 10:48:06.020513|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml|1  |\n|Jane Smith|Marketing |Coordinator|source_name|2025|09   |07 |2025-09-07 10:48:06.020513|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml|2  |\n+----------+----------+-----------+-----------+----+-----+---+--------------------------+---------------------------------------------------------------------+---+\n\nCorrupt sample:\n+------+----+--------+---------------+\n|id_raw|name|role    |_corrupt_reason|\n+------+----+--------+---------------+\n|abc   |Bob |Engineer|INVALID_INT:id |\n+------+----+--------+---------------+\n\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "# === XML Bronze MVP with switchable behavior ==================================\n",
        "# Requires: com.databricks:spark-xml_2.12:0.18.0 (or compatible)\n",
        "\n",
        "from pyspark.sql.functions import (\n",
        "    col, explode, when, array, lit, current_timestamp, input_file_name\n",
        ")\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
        "\n",
        "# ---- Toggle here -------------------------------------------------------------\n",
        "MODE = \"PERMISSIVE_VALIDATE\"  # PERMISSIVE_VALIDATE | DROPMALFORMED | FAILFAST\n",
        "ROW_TAG = \"record\"\n",
        "\n",
        "CONFIG = {\n",
        "    \"container\": \"ds-india\",\n",
        "    \"storage_account\": \"demodsindia\",\n",
        "    \"source_name\": \"source_name\",\n",
        "    \"year\": \"2025\",\n",
        "    \"month\": \"09\",\n",
        "    \"day\": \"07\",\n",
        "    \"landing_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/sample.xml\",\n",
        "    \"xml_structured_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/bronze/history/xml_structured/\",\n",
        "    \"corrupt_records_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/bronze/corrupt_records/source={source_name}/\",\n",
        "}\n",
        "\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"16\")\n",
        "\n",
        "landing_path = CONFIG[\"landing_path\"].format(**CONFIG)\n",
        "out_struct   = CONFIG[\"xml_structured_path\"].format(**CONFIG)\n",
        "out_corrupt  = CONFIG[\"corrupt_records_path\"].format(**CONFIG)\n",
        "\n",
        "def normalize_roles(df):\n",
        "    roles = col(\"details.roles.role\")\n",
        "    roles_arr = roles.cast(ArrayType(StringType()))\n",
        "    roles_norm = when(roles_arr.isNotNull(), roles_arr) \\\n",
        "                 .when(roles.isNull(), array().cast(ArrayType(StringType()))) \\\n",
        "                 .otherwise(array(roles.cast(StringType())))\n",
        "    return roles_norm\n",
        "\n",
        "def with_common_cols(df):\n",
        "    return (df\n",
        "        .withColumn(\"source\", lit(CONFIG[\"source_name\"]))\n",
        "        .withColumn(\"year\",   lit(CONFIG[\"year\"]))\n",
        "        .withColumn(\"month\",  lit(CONFIG[\"month\"]))\n",
        "        .withColumn(\"day\",    lit(CONFIG[\"day\"]))\n",
        "        .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
        "        .withColumn(\"file_path\", input_file_name())\n",
        "    )\n",
        "\n",
        "if MODE == \"PERMISSIVE_VALIDATE\":\n",
        "    # Read id as STRING; keep everything\n",
        "    schema = StructType([\n",
        "        StructField(\"id\", StringType(), True),\n",
        "        StructField(\"details\", StructType([\n",
        "            StructField(\"name\", StringType(), True),\n",
        "            StructField(\"department\", StringType(), True),\n",
        "            StructField(\"roles\", StructType([\n",
        "                StructField(\"role\", ArrayType(StringType()), True)\n",
        "            ]), True)\n",
        "        ]), True)\n",
        "    ])\n",
        "\n",
        "    df = (spark.read.format(\"com.databricks.spark.xml\")\n",
        "            .option(\"rowTag\", ROW_TAG)\n",
        "            .option(\"mode\", \"PERMISSIVE\")\n",
        "            .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
        "            .schema(schema)\n",
        "            .load(landing_path))\n",
        "\n",
        "    # ensure _corrupt_record exists for consistent downstream logic\n",
        "    if \"_corrupt_record\" not in df.columns:\n",
        "        df = df.withColumn(\"_corrupt_record\", lit(None).cast(StringType()))\n",
        "\n",
        "    roles_norm = normalize_roles(df)\n",
        "\n",
        "    base = (df.select(\n",
        "                col(\"id\").alias(\"id_raw\"),\n",
        "                col(\"details.name\").alias(\"name\"),\n",
        "                col(\"details.department\").alias(\"department\"),\n",
        "                explode(roles_norm).alias(\"role\"),\n",
        "                col(\"_corrupt_record\"))\n",
        "            )\n",
        "\n",
        "    base = with_common_cols(base)\n",
        "\n",
        "    # manual validation: id must be integer\n",
        "    id_int = col(\"id_raw\").cast(IntegerType())\n",
        "    is_bad_type = id_int.isNull() & col(\"id_raw\").isNotNull()\n",
        "    # rows structurally corrupt (rare here) OR type-invalid go to corrupt bucket\n",
        "    corrupt_df = (base.where(is_bad_type | col(\"_corrupt_record\").isNotNull())\n",
        "                        .withColumn(\"_corrupt_reason\",\n",
        "                                    when(col(\"_corrupt_record\").isNotNull(), lit(\"STRUCTURAL_CORRUPT\"))\n",
        "                                    .otherwise(lit(\"INVALID_INT:id\"))))\n",
        "    valid_df = (base.where(~is_bad_type & col(\"_corrupt_record\").isNull())\n",
        "                    .withColumn(\"id\", id_int)\n",
        "                    .drop(\"id_raw\", \"_corrupt_record\", \"_corrupt_reason\"))\n",
        "\n",
        "    print(\"=== PERMISSIVE_VALIDATE ===\")\n",
        "    print(\"Valid sample:\")\n",
        "    valid_df.show(10, truncate=False)\n",
        "    print(\"Corrupt sample:\")\n",
        "    corrupt_df.select(\"id_raw\",\"name\",\"role\",\"_corrupt_reason\").show(10, truncate=False)\n",
        "\n",
        "    # write\n",
        "    (valid_df.write.mode(\"append\")\n",
        "        .partitionBy(\"source\",\"year\",\"month\",\"day\")\n",
        "        .parquet(out_struct))\n",
        "    if not corrupt_df.rdd.isEmpty():\n",
        "        (corrupt_df.write.mode(\"append\")\n",
        "            .partitionBy(\"source\",\"year\",\"month\",\"day\")\n",
        "            .parquet(out_corrupt))\n",
        "\n",
        "elif MODE in (\"DROPMALFORMED\", \"FAILFAST\"):\n",
        "    # Read with typed schema; bad rows are dropped (DROPMALFORMED) or job fails (FAILFAST)\n",
        "    schema = StructType([\n",
        "        StructField(\"id\", IntegerType(), True),\n",
        "        StructField(\"details\", StructType([\n",
        "            StructField(\"name\", StringType(), True),\n",
        "            StructField(\"department\", StringType(), True),\n",
        "            StructField(\"roles\", StructType([\n",
        "                StructField(\"role\", ArrayType(StringType()), True)\n",
        "            ]), True)\n",
        "        ]), True)\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        df = (spark.read.format(\"com.databricks.spark.xml\")\n",
        "                .option(\"rowTag\", ROW_TAG)\n",
        "                .option(\"mode\", MODE)  # DROPMALFORMED | FAILFAST\n",
        "                .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
        "                .schema(schema)\n",
        "                .load(landing_path))\n",
        "        # guard (in case PERMISSIVE sneaks in)\n",
        "        if \"_corrupt_record\" not in df.columns:\n",
        "            df = df.withColumn(\"_corrupt_record\", lit(None).cast(StringType()))\n",
        "\n",
        "        roles_norm = normalize_roles(df)\n",
        "        flat = (df.select(\n",
        "                    col(\"id\"),\n",
        "                    col(\"details.name\").alias(\"name\"),\n",
        "                    explode(roles_norm).alias(\"role\"),\n",
        "                    col(\"_corrupt_record\"))\n",
        "                )\n",
        "        flat = with_common_cols(flat).drop(\"_corrupt_record\")\n",
        "\n",
        "        print(f\"=== {MODE} ===\")\n",
        "        flat.show(10, truncate=False)\n",
        "\n",
        "        (flat.write.mode(\"append\")\n",
        "            .partitionBy(\"source\",\"year\",\"month\",\"day\")\n",
        "            .parquet(out_struct))\n",
        "\n",
        "    except Exception as e:\n",
        "        # FAILFAST will land here for first malformed/type-failure\n",
        "        print(f\"{MODE} aborted: {type(e).__name__}: {str(e)[:300]}\")\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"MODE must be one of: PERMISSIVE_VALIDATE | DROPMALFORMED | FAILFAST\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Sample "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 6,
              "statement_ids": [
                6
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "10",
              "normalized_state": "finished",
              "queued_time": "2025-09-07T11:01:17.5604661Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-07T11:01:17.5627765Z",
              "execution_finish_time": "2025-09-07T11:01:19.6387048Z",
              "parent_msg_id": "0e72597c-e9d8-4939-b4db-b69fbf15fc35"
            },
            "text/plain": "StatementMeta(demospark, 10, 6, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== VALID sample ===\n+----+----------+----------+-----------+-----------+----+-----+---+--------------------------+---------------------------------------------------------------------+\n|id  |name      |department|role       |source     |year|month|day|ingestion_timestamp       |file_path                                                            |\n+----+----------+----------+-----------+-----------+----+-----+---+--------------------------+---------------------------------------------------------------------+\n|1   |John Doe  |Sales     |Manager    |source_name|2025|09   |07 |2025-09-07 11:01:17.904902|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml|\n|1   |John Doe  |Sales     |Analyst    |source_name|2025|09   |07 |2025-09-07 11:01:17.904902|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml|\n|2   |Jane Smith|Marketing |Coordinator|source_name|2025|09   |07 |2025-09-07 11:01:17.904902|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml|\n|NULL|Bob       |IT        |Engineer   |source_name|2025|09   |07 |2025-09-07 11:01:17.904902|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml|\n+----+----------+----------+-----------+-----------+----+-----+---+--------------------------+---------------------------------------------------------------------+\n\n=== CORRUPT sample (with reasons) ===\n+---+----+----+------------------+\n|id |name|role|_validation_errors|\n+---+----+----+------------------+\n+---+----+----+------------------+\n\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "# === Schema-driven XML validation  corrected order ===========================\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import (\n",
        "    StructType, StructField, StringType, IntegerType, ArrayType, DataType\n",
        ")\n",
        "\n",
        "CONFIG = {\n",
        "    \"container\": \"ds-india\",\n",
        "    \"storage_account\": \"demodsindia\",\n",
        "    \"source_name\": \"source_name\",\n",
        "    \"year\": \"2025\", \"month\": \"09\", \"day\": \"07\",\n",
        "    \"landing_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/sample.xml\",\n",
        "    \"xml_structured_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/bronze/history/xml_structured/\",\n",
        "    \"corrupt_records_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/bronze/corrupt_records/source={source_name}/\",\n",
        "    \"row_tag\": \"record\"\n",
        "}\n",
        "\n",
        "# 1) Target schema (typed output you want)\n",
        "TARGET_SCHEMA = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"details\", StructType([\n",
        "        StructField(\"name\", StringType(), True),\n",
        "        StructField(\"department\", StringType(), True),\n",
        "        StructField(\"roles\", StructType([\n",
        "            StructField(\"role\", ArrayType(StringType()), True)\n",
        "        ]), True)\n",
        "    ]), True)\n",
        "])\n",
        "\n",
        "# 2) Shape-only (stringy) schema\n",
        "def to_stringy_schema(dt: DataType) -> DataType:\n",
        "    if isinstance(dt, StructType):\n",
        "        return StructType([StructField(f.name, to_stringy_schema(f.dataType), True) for f in dt.fields])\n",
        "    if isinstance(dt, ArrayType):\n",
        "        return ArrayType(StringType(), True)\n",
        "    return StringType()\n",
        "\n",
        "RAW_SCHEMA = to_stringy_schema(TARGET_SCHEMA)\n",
        "\n",
        "# 3) Read raw (all text), permissive\n",
        "landing = CONFIG[\"landing_path\"].format(**CONFIG)\n",
        "df_raw = (spark.read.format(\"com.databricks.spark.xml\")\n",
        "          .option(\"rowTag\", CONFIG[\"row_tag\"])\n",
        "          .option(\"mode\", \"PERMISSIVE\")\n",
        "          .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
        "          .schema(RAW_SCHEMA)\n",
        "          .load(landing))\n",
        "\n",
        "if \"_corrupt_record\" not in df_raw.columns:\n",
        "    df_raw = df_raw.withColumn(\"_corrupt_record\", F.lit(None).cast(StringType()))\n",
        "\n",
        "# Normalize roles.role -> array<string>\n",
        "roles_col = F.col(\"details.roles.role\")\n",
        "roles_arr = roles_col.cast(ArrayType(StringType()))\n",
        "roles_norm = (F.when(roles_arr.isNotNull(), roles_arr)\n",
        "              .when(roles_col.isNull(), F.array().cast(ArrayType(StringType())))\n",
        "              .otherwise(F.array(roles_col.cast(StringType()))))\n",
        "df_raw = df_raw.withColumn(\"_roles_norm\", roles_norm)\n",
        "\n",
        "# 4) Build validation errors on RAW (IMPORTANT: before casting/replacing columns)\n",
        "def collect_errors(path: str, dt: DataType):\n",
        "    if isinstance(dt, StructType):\n",
        "        errs = []\n",
        "        for f in dt.fields:\n",
        "            errs += collect_errors(f\"{path}.{f.name}\", f.dataType)\n",
        "        return errs\n",
        "    if isinstance(dt, ArrayType):\n",
        "        el = dt.elementType\n",
        "        # flag if any non-null element fails cast\n",
        "        invalid_elem = F.exists(F.col(path), lambda x: x.isNotNull() & x.cast(el).isNull())\n",
        "        return [F.when(invalid_elem, F.lit(f\"INVALID_ARRAY_ELEMENT_{el.simpleString()}:{path}\"))]\n",
        "    # primitive\n",
        "    casted = F.col(path).cast(dt)\n",
        "    invalid = F.col(path).isNotNull() & casted.isNull()\n",
        "    return [F.when(invalid, F.lit(f\"INVALID_{dt.simpleString()}:{path}\"))]\n",
        "\n",
        "errs = [F.when(F.col(\"_corrupt_record\").isNotNull(), F.lit(\"STRUCTURAL_CORRUPT\"))]\n",
        "for f in TARGET_SCHEMA.fields:\n",
        "    errs += collect_errors(f.name, f.dataType)\n",
        "\n",
        "errors_array = F.array_remove(F.array(*[e for e in errs if e is not None]), None)\n",
        "df_enriched = (df_raw\n",
        "               .withColumn(\"_validation_errors\", errors_array)\n",
        "               .withColumn(\"_is_corrupt\", F.size(errors_array) > 0))\n",
        "\n",
        "# 5) Now create the TYPED projection (safe to reuse names)\n",
        "def typed_col(path: str, dt: DataType):\n",
        "    if isinstance(dt, StructType):\n",
        "        return F.struct(*[typed_col(f\"{path}.{f.name}\", f.dataType).alias(f.name) for f in dt.fields])\n",
        "    if isinstance(dt, ArrayType):\n",
        "        return F.transform(F.col(path), lambda x: x.cast(dt.elementType))\n",
        "    return F.col(path).cast(dt)\n",
        "\n",
        "typed_cols = [typed_col(f.name, f.dataType).alias(f.name) for f in TARGET_SCHEMA.fields]\n",
        "df_typed = df_enriched.select(*typed_cols, \"_roles_norm\", \"_validation_errors\", \"_is_corrupt\")\n",
        "\n",
        "# 6) Flatten, add audit cols, split valid/corrupt\n",
        "df_flat = (df_typed.select(\n",
        "              F.col(\"id\"),\n",
        "              F.col(\"details.name\").alias(\"name\"),\n",
        "              F.col(\"details.department\").alias(\"department\"),\n",
        "              F.explode(F.col(\"_roles_norm\")).alias(\"role\"),\n",
        "              \"_is_corrupt\", \"_validation_errors\")\n",
        "          .withColumn(\"source\", F.lit(CONFIG[\"source_name\"]))\n",
        "          .withColumn(\"year\", F.lit(CONFIG[\"year\"]))\n",
        "          .withColumn(\"month\", F.lit(CONFIG[\"month\"]))\n",
        "          .withColumn(\"day\", F.lit(CONFIG[\"day\"]))\n",
        "          .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
        "          .withColumn(\"file_path\", F.input_file_name())\n",
        ")\n",
        "\n",
        "valid_df   = df_flat.where(~F.col(\"_is_corrupt\")).drop(\"_is_corrupt\", \"_validation_errors\")\n",
        "corrupt_df = df_flat.where(F.col(\"_is_corrupt\"))\n",
        "\n",
        "print(\"=== VALID sample ===\")\n",
        "valid_df.show(10, truncate=False)\n",
        "print(\"=== CORRUPT sample (with reasons) ===\")\n",
        "corrupt_df.select(\"id\",\"name\",\"role\",\"_validation_errors\").show(10, truncate=False)\n",
        "\n",
        "# 7) Optional writes\n",
        "out_struct  = CONFIG[\"xml_structured_path\"].format(**CONFIG)\n",
        "out_corrupt = CONFIG[\"corrupt_records_path\"].format(**CONFIG)\n",
        "(valid_df.write.mode(\"append\")\n",
        "    .partitionBy(\"source\",\"year\",\"month\",\"day\")\n",
        "    .parquet(out_struct))\n",
        "if not corrupt_df.rdd.isEmpty():\n",
        "    (corrupt_df.write.mode(\"append\")\n",
        "        .partitionBy(\"source\",\"year\",\"month\",\"day\")\n",
        "        .parquet(out_corrupt))\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Sample "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 5,
              "statement_ids": [
                5
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "11",
              "normalized_state": "finished",
              "queued_time": "2025-09-07T11:45:40.972832Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-07T11:45:40.9750341Z",
              "execution_finish_time": "2025-09-07T11:45:51.9475629Z",
              "parent_msg_id": "5e6f2592-2501-4f4a-8e8e-3ab88b465605"
            },
            "text/plain": "StatementMeta(demospark, 11, 5, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== VALID sample ===\n+---+----------+----------+-----------+-----------+----+-----+---+--------------------------+---------------------------------------------------------------------+\n|id |name      |department|role       |source     |year|month|day|ingestion_timestamp       |file_path                                                            |\n+---+----------+----------+-----------+-----------+----+-----+---+--------------------------+---------------------------------------------------------------------+\n|1  |John Doe  |Sales     |Manager    |source_name|2025|09   |07 |2025-09-07 11:45:41.335427|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml|\n|1  |John Doe  |Sales     |Analyst    |source_name|2025|09   |07 |2025-09-07 11:45:41.335427|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml|\n|2  |Jane Smith|Marketing |Coordinator|source_name|2025|09   |07 |2025-09-07 11:45:41.335427|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml|\n+---+----------+----------+-----------+-----------+----+-----+---+--------------------------+---------------------------------------------------------------------+\n\n=== CORRUPT sample ===\n+------+----+--------+----------------+\n|id_raw|name|role    |_corrupt_reasons|\n+------+----+--------+----------------+\n|abc   |Bob |Engineer|[INVALID_int:id]|\n+------+----+--------+----------------+\n\nWrote valid  abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/bronze/history/xml_structured/\nWrote corrupt  abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/bronze/corrupt_records/source=source_name/\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "# === XML -> Bronze with schema-driven validation (no XSD) =====================\n",
        "# Requires: com.databricks:spark-xml_2.12:0.18.0 (or compatible) on your Spark pool\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import (\n",
        "    StructType, StructField, StringType, IntegerType, ArrayType, DataType\n",
        ")\n",
        "\n",
        "# ---------------- Config ----------------\n",
        "CONFIG = {\n",
        "    \"container\": \"ds-india\",\n",
        "    \"storage_account\": \"demodsindia\",\n",
        "    \"source_name\": \"source_name\",\n",
        "    \"year\": \"2025\", \"month\": \"09\", \"day\": \"07\",\n",
        "    \"landing_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/sample.xml\",\n",
        "    \"xml_structured_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/bronze/history/xml_structured/\",\n",
        "    \"corrupt_records_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/bronze/corrupt_records/source={source_name}/\",\n",
        "    \"row_tag\": \"record\"\n",
        "}\n",
        "\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"16\")\n",
        "\n",
        "# ---------------- Target schema (typed output you want) ----------------\n",
        "TARGET_SCHEMA = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),                    # validate as INT\n",
        "    StructField(\"details\", StructType([\n",
        "        StructField(\"name\", StringType(), True),\n",
        "        StructField(\"department\", StringType(), True),\n",
        "        StructField(\"roles\", StructType([\n",
        "            StructField(\"role\", ArrayType(StringType()), True) # array<string>\n",
        "        ]), True)\n",
        "    ]), True)\n",
        "])\n",
        "\n",
        "# ---------------- Build \"shape-only\" schema (same structure, leaves as string) ----------------\n",
        "def to_stringy_schema(dt: DataType) -> DataType:\n",
        "    if isinstance(dt, StructType):\n",
        "        return StructType([StructField(f.name, to_stringy_schema(f.dataType), True) for f in dt.fields])\n",
        "    if isinstance(dt, ArrayType):\n",
        "        return ArrayType(StringType(), True)\n",
        "    return StringType()\n",
        "\n",
        "RAW_SCHEMA = to_stringy_schema(TARGET_SCHEMA)\n",
        "\n",
        "# ---------------- Read raw XML as text (PERMISSIVE) ----------------\n",
        "landing = CONFIG[\"landing_path\"].format(**CONFIG)\n",
        "df_raw = (spark.read.format(\"com.databricks.spark.xml\")\n",
        "          .option(\"rowTag\", CONFIG[\"row_tag\"])\n",
        "          .option(\"mode\", \"PERMISSIVE\")\n",
        "          .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
        "          .schema(RAW_SCHEMA)\n",
        "          .load(landing))\n",
        "\n",
        "# Ensure _corrupt_record exists (only created when structure is malformed)\n",
        "if \"_corrupt_record\" not in df_raw.columns:\n",
        "    df_raw = df_raw.withColumn(\"_corrupt_record\", F.lit(None).cast(StringType()))\n",
        "\n",
        "# Keep raw id for cast-failure detection later\n",
        "df_raw = df_raw.withColumn(\"_id_raw\", F.col(\"id\"))\n",
        "\n",
        "# Normalize roles: sometimes scalar, sometimes array -> always array<string>\n",
        "roles_col = F.col(\"details.roles.role\")\n",
        "roles_arr = roles_col.cast(ArrayType(StringType()))\n",
        "empty_arr = F.expr(\"array()\").cast(ArrayType(StringType()))\n",
        "roles_norm = (F.when(roles_arr.isNotNull(), roles_arr)\n",
        "              .when(roles_col.isNull(), empty_arr)\n",
        "              .otherwise(F.array(roles_col.cast(StringType()))))\n",
        "df_raw = df_raw.withColumn(\"_roles_norm\", roles_norm)\n",
        "\n",
        "# ---------------- Build validation errors on RAW text ----------------\n",
        "# datatype rule: id must be integer\n",
        "id_cast = F.col(\"_id_raw\").cast(IntegerType())\n",
        "invalid_id = F.col(\"_id_raw\").isNotNull() & id_cast.isNull()\n",
        "\n",
        "errs = [\n",
        "    F.when(F.col(\"_corrupt_record\").isNotNull(), F.lit(\"STRUCTURAL_CORRUPT\")),\n",
        "    F.when(invalid_id, F.lit(\"INVALID_int:id\"))\n",
        "]\n",
        "\n",
        "errors_array = F.array_remove(F.array(*[e for e in errs if e is not None]), None)\n",
        "df_enriched = (df_raw\n",
        "               .withColumn(\"_validation_errors\", errors_array)\n",
        "               .withColumn(\"_has_validation_errors\", F.size(errors_array) > 0))\n",
        "\n",
        "# ---------------- Create typed columns (use casted id) ----------------\n",
        "df_typed = (df_enriched\n",
        "            .withColumn(\"id\", id_cast)  # typed id\n",
        "            .select(\n",
        "                \"id\",\n",
        "                \"details\",\n",
        "                \"_roles_norm\",\n",
        "                \"_validation_errors\",\n",
        "                \"_has_validation_errors\",\n",
        "                \"_id_raw\"\n",
        "            ))\n",
        "\n",
        "# ---------------- Flatten, add audit columns ----------------\n",
        "df_flat = (df_typed.select(\n",
        "              F.col(\"id\"),\n",
        "              F.col(\"details.name\").alias(\"name\"),\n",
        "              F.col(\"details.department\").alias(\"department\"),\n",
        "              F.explode(F.col(\"_roles_norm\")).alias(\"role\"),\n",
        "              \"_has_validation_errors\", \"_validation_errors\", \"_id_raw\")\n",
        "          .withColumn(\"source\", F.lit(CONFIG[\"source_name\"]))\n",
        "          .withColumn(\"year\",   F.lit(CONFIG[\"year\"]))\n",
        "          .withColumn(\"month\",  F.lit(CONFIG[\"month\"]))\n",
        "          .withColumn(\"day\",    F.lit(CONFIG[\"day\"]))\n",
        "          .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
        "          .withColumn(\"file_path\", F.input_file_name())\n",
        ")\n",
        "\n",
        "# ---------------- Final split using RAW vs TYPED check ----------------\n",
        "bad_id = F.col(\"_id_raw\").isNotNull() & F.col(\"id\").isNull()\n",
        "has_other_errors = F.col(\"_has_validation_errors\")\n",
        "is_corrupt = bad_id | has_other_errors\n",
        "\n",
        "# Build reasons first (before any renames), using concat for arrays\n",
        "reasons_base = F.coalesce(F.col(\"_validation_errors\"), empty_arr)\n",
        "reasons_appended = F.when(bad_id, F.concat(reasons_base, F.array(F.lit(\"INVALID_int:id\")))).otherwise(reasons_base)\n",
        "reasons = F.array_distinct(reasons_appended)  # avoid duplicates\n",
        "\n",
        "# Compute corrupt columns, THEN rename _id_raw -> id_raw\n",
        "corrupt_pre = (\n",
        "    df_flat.where(is_corrupt)\n",
        "           .withColumn(\"_corrupt_reasons\", reasons)\n",
        "           .withColumn(\"_corrupt_reason\", F.concat_ws(\"|\", F.col(\"_corrupt_reasons\")))\n",
        ")\n",
        "\n",
        "corrupt_df = corrupt_pre.withColumnRenamed(\"_id_raw\", \"id_raw\")\n",
        "\n",
        "valid_df = (\n",
        "    df_flat.where(~is_corrupt)\n",
        "           .drop(\"_validation_errors\", \"_has_validation_errors\", \"_id_raw\")\n",
        ")\n",
        "\n",
        "# ---------------- Preview ----------------\n",
        "print(\"=== VALID sample ===\")\n",
        "valid_df.show(10, truncate=False)\n",
        "print(\"=== CORRUPT sample ===\")\n",
        "corrupt_df.select(\"id_raw\",\"name\",\"role\",\"_corrupt_reasons\").show(10, truncate=False)\n",
        "\n",
        "# ---------------- Writes (Parquet; switch to .format('delta') if you prefer) ----------------\n",
        "out_struct  = CONFIG[\"xml_structured_path\"].format(**CONFIG)\n",
        "out_corrupt = CONFIG[\"corrupt_records_path\"].format(**CONFIG)\n",
        "\n",
        "(valid_df.write.mode(\"append\")\n",
        "    .partitionBy(\"source\",\"year\",\"month\",\"day\")\n",
        "    .parquet(out_struct))\n",
        "\n",
        "if not corrupt_df.rdd.isEmpty():\n",
        "    (corrupt_df.write.mode(\"append\")\n",
        "        .partitionBy(\"source\",\"year\",\"month\",\"day\")\n",
        "        .parquet(out_corrupt))\n",
        "\n",
        "print(f\"Wrote valid  {out_struct}\")\n",
        "if not corrupt_df.rdd.isEmpty():\n",
        "    print(f\"Wrote corrupt  {out_corrupt}\")\n",
        "else:\n",
        "    print(\"No corrupt rows this run.\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Sample with Config and Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 8,
              "statement_ids": [
                8
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "11",
              "normalized_state": "finished",
              "queued_time": "2025-09-07T12:10:09.6143874Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-07T12:10:09.6172406Z",
              "execution_finish_time": "2025-09-07T12:10:13.8180703Z",
              "parent_msg_id": "2523b3e5-7c9b-4c41-8017-1f93f3c8255b"
            },
            "text/plain": "StatementMeta(demospark, 11, 8, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== VALID sample ===\n+---+----------+----------+-----------+-----------+----+-----+---+--------------------------+---------------------------------------------------------------------+\n|id |name      |department|role       |source     |year|month|day|ingestion_timestamp       |file_path                                                            |\n+---+----------+----------+-----------+-----------+----+-----+---+--------------------------+---------------------------------------------------------------------+\n|1  |John Doe  |Sales     |Manager    |source_name|2025|09   |07 |2025-09-07 12:10:12.730214|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml|\n|1  |John Doe  |Sales     |Analyst    |source_name|2025|09   |07 |2025-09-07 12:10:12.730214|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml|\n|2  |Jane Smith|Marketing |Coordinator|source_name|2025|09   |07 |2025-09-07 12:10:12.730214|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml|\n+---+----------+----------+-----------+-----------+----+-----+---+--------------------------+---------------------------------------------------------------------+\n\n=== CORRUPT sample ===\n+----+----+----------+--------+-----------+----+-----+---+--------------------------+---------------------------------------------------------------------+----------------+------+\n|id  |name|department|role    |source     |year|month|day|ingestion_timestamp       |file_path                                                            |_corrupt_reasons|raw_id|\n+----+----+----------+--------+-----------+----+-----+---+--------------------------+---------------------------------------------------------------------+----------------+------+\n|NULL|Bob |IT        |Engineer|source_name|2025|09   |07 |2025-09-07 12:10:12.971101|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml|[CAST_FAIL:id]  |abc   |\n+----+----+----------+--------+-----------+----+-----+---+--------------------------+---------------------------------------------------------------------+----------------+------+\n\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "# ========================= XML Bronze Framework (no XSD) =========================\n",
        "# Requires: com.databricks:spark-xml_2.12:0.18.0 (or compatible).\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from typing import Dict, List, Any\n",
        "import json\n",
        "\n",
        "# ---------------------------- Utilities -----------------------------------\n",
        "\n",
        "def to_stringy_schema(dt: DataType) -> DataType:\n",
        "    \"\"\"Shape-only schema: same structure as target, but leaves are strings (arrays preserved).\"\"\"\n",
        "    if isinstance(dt, StructType):\n",
        "        return StructType([StructField(f.name, to_stringy_schema(f.dataType), True) for f in dt.fields])\n",
        "    if isinstance(dt, ArrayType):\n",
        "        return ArrayType(to_stringy_schema(dt.elementType), True)\n",
        "    return StringType()\n",
        "\n",
        "def _sanitize(path: str) -> str:\n",
        "    \"\"\"Turn a dot path into a safe column name.\"\"\"\n",
        "    return path.replace(\".\", \"__\").replace(\"[\", \"_\").replace(\"]\", \"_\")\n",
        "\n",
        "def ensure_array(df, path: str, array_dt: ArrayType, out_col: str):\n",
        "    \"\"\"Normalize 'path' to always be ArrayType(array_dt.elementType).\"\"\"\n",
        "    col_ref = F.col(path)\n",
        "    arr_cast = col_ref.cast(array_dt)\n",
        "    empty_arr = F.expr(\"array()\").cast(array_dt)\n",
        "    norm = F.when(arr_cast.isNotNull(), arr_cast) \\\n",
        "            .when(col_ref.isNull(), empty_arr) \\\n",
        "            .otherwise(F.array(col_ref.cast(array_dt.elementType)))\n",
        "    return df.withColumn(out_col, norm)\n",
        "\n",
        "def exists_invalid_primitive(col_arr, elem_type: DataType):\n",
        "    \"\"\"True if any non-null array element fails cast to elem_type.\"\"\"\n",
        "    return F.exists(col_arr, lambda x: x.isNotNull() & x.cast(elem_type).isNull())\n",
        "\n",
        "def exists_invalid_struct_field(col_arr, field: StructField):\n",
        "    \"\"\"True if any struct element has a non-null child that fails cast to the child's type.\"\"\"\n",
        "    return F.exists(col_arr, lambda e: e[field.name].isNotNull() & e[field.name].cast(field.dataType).isNull())\n",
        "\n",
        "def get_dt(path: str, dt: DataType) -> DataType:\n",
        "    \"\"\"Walk a dot path on a StructType to return the nested DataType.\"\"\"\n",
        "    if path == \"\" or path is None:\n",
        "        return dt\n",
        "    if not isinstance(dt, StructType):\n",
        "        raise KeyError(f\"Path '{path}' not found in schema.\")\n",
        "    parts = path.split(\".\", 1)\n",
        "    head = parts[0]\n",
        "    tail = parts[1] if len(parts) > 1 else \"\"\n",
        "    child = next((f for f in dt.fields if f.name == head), None)\n",
        "    if child is None:\n",
        "        raise KeyError(f\"Field '{head}' not found under path '{path}'.\")\n",
        "    return get_dt(tail, child.dataType)\n",
        "\n",
        "def collect_validation_errors(df, target_schema: StructType, normalized_map: Dict[str, str]):\n",
        "    \"\"\"\n",
        "    Build an array<string> of validation error labels by comparing RAW strings to target types.\n",
        "    Uses normalized array columns when validating arrays (so it works even if source was scalar).\n",
        "    \"\"\"\n",
        "    errs = [F.when(F.col(\"_corrupt_record\").isNotNull(), F.lit(\"STRUCTURAL_CORRUPT\"))]\n",
        "\n",
        "    def walk(path: str, dt: DataType):\n",
        "        nonlocal errs\n",
        "        if isinstance(dt, StructType):\n",
        "            for f in dt.fields:\n",
        "                walk(f\"{path}.{f.name}\" if path else f.name, f.dataType)\n",
        "        elif isinstance(dt, ArrayType):\n",
        "            use_col = F.col(normalized_map.get(path, path))\n",
        "            elem = dt.elementType\n",
        "            if isinstance(elem, StructType):\n",
        "                for ch in elem.fields:\n",
        "                    errs.append(F.when(\n",
        "                        exists_invalid_struct_field(use_col, ch),\n",
        "                        F.lit(f\"INVALID_ARRAY_ELEM_FIELD_{ch.dataType.simpleString()}:{path}.{ch.name}\")\n",
        "                    ))\n",
        "            else:\n",
        "                errs.append(F.when(\n",
        "                    exists_invalid_primitive(use_col, elem),\n",
        "                    F.lit(f\"INVALID_ARRAY_ELEMENT_{elem.simpleString()}:{path}\")\n",
        "                ))\n",
        "        else:\n",
        "            raw_col = F.col(path)\n",
        "            casted  = raw_col.cast(dt)\n",
        "            errs.append(F.when(raw_col.isNotNull() & casted.isNull(),\n",
        "                               F.lit(f\"INVALID_{dt.simpleString()}:{path}\")))\n",
        "    walk(\"\", target_schema)\n",
        "\n",
        "    return F.array_remove(F.array(*[e for e in errs if e is not None]), None)\n",
        "\n",
        "def typed_projection(path: str, dt: DataType, normalized_map: Dict[str, str]):\n",
        "    \"\"\"Produce a typed Column (recursively cast to target schema).\"\"\"\n",
        "    if isinstance(dt, StructType):\n",
        "        return F.struct(*[typed_projection(f\"{path}.{f.name}\", f.dataType, normalized_map).alias(f.name)\n",
        "                          for f in dt.fields])\n",
        "    if isinstance(dt, ArrayType):\n",
        "        col_ref = F.col(normalized_map.get(path, path))\n",
        "        elem = dt.elementType\n",
        "        if isinstance(elem, StructType):\n",
        "            return F.transform(col_ref, lambda e:\n",
        "                               F.struct(*[(e[ch.name].cast(ch.dataType)).alias(ch.name) for ch in elem.fields]))\n",
        "        else:\n",
        "            return F.transform(col_ref, lambda x: x.cast(elem))\n",
        "    return F.col(path).cast(dt)\n",
        "\n",
        "def explode_paths(df, explode_plan: List[Dict[str, str]], normalized_map: Dict[str, str]):\n",
        "    \"\"\"Apply explode (outer/inner) for each configured array path in order.\"\"\"\n",
        "    for item in explode_plan or []:\n",
        "        p = item[\"path\"]\n",
        "        alias = item.get(\"alias\") or _sanitize(p)\n",
        "        mode  = (item.get(\"explode_mode\") or \"outer\").lower()\n",
        "        norm_col = normalized_map[p]  # must exist\n",
        "        if mode == \"inner\":\n",
        "            df = df.withColumn(alias, F.explode(F.col(norm_col)))\n",
        "        else:\n",
        "            df = df.withColumn(alias, F.explode_outer(F.col(norm_col)))\n",
        "    return df\n",
        "\n",
        "def add_audit_cols(df, config: Dict[str, Any]):\n",
        "    return (df\n",
        "            .withColumn(\"source\", F.lit(config.get(\"source_name\", \"unknown\")))\n",
        "            .withColumn(\"year\",   F.lit(config[\"year\"]))\n",
        "            .withColumn(\"month\",  F.lit(config[\"month\"]))\n",
        "            .withColumn(\"day\",    F.lit(config[\"day\"]))\n",
        "            .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
        "            .withColumn(\"file_path\", F.input_file_name()))\n",
        "\n",
        "def load_metadata(json_str_or_dict):\n",
        "    \"\"\"Accepts JSON string or dict; normalizes keys to a unified internal shape.\"\"\"\n",
        "    meta = json.loads(json_str_or_dict) if isinstance(json_str_or_dict, str) else dict(json_str_or_dict)\n",
        "\n",
        "    # normalize 'select' entries: allow either source/alias or expr/as\n",
        "    norm_select = []\n",
        "    for it in meta.get(\"select\", []):\n",
        "        expr  = it.get(\"source\") or it.get(\"expr\")\n",
        "        alias = it.get(\"alias\") or it.get(\"as\") or expr\n",
        "        if not expr:\n",
        "            raise ValueError(\"select item missing 'source'/'expr'\")\n",
        "        norm_select.append({\"expr\": expr, \"as\": alias})\n",
        "    meta[\"select\"] = norm_select\n",
        "\n",
        "    # normalize 'explode' entries\n",
        "    norm_explode = []\n",
        "    for it in meta.get(\"explode\", []):\n",
        "        path = it[\"path\"]\n",
        "        alias = it.get(\"alias\") or it.get(\"as\") or _sanitize(path)\n",
        "        norm_explode.append({\n",
        "            \"path\": path,\n",
        "            \"alias\": alias,\n",
        "            \"explode_mode\": it.get(\"explode_mode\", \"outer\"),\n",
        "            \"null_handling\": it.get(\"null_handling\", \"empty_array\")\n",
        "        })\n",
        "    meta[\"explode\"] = norm_explode\n",
        "\n",
        "    # raw evidence fields (optional)\n",
        "    evid = meta.get(\"evidence\", [])\n",
        "    meta[\"evidence\"] = evid if isinstance(evid, list) else []\n",
        "\n",
        "    # optional partitions override\n",
        "    parts = meta.get(\"partitions\")\n",
        "    if parts and isinstance(parts, list):\n",
        "        meta[\"partitions\"] = parts\n",
        "\n",
        "    meta[\"version\"] = meta.get(\"version\", \"1.0\")\n",
        "    return meta\n",
        "\n",
        "# ---------------------------- Main entry point ---------------------------------\n",
        "\n",
        "def process_xml(config: Dict[str, Any],\n",
        "                target_schema: StructType,\n",
        "                metadata: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Config:\n",
        "      - landing_path (templated), xml_structured_path, corrupt_records_path\n",
        "      - row_tag, read_mode ('PERMISSIVE'|'DROPMALFORMED'|'FAILFAST'), write_format ('parquet'|'delta')\n",
        "      - partitions (list), source_name, year, month, day\n",
        "\n",
        "    Metadata (JSON normalized via load_metadata):\n",
        "      - explode: [{ \"path\": \"<dot.path>\", \"alias\": \"<name>\", \"explode_mode\": \"outer|inner\" }]\n",
        "      - select:  [{ \"expr\": \"<col or expr>\", \"as\": \"<alias>\" }, ...]\n",
        "      - evidence: [\"<dot.path>\", ...]  # raw fields to keep & cast-check (e.g., [\"id\"])\n",
        "      - partitions: [\"source\",\"year\",\"month\",\"day\"]  # optional override\n",
        "    \"\"\"\n",
        "    # 1) Read raw as strings (shape-only schema)\n",
        "    raw_schema = to_stringy_schema(target_schema)\n",
        "    landing = config[\"landing_path\"].format(**config)\n",
        "    read_mode = config.get(\"read_mode\", \"PERMISSIVE\")\n",
        "\n",
        "    df_raw = (spark.read.format(\"com.databricks.spark.xml\")\n",
        "              .option(\"rowTag\", config[\"row_tag\"])\n",
        "              .option(\"mode\", read_mode)\n",
        "              .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
        "              .schema(raw_schema)\n",
        "              .load(landing))\n",
        "\n",
        "    if \"_corrupt_record\" not in df_raw.columns:\n",
        "        df_raw = df_raw.withColumn(\"_corrupt_record\", F.lit(None).cast(StringType()))\n",
        "\n",
        "    # 2) Normalize arrays to explode & capture evidence columns\n",
        "    normalized_map = {}  # path -> normalized col name\n",
        "    for item in (metadata.get(\"explode\") or []):\n",
        "        p = item[\"path\"]\n",
        "        arr_dt_target = get_dt(p, target_schema)\n",
        "        if not isinstance(arr_dt_target, ArrayType):\n",
        "            raise TypeError(f\"explode path '{p}' is not ArrayType in target schema.\")\n",
        "        out_col = f\"__norm__{_sanitize(p)}\"\n",
        "        df_raw = ensure_array(df_raw, p, ArrayType(to_stringy_schema(arr_dt_target.elementType), True), out_col)\n",
        "        normalized_map[p] = out_col\n",
        "\n",
        "    evidence_map = {}  # path -> raw col name\n",
        "    for p in (metadata.get(\"evidence\") or []):\n",
        "        ev_col = f\"__raw__{_sanitize(p)}\"\n",
        "        df_raw = df_raw.withColumn(ev_col, F.col(p))\n",
        "        evidence_map[p] = ev_col\n",
        "\n",
        "    # 3) Validation errors on RAW vs target types\n",
        "    errors_col = collect_validation_errors(df_raw, target_schema, normalized_map)\n",
        "    df_checked = (df_raw\n",
        "                  .withColumn(\"_validation_errors\", errors_col)\n",
        "                  .withColumn(\"_has_validation_errors\", F.size(errors_col) > 0))\n",
        "\n",
        "    # 4) Typed projection (from RAW + normalized arrays)\n",
        "    typed_cols = [typed_projection(f.name, f.dataType, normalized_map).alias(f.name)\n",
        "                  for f in target_schema.fields]\n",
        "    keep_cols = list(normalized_map.values()) + list(evidence_map.values()) + [\"_validation_errors\", \"_has_validation_errors\"]\n",
        "    df_typed = df_checked.select(*typed_cols, *[F.col(c) for c in keep_cols])\n",
        "\n",
        "    # 5) Generic cast-failure guard for evidence fields (raw non-null & typed null)\n",
        "    empty_str_arr = F.expr(\"array()\").cast(ArrayType(StringType()))\n",
        "    bad_any = F.lit(False)\n",
        "    cast_reasons = empty_str_arr\n",
        "    for p, raw_col in evidence_map.items():\n",
        "        cond = F.col(raw_col).isNotNull() & F.col(p).isNull()\n",
        "        bad_any = bad_any | cond\n",
        "        cast_reasons = F.concat(cast_reasons,\n",
        "                                F.when(cond, F.array(F.lit(f\"CAST_FAIL:{p}\"))).otherwise(empty_str_arr))\n",
        "    df_typed = (df_typed\n",
        "                .withColumn(\"_bad_cast_any\", bad_any)\n",
        "                .withColumn(\"_cast_fail_reasons\", F.array_distinct(cast_reasons)))\n",
        "\n",
        "    # 6) Explode arrays & project final columns\n",
        "    df_exp = explode_paths(df_typed, metadata.get(\"explode\"), normalized_map)\n",
        "\n",
        "    select_items = metadata.get(\"select\") or [{\"expr\": f.name, \"as\": f.name} for f in target_schema.fields]\n",
        "    proj_cols = [F.expr(item[\"expr\"]).alias(item.get(\"as\", item[\"expr\"])) for item in select_items]\n",
        "    df_out = df_exp.select(*proj_cols,\n",
        "                           \"_validation_errors\", \"_has_validation_errors\",\n",
        "                           \"_bad_cast_any\", \"_cast_fail_reasons\",\n",
        "                           *[F.col(c) for c in evidence_map.values()])\n",
        "    df_out = add_audit_cols(df_out, config)\n",
        "\n",
        "    # 7) Final split & reasons\n",
        "    is_corrupt = F.col(\"_has_validation_errors\") | F.col(\"_bad_cast_any\")\n",
        "    reasons = F.array_distinct(F.concat(\n",
        "        F.coalesce(F.col(\"_validation_errors\"), empty_str_arr),\n",
        "        F.coalesce(F.col(\"_cast_fail_reasons\"), empty_str_arr)\n",
        "    ))\n",
        "\n",
        "    corrupt_df = (df_out.where(is_corrupt)\n",
        "                  .withColumn(\"_corrupt_reasons\", reasons)\n",
        "                  .drop(\"_validation_errors\", \"_has_validation_errors\", \"_bad_cast_any\", \"_cast_fail_reasons\"))\n",
        "\n",
        "    # rename evidence columns to readable raw_<path>\n",
        "    for p, raw_col in evidence_map.items():\n",
        "        corrupt_df = corrupt_df.withColumn(f\"raw_{_sanitize(p)}\", F.col(raw_col)).drop(raw_col)\n",
        "\n",
        "    valid_df = (df_out.where(~is_corrupt)\n",
        "                .drop(\"_validation_errors\", \"_has_validation_errors\", \"_bad_cast_any\", \"_cast_fail_reasons\",\n",
        "                      *evidence_map.values()))\n",
        "\n",
        "    # 8) Writes\n",
        "    writer_fmt = config.get(\"write_format\", \"parquet\")\n",
        "    partitions = metadata.get(\"partitions\") or config.get(\"partitions\") or [\"source\",\"year\",\"month\",\"day\"]\n",
        "    out_struct  = config[\"xml_structured_path\"].format(**config)\n",
        "    out_corrupt = config[\"corrupt_records_path\"].format(**config)\n",
        "\n",
        "    if writer_fmt == \"delta\":\n",
        "        (valid_df.write.format(\"delta\").mode(\"append\").partitionBy(*partitions).save(out_struct))\n",
        "        if not corrupt_df.rdd.isEmpty():\n",
        "            (corrupt_df.write.format(\"delta\").mode(\"append\").partitionBy(*partitions).save(out_corrupt))\n",
        "    else:\n",
        "        (valid_df.write.mode(\"append\").partitionBy(*partitions).parquet(out_struct))\n",
        "        if not corrupt_df.rdd.isEmpty():\n",
        "            (corrupt_df.write.mode(\"append\").partitionBy(*partitions).parquet(out_corrupt))\n",
        "\n",
        "    # Preview\n",
        "    print(\"=== VALID sample ===\")\n",
        "    valid_df.show(10, truncate=False)\n",
        "    print(\"=== CORRUPT sample ===\")\n",
        "    corrupt_df.show(10, truncate=False)\n",
        "\n",
        "    return valid_df, corrupt_df\n",
        "\n",
        "# =============================== Example usage ===============================\n",
        "\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"16\")\n",
        "\n",
        "CONFIG = {\n",
        "    \"container\": \"ds-india\",\n",
        "    \"storage_account\": \"demodsindia\",\n",
        "    \"source_name\": \"source_name\",\n",
        "    \"year\": \"2025\", \"month\": \"09\", \"day\": \"07\",\n",
        "    \"landing_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/sample.xml\",\n",
        "    \"xml_structured_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/bronze/history/xml_structured/\",\n",
        "    \"corrupt_records_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/bronze/corrupt_records/source={source_name}/\",\n",
        "    \"row_tag\": \"record\",\n",
        "    \"read_mode\": \"PERMISSIVE\",\n",
        "    \"write_format\": \"parquet\",\n",
        "    \"partitions\": [\"source\",\"year\",\"month\",\"day\"]\n",
        "}\n",
        "\n",
        "TARGET_SCHEMA = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"details\", StructType([\n",
        "        StructField(\"name\", StringType(), True),\n",
        "        StructField(\"department\", StringType(), True),\n",
        "        StructField(\"roles\", StructType([\n",
        "            StructField(\"role\", ArrayType(StringType()), True)\n",
        "        ]), True)\n",
        "    ]), True)\n",
        "])\n",
        "\n",
        "# Professional JSON metadata\n",
        "METADATA_JSON = \"\"\"\n",
        "{\n",
        "  \"version\": \"1.0\",\n",
        "  \"explode\": [\n",
        "    { \"path\": \"details.roles.role\", \"alias\": \"role\", \"explode_mode\": \"outer\" }\n",
        "  ],\n",
        "  \"select\": [\n",
        "    { \"source\": \"id\",                 \"alias\": \"id\" },\n",
        "    { \"source\": \"details.name\",       \"alias\": \"name\" },\n",
        "    { \"source\": \"details.department\", \"alias\": \"department\" },\n",
        "    { \"source\": \"role\",               \"alias\": \"role\" }\n",
        "  ],\n",
        "  \"evidence\": [\"id\"],\n",
        "  \"partitions\": [\"source\",\"year\",\"month\",\"day\"]\n",
        "}\n",
        "\"\"\"\n",
        "METADATA = load_metadata(METADATA_JSON)\n",
        "\n",
        "# Run\n",
        "valid_df, corrupt_df = process_xml(CONFIG, TARGET_SCHEMA, METADATA)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 11,
              "statement_ids": [
                11
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "11",
              "normalized_state": "finished",
              "queued_time": "2025-09-07T12:33:53.4591974Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-07T12:33:53.4621442Z",
              "execution_finish_time": "2025-09-07T12:33:57.6937196Z",
              "parent_msg_id": "39d50cf8-3d3c-4db9-9bd9-017191f5795c"
            },
            "text/plain": "StatementMeta(demospark, 11, 11, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== VALID sample ===\n+---+----------+----------+-----------+-----------+----+-----+---+--------------------------+---------------------------------------------------------------------+\n|id |name      |department|role       |source     |year|month|day|ingestion_timestamp       |file_path                                                            |\n+---+----------+----------+-----------+-----------+----+-----+---+--------------------------+---------------------------------------------------------------------+\n|1  |John Doe  |Sales     |Manager    |source_name|2025|09   |07 |2025-09-07 12:33:56.649778|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml|\n|1  |John Doe  |Sales     |Analyst    |source_name|2025|09   |07 |2025-09-07 12:33:56.649778|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml|\n|2  |Jane Smith|Marketing |Coordinator|source_name|2025|09   |07 |2025-09-07 12:33:56.649778|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml|\n+---+----------+----------+-----------+-----------+----+-----+---+--------------------------+---------------------------------------------------------------------+\n\n=== CORRUPT sample ===\n+----+----+----------+--------+-----------+----+-----+---+--------------------------+---------------------------------------------------------------------+----------------+------+\n|id  |name|department|role    |source     |year|month|day|ingestion_timestamp       |file_path                                                            |_corrupt_reasons|raw_id|\n+----+----+----------+--------+-----------+----+-----+---+--------------------------+---------------------------------------------------------------------+----------------+------+\n|NULL|Bob |IT        |Engineer|source_name|2025|09   |07 |2025-09-07 12:33:56.925191|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/sample.xml|[CAST_FAIL:id]  |abc   |\n+----+----+----------+--------+-----------+----+-----+---+--------------------------+---------------------------------------------------------------------+----------------+------+\n\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "# ========================= XML Bronze Framework (no XSD) =========================\n",
        "# Requires: com.databricks:spark-xml_2.12:0.18.0 (or compatible).\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from typing import Dict, List, Any\n",
        "import json\n",
        "\n",
        "# ---------------------------- Utilities -----------------------------------\n",
        "\n",
        "def to_stringy_schema(dt: DataType) -> DataType:\n",
        "    \"\"\"Shape-only schema: same structure as target, but leaves are strings (arrays preserved).\"\"\"\n",
        "    if isinstance(dt, StructType):\n",
        "        return StructType([StructField(f.name, to_stringy_schema(f.dataType), True) for f in dt.fields])\n",
        "    if isinstance(dt, ArrayType):\n",
        "        return ArrayType(to_stringy_schema(dt.elementType), True)\n",
        "    return StringType()\n",
        "\n",
        "def _sanitize(path: str) -> str:\n",
        "    \"\"\"Turn a dot path into a safe column name.\"\"\"\n",
        "    return path.replace(\".\", \"__\").replace(\"[\", \"_\").replace(\"]\", \"_\")\n",
        "\n",
        "def ensure_array(df, path: str, array_dt: ArrayType, out_col: str):\n",
        "    \"\"\"\n",
        "    Normalize 'path' to ArrayType(array_dt.elementType) without scalar fallback.\n",
        "    Avoids CASE branches that force Spark to type-check array->struct casts.\n",
        "    \"\"\"\n",
        "    col_ref = F.col(path)\n",
        "    empty_arr = F.expr(\"array()\").cast(array_dt)\n",
        "    norm = F.coalesce(col_ref.cast(array_dt), empty_arr)\n",
        "    return df.withColumn(out_col, norm)\n",
        "\n",
        "\n",
        "def exists_invalid_primitive(col_arr, elem_type: DataType):\n",
        "    \"\"\"True if any non-null array element fails cast to elem_type.\"\"\"\n",
        "    return F.exists(col_arr, lambda x: x.isNotNull() & x.cast(elem_type).isNull())\n",
        "\n",
        "def exists_invalid_struct_field(col_arr, field: StructField):\n",
        "    \"\"\"True if any struct element has a non-null child that fails cast to the child's type.\"\"\"\n",
        "    return F.exists(col_arr, lambda e: e[field.name].isNotNull() & e[field.name].cast(field.dataType).isNull())\n",
        "\n",
        "def get_dt(path: str, dt: DataType) -> DataType:\n",
        "    \"\"\"Walk a dot path on a StructType to return the nested DataType.\"\"\"\n",
        "    if path == \"\" or path is None:\n",
        "        return dt\n",
        "    if not isinstance(dt, StructType):\n",
        "        raise KeyError(f\"Path '{path}' not found in schema.\")\n",
        "    parts = path.split(\".\", 1)\n",
        "    head = parts[0]\n",
        "    tail = parts[1] if len(parts) > 1 else \"\"\n",
        "    child = next((f for f in dt.fields if f.name == head), None)\n",
        "    if child is None:\n",
        "        raise KeyError(f\"Field '{head}' not found under path '{path}'.\")\n",
        "    return get_dt(tail, child.dataType)\n",
        "\n",
        "def collect_validation_errors(df, target_schema: StructType, normalized_map: Dict[str, str]):\n",
        "    \"\"\"\n",
        "    Build an array<string> of validation error labels by comparing RAW strings to target types.\n",
        "    Uses normalized array columns when validating arrays (so it works even if source was scalar).\n",
        "    \"\"\"\n",
        "    errs = [F.when(F.col(\"_corrupt_record\").isNotNull(), F.lit(\"STRUCTURAL_CORRUPT\"))]\n",
        "\n",
        "    def walk(path: str, dt: DataType):\n",
        "        nonlocal errs\n",
        "        if isinstance(dt, StructType):\n",
        "            for f in dt.fields:\n",
        "                walk(f\"{path}.{f.name}\" if path else f.name, f.dataType)\n",
        "        elif isinstance(dt, ArrayType):\n",
        "            use_col = F.col(normalized_map.get(path, path))\n",
        "            elem = dt.elementType\n",
        "            if isinstance(elem, StructType):\n",
        "                for ch in elem.fields:\n",
        "                    errs.append(F.when(\n",
        "                        exists_invalid_struct_field(use_col, ch),\n",
        "                        F.lit(f\"INVALID_ARRAY_ELEM_FIELD_{ch.dataType.simpleString()}:{path}.{ch.name}\")\n",
        "                    ))\n",
        "            else:\n",
        "                errs.append(F.when(\n",
        "                    exists_invalid_primitive(use_col, elem),\n",
        "                    F.lit(f\"INVALID_ARRAY_ELEMENT_{elem.simpleString()}:{path}\")\n",
        "                ))\n",
        "        else:\n",
        "            raw_col = F.col(path)\n",
        "            casted  = raw_col.cast(dt)\n",
        "            errs.append(F.when(raw_col.isNotNull() & casted.isNull(),\n",
        "                               F.lit(f\"INVALID_{dt.simpleString()}:{path}\")))\n",
        "    walk(\"\", target_schema)\n",
        "\n",
        "    return F.array_remove(F.array(*[e for e in errs if e is not None]), None)\n",
        "\n",
        "def typed_projection(path: str, dt: DataType, normalized_map: Dict[str, str]):\n",
        "    \"\"\"Produce a typed Column (recursively cast to target schema).\"\"\"\n",
        "    if isinstance(dt, StructType):\n",
        "        return F.struct(*[typed_projection(f\"{path}.{f.name}\", f.dataType, normalized_map).alias(f.name)\n",
        "                          for f in dt.fields])\n",
        "    if isinstance(dt, ArrayType):\n",
        "        col_ref = F.col(normalized_map.get(path, path))\n",
        "        elem = dt.elementType\n",
        "        if isinstance(elem, StructType):\n",
        "            return F.transform(col_ref, lambda e:\n",
        "                               F.struct(*[(e[ch.name].cast(ch.dataType)).alias(ch.name) for ch in elem.fields]))\n",
        "        else:\n",
        "            return F.transform(col_ref, lambda x: x.cast(elem))\n",
        "    return F.col(path).cast(dt)\n",
        "\n",
        "def explode_paths(df, explode_plan: List[Dict[str, str]], normalized_map: Dict[str, str]):\n",
        "    \"\"\"Apply explode (outer/inner) for each configured array path in order.\"\"\"\n",
        "    for item in explode_plan or []:\n",
        "        p = item[\"path\"]\n",
        "        alias = item.get(\"alias\") or _sanitize(p)\n",
        "        mode  = (item.get(\"explode_mode\") or \"outer\").lower()\n",
        "        norm_col = normalized_map[p]  # must exist\n",
        "        if mode == \"inner\":\n",
        "            df = df.withColumn(alias, F.explode(F.col(norm_col)))\n",
        "        else:\n",
        "            df = df.withColumn(alias, F.explode_outer(F.col(norm_col)))\n",
        "    return df\n",
        "\n",
        "def add_audit_cols(df, config: Dict[str, Any]):\n",
        "    return (df\n",
        "            .withColumn(\"source\", F.lit(config.get(\"source_name\", \"unknown\")))\n",
        "            .withColumn(\"year\",   F.lit(config[\"year\"]))\n",
        "            .withColumn(\"month\",  F.lit(config[\"month\"]))\n",
        "            .withColumn(\"day\",    F.lit(config[\"day\"]))\n",
        "            .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
        "            .withColumn(\"file_path\", F.input_file_name()))\n",
        "\n",
        "def load_metadata(json_str_or_dict):\n",
        "    \"\"\"Accepts JSON string or dict; normalizes keys to a unified internal shape.\"\"\"\n",
        "    meta = json.loads(json_str_or_dict) if isinstance(json_str_or_dict, str) else dict(json_str_or_dict)\n",
        "\n",
        "    # normalize 'select' entries: allow either source/alias or expr/as\n",
        "    norm_select = []\n",
        "    for it in meta.get(\"select\", []):\n",
        "        expr  = it.get(\"source\") or it.get(\"expr\")\n",
        "        alias = it.get(\"alias\") or it.get(\"as\") or expr\n",
        "        if not expr:\n",
        "            raise ValueError(\"select item missing 'source'/'expr'\")\n",
        "        norm_select.append({\"expr\": expr, \"as\": alias})\n",
        "    meta[\"select\"] = norm_select\n",
        "\n",
        "    # normalize 'explode' entries\n",
        "    norm_explode = []\n",
        "    for it in meta.get(\"explode\", []):\n",
        "        path = it[\"path\"]\n",
        "        alias = it.get(\"alias\") or it.get(\"as\") or _sanitize(path)\n",
        "        norm_explode.append({\n",
        "            \"path\": path,\n",
        "            \"alias\": alias,\n",
        "            \"explode_mode\": it.get(\"explode_mode\", \"outer\"),\n",
        "            \"null_handling\": it.get(\"null_handling\", \"empty_array\")\n",
        "        })\n",
        "    meta[\"explode\"] = norm_explode\n",
        "\n",
        "    # raw evidence fields (optional)\n",
        "    evid = meta.get(\"evidence\", [])\n",
        "    meta[\"evidence\"] = evid if isinstance(evid, list) else []\n",
        "\n",
        "    # optional partitions override\n",
        "    parts = meta.get(\"partitions\")\n",
        "    if parts and isinstance(parts, list):\n",
        "        meta[\"partitions\"] = parts\n",
        "\n",
        "    meta[\"version\"] = meta.get(\"version\", \"1.0\")\n",
        "    return meta\n",
        "\n",
        "# ---------------------------- Main entry point ---------------------------------\n",
        "\n",
        "def process_xml(config: Dict[str, Any],\n",
        "                target_schema: StructType,\n",
        "                metadata: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Config:\n",
        "      - landing_path (templated), xml_structured_path, corrupt_records_path\n",
        "      - row_tag, read_mode ('PERMISSIVE'|'DROPMALFORMED'|'FAILFAST'), write_format ('parquet'|'delta')\n",
        "      - partitions (list), source_name, year, month, day\n",
        "\n",
        "    Metadata (JSON normalized via load_metadata):\n",
        "      - explode: [{ \"path\": \"<dot.path>\", \"alias\": \"<name>\", \"explode_mode\": \"outer|inner\" }]\n",
        "      - select:  [{ \"expr\": \"<col or expr>\", \"as\": \"<alias>\" }, ...]\n",
        "      - evidence: [\"<dot.path>\", ...]  # raw fields to keep & cast-check (e.g., [\"id\"])\n",
        "      - partitions: [\"source\",\"year\",\"month\",\"day\"]  # optional override\n",
        "    \"\"\"\n",
        "    # 1) Read raw as strings (shape-only schema)\n",
        "    raw_schema = to_stringy_schema(target_schema)\n",
        "    landing = config[\"landing_path\"].format(**config)\n",
        "    read_mode = config.get(\"read_mode\", \"PERMISSIVE\")\n",
        "\n",
        "    df_raw = (spark.read.format(\"com.databricks.spark.xml\")\n",
        "              .option(\"rowTag\", config[\"row_tag\"])\n",
        "              .option(\"mode\", read_mode)\n",
        "              .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
        "              .schema(raw_schema)\n",
        "              .load(landing))\n",
        "\n",
        "    if \"_corrupt_record\" not in df_raw.columns:\n",
        "        df_raw = df_raw.withColumn(\"_corrupt_record\", F.lit(None).cast(StringType()))\n",
        "\n",
        "    # 2) Normalize arrays to explode & capture evidence columns\n",
        "    normalized_map = {}  # path -> normalized col name\n",
        "    for item in (metadata.get(\"explode\") or []):\n",
        "        p = item[\"path\"]\n",
        "        arr_dt_target = get_dt(p, target_schema)\n",
        "        if not isinstance(arr_dt_target, ArrayType):\n",
        "            raise TypeError(f\"explode path '{p}' is not ArrayType in target schema.\")\n",
        "        out_col = f\"__norm__{_sanitize(p)}\"\n",
        "        df_raw = ensure_array(df_raw, p, ArrayType(to_stringy_schema(arr_dt_target.elementType), True), out_col)\n",
        "        normalized_map[p] = out_col\n",
        "\n",
        "    evidence_map = {}  # path -> raw col name\n",
        "    for p in (metadata.get(\"evidence\") or []):\n",
        "        ev_col = f\"__raw__{_sanitize(p)}\"\n",
        "        df_raw = df_raw.withColumn(ev_col, F.col(p))\n",
        "        evidence_map[p] = ev_col\n",
        "\n",
        "    # 3) Validation errors on RAW vs target types\n",
        "    errors_col = collect_validation_errors(df_raw, target_schema, normalized_map)\n",
        "    df_checked = (df_raw\n",
        "                  .withColumn(\"_validation_errors\", errors_col)\n",
        "                  .withColumn(\"_has_validation_errors\", F.size(errors_col) > 0))\n",
        "\n",
        "    # 4) Typed projection (from RAW + normalized arrays)\n",
        "    typed_cols = [typed_projection(f.name, f.dataType, normalized_map).alias(f.name)\n",
        "                  for f in target_schema.fields]\n",
        "    keep_cols = list(normalized_map.values()) + list(evidence_map.values()) + [\"_validation_errors\", \"_has_validation_errors\"]\n",
        "    df_typed = df_checked.select(*typed_cols, *[F.col(c) for c in keep_cols])\n",
        "\n",
        "    # 5) Generic cast-failure guard for evidence fields (raw non-null & typed null)\n",
        "    empty_str_arr = F.expr(\"array()\").cast(ArrayType(StringType()))\n",
        "    bad_any = F.lit(False)\n",
        "    cast_reasons = empty_str_arr\n",
        "    for p, raw_col in evidence_map.items():\n",
        "        cond = F.col(raw_col).isNotNull() & F.col(p).isNull()\n",
        "        bad_any = bad_any | cond\n",
        "        cast_reasons = F.concat(cast_reasons,\n",
        "                                F.when(cond, F.array(F.lit(f\"CAST_FAIL:{p}\"))).otherwise(empty_str_arr))\n",
        "    df_typed = (df_typed\n",
        "                .withColumn(\"_bad_cast_any\", bad_any)\n",
        "                .withColumn(\"_cast_fail_reasons\", F.array_distinct(cast_reasons)))\n",
        "\n",
        "    # 6) Explode arrays & project final columns\n",
        "    df_exp = explode_paths(df_typed, metadata.get(\"explode\"), normalized_map)\n",
        "\n",
        "    select_items = metadata.get(\"select\") or [{\"expr\": f.name, \"as\": f.name} for f in target_schema.fields]\n",
        "    proj_cols = [F.expr(item[\"expr\"]).alias(item.get(\"as\", item[\"expr\"])) for item in select_items]\n",
        "    df_out = df_exp.select(*proj_cols,\n",
        "                           \"_validation_errors\", \"_has_validation_errors\",\n",
        "                           \"_bad_cast_any\", \"_cast_fail_reasons\",\n",
        "                           *[F.col(c) for c in evidence_map.values()])\n",
        "    df_out = add_audit_cols(df_out, config)\n",
        "\n",
        "    # 7) Final split & reasons\n",
        "    is_corrupt = F.col(\"_has_validation_errors\") | F.col(\"_bad_cast_any\")\n",
        "    reasons = F.array_distinct(F.concat(\n",
        "        F.coalesce(F.col(\"_validation_errors\"), empty_str_arr),\n",
        "        F.coalesce(F.col(\"_cast_fail_reasons\"), empty_str_arr)\n",
        "    ))\n",
        "\n",
        "    corrupt_df = (df_out.where(is_corrupt)\n",
        "                  .withColumn(\"_corrupt_reasons\", reasons)\n",
        "                  .drop(\"_validation_errors\", \"_has_validation_errors\", \"_bad_cast_any\", \"_cast_fail_reasons\"))\n",
        "\n",
        "    # rename evidence columns to readable raw_<path>\n",
        "    for p, raw_col in evidence_map.items():\n",
        "        corrupt_df = corrupt_df.withColumn(f\"raw_{_sanitize(p)}\", F.col(raw_col)).drop(raw_col)\n",
        "\n",
        "    valid_df = (df_out.where(~is_corrupt)\n",
        "                .drop(\"_validation_errors\", \"_has_validation_errors\", \"_bad_cast_any\", \"_cast_fail_reasons\",\n",
        "                      *evidence_map.values()))\n",
        "\n",
        "    # 8) Writes\n",
        "    writer_fmt = config.get(\"write_format\", \"parquet\")\n",
        "    partitions = metadata.get(\"partitions\") or config.get(\"partitions\") or [\"source\",\"year\",\"month\",\"day\"]\n",
        "    out_struct  = config[\"xml_structured_path\"].format(**config)\n",
        "    out_corrupt = config[\"corrupt_records_path\"].format(**config)\n",
        "\n",
        "    if writer_fmt == \"delta\":\n",
        "        (valid_df.write.format(\"delta\").mode(\"append\").partitionBy(*partitions).save(out_struct))\n",
        "        if not corrupt_df.rdd.isEmpty():\n",
        "            (corrupt_df.write.format(\"delta\").mode(\"append\").partitionBy(*partitions).save(out_corrupt))\n",
        "    else:\n",
        "        (valid_df.write.mode(\"append\").partitionBy(*partitions).parquet(out_struct))\n",
        "        if not corrupt_df.rdd.isEmpty():\n",
        "            (corrupt_df.write.mode(\"append\").partitionBy(*partitions).parquet(out_corrupt))\n",
        "\n",
        "    # Preview\n",
        "    print(\"=== VALID sample ===\")\n",
        "    valid_df.show(10, truncate=False)\n",
        "    print(\"=== CORRUPT sample ===\")\n",
        "    corrupt_df.show(10, truncate=False)\n",
        "\n",
        "    return valid_df, corrupt_df\n",
        "\n",
        "# =============================== Example usage ===============================\n",
        "\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"16\")\n",
        "\n",
        "CONFIG = {\n",
        "    \"container\": \"ds-india\",\n",
        "    \"storage_account\": \"demodsindia\",\n",
        "    \"source_name\": \"source_name\",\n",
        "    \"year\": \"2025\", \"month\": \"09\", \"day\": \"07\",\n",
        "    \"landing_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/sample.xml\",\n",
        "    \"xml_structured_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/bronze/history/xml_structured/\",\n",
        "    \"corrupt_records_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/bronze/corrupt_records/source={source_name}/\",\n",
        "    \"row_tag\": \"record\",\n",
        "    \"read_mode\": \"PERMISSIVE\",\n",
        "    \"write_format\": \"parquet\",\n",
        "    \"partitions\": [\"source\",\"year\",\"month\",\"day\"]\n",
        "}\n",
        "\n",
        "TARGET_SCHEMA = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"details\", StructType([\n",
        "        StructField(\"name\", StringType(), True),\n",
        "        StructField(\"department\", StringType(), True),\n",
        "        StructField(\"roles\", StructType([\n",
        "            StructField(\"role\", ArrayType(StringType()), True)\n",
        "        ]), True)\n",
        "    ]), True)\n",
        "])\n",
        "\n",
        "# Professional JSON metadata\n",
        "METADATA_JSON = \"\"\"\n",
        "{\n",
        "  \"version\": \"1.0\",\n",
        "  \"explode\": [\n",
        "    { \"path\": \"details.roles.role\", \"alias\": \"role\", \"explode_mode\": \"outer\" }\n",
        "  ],\n",
        "  \"select\": [\n",
        "    { \"source\": \"id\",                 \"alias\": \"id\" },\n",
        "    { \"source\": \"details.name\",       \"alias\": \"name\" },\n",
        "    { \"source\": \"details.department\", \"alias\": \"department\" },\n",
        "    { \"source\": \"role\",               \"alias\": \"role\" }\n",
        "  ],\n",
        "  \"evidence\": [\"id\"],\n",
        "  \"partitions\": [\"source\",\"year\",\"month\",\"day\"]\n",
        "}\n",
        "\"\"\"\n",
        "METADATA = load_metadata(METADATA_JSON)\n",
        "\n",
        "# Run\n",
        "valid_df, corrupt_df = process_xml(CONFIG, TARGET_SCHEMA, METADATA)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 12,
              "statement_ids": [
                12
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "11",
              "normalized_state": "finished",
              "queued_time": "2025-09-07T12:34:06.3124468Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-07T12:34:06.3152098Z",
              "execution_finish_time": "2025-09-07T12:34:06.9818821Z",
              "parent_msg_id": "8742a6e4-06d6-4d4c-a8db-dd70ee1608ec"
            },
            "text/plain": "StatementMeta(demospark, 11, 12, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Path 'Taxes.Tax' not found in schema.\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[25], line 151\u001b[0m\n\u001b[1;32m    144\u001b[0m METADATA_INVOICES \u001b[38;5;241m=\u001b[39m load_metadata(METADATA_JSON_INVOICES)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Small tip: for attributes/text nodes, spark-xml defaults usually match our schema,\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# but if you customized the reader, ensure attribute/value options are:\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m#   .option(\"attributePrefix\", \"_\").option(\"valueTag\", \"_VALUE\")\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# In my framework, you can add them in the read() call if needed.\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m valid_df, corrupt_df \u001b[38;5;241m=\u001b[39m process_xml(CONFIG_INVOICES, TARGET_SCHEMA_INVOICES, METADATA_INVOICES)\n",
            "Cell \u001b[0;32mIn[23], line 203\u001b[0m, in \u001b[0;36mprocess_xml\u001b[0;34m(config, target_schema, metadata)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m (metadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m []):\n\u001b[1;32m    202\u001b[0m     p \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 203\u001b[0m     arr_dt_target \u001b[38;5;241m=\u001b[39m get_dt(p, target_schema)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr_dt_target, ArrayType):\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplode path \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not ArrayType in target schema.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[23], line 54\u001b[0m, in \u001b[0;36mget_dt\u001b[0;34m(path, dt)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mField \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhead\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found under path \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_dt(tail, child\u001b[38;5;241m.\u001b[39mdataType)\n",
            "Cell \u001b[0;32mIn[23], line 54\u001b[0m, in \u001b[0;36mget_dt\u001b[0;34m(path, dt)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mField \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhead\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found under path \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_dt(tail, child\u001b[38;5;241m.\u001b[39mdataType)\n",
            "Cell \u001b[0;32mIn[23], line 54\u001b[0m, in \u001b[0;36mget_dt\u001b[0;34m(path, dt)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mField \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhead\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found under path \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_dt(tail, child\u001b[38;5;241m.\u001b[39mdataType)\n",
            "Cell \u001b[0;32mIn[23], line 47\u001b[0m, in \u001b[0;36mget_dt\u001b[0;34m(path, dt)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dt\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dt, StructType):\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in schema.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m parts \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     49\u001b[0m head \u001b[38;5;241m=\u001b[39m parts[\u001b[38;5;241m0\u001b[39m]\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Path 'Taxes.Tax' not found in schema.\""
          ]
        }
      ],
      "metadata": {},
      "source": [
        "# ====================== Invoices XML: CONFIG, TARGET_SCHEMA, METADATA ======================\n",
        "\n",
        "# 1) CONFIG  point this at your file and set rowTag to \"Invoice\"\n",
        "CONFIG_INVOICES = {\n",
        "    \"container\": \"ds-india\",\n",
        "    \"storage_account\": \"demodsindia\",\n",
        "    \"source_name\": \"invoices_v1\",\n",
        "    \"year\": \"2025\", \"month\": \"09\", \"day\": \"07\",\n",
        "    # Update this path to where you saved the XML shown in your message:\n",
        "    \"landing_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/invoices.xml\",\n",
        "    \"xml_structured_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/bronze/history/xml_structured/\",\n",
        "    \"corrupt_records_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/bronze/corrupt_records/source={source_name}/\",\n",
        "    \"row_tag\": \"Invoice\",\n",
        "    \"read_mode\": \"PERMISSIVE\",\n",
        "    \"write_format\": \"parquet\",\n",
        "    \"partitions\": [\"source\",\"year\",\"month\",\"day\"]\n",
        "}\n",
        "\n",
        "# 2) TARGET_SCHEMA  typed, with arrays/structs and XML attributes (_*) + text nodes (_VALUE)\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "TARGET_SCHEMA_INVOICES = StructType([\n",
        "    StructField(\"Header\", StructType([\n",
        "        StructField(\"InvoiceNumber\", StringType(), True),\n",
        "        StructField(\"InvoiceDate\", DateType(), True),\n",
        "        StructField(\"Buyer\", StructType([\n",
        "            StructField(\"Name\", StringType(), True),\n",
        "            StructField(\"Address\", StructType([\n",
        "                StructField(\"Line1\", StringType(), True),\n",
        "                StructField(\"City\", StringType(), True),\n",
        "                StructField(\"State\", StringType(), True),\n",
        "                StructField(\"Postcode\", StringType(), True),\n",
        "                StructField(\"Country\", StringType(), True)\n",
        "            ]), True)\n",
        "        ]), True)\n",
        "    ]), True),\n",
        "\n",
        "    StructField(\"Body\", StructType([\n",
        "        StructField(\"Items\", StructType([\n",
        "            StructField(\"LineItem\", ArrayType(StructType([\n",
        "                StructField(\"_id\", StringType(), True),                 # <LineItem id=\"...\">\n",
        "                StructField(\"ItemId\", StringType(), True),\n",
        "                StructField(\"Description\", StringType(), True),\n",
        "                StructField(\"Quantity\", IntegerType(), True),\n",
        "                StructField(\"Price\", DecimalType(18, 2), True),\n",
        "\n",
        "                StructField(\"Meta\", StructType([\n",
        "                    StructField(\"CreatedAt\", TimestampType(), True),\n",
        "                    StructField(\"UpdatedAt\", TimestampType(), True),\n",
        "                    StructField(\"Flags\", StructType([\n",
        "                        StructField(\"Flag\", ArrayType(StructType([      # <Flag name=\"...\" value=\"...\"/>\n",
        "                            StructField(\"_name\", StringType(), True),\n",
        "                            StructField(\"_value\", BooleanType(), True)   # cast \"true\"/\"false\" to boolean\n",
        "                        ])), True)\n",
        "                    ]), True)\n",
        "                ]), True),\n",
        "\n",
        "                StructField(\"Taxes\", StructType([\n",
        "                    StructField(\"Tax\", ArrayType(StructType([\n",
        "                        StructField(\"Code\", StringType(), True),\n",
        "                        StructField(\"Amount\", DecimalType(18, 3), True),           # 1.999, 0.500, 4.995\n",
        "                        StructField(\"TaxBreakup\", StructType([\n",
        "                            StructField(\"Break\", ArrayType(StructType([\n",
        "                                StructField(\"Type\", StringType(), True),           # \"percentage\"/\"fixed\"\n",
        "                                StructField(\"Rate\", DecimalType(18, 3), True)      # 10, 0.5, etc.\n",
        "                            ])), True)\n",
        "                        ]), True)\n",
        "                    ])), True)\n",
        "                ]), True),\n",
        "\n",
        "                StructField(\"Attributes\", StructType([\n",
        "                    StructField(\"Attribute\", ArrayType(StructType([               # <Attribute name=\"\" code=\"\">VALUE</Attribute>\n",
        "                        StructField(\"_name\", StringType(), True),\n",
        "                        StructField(\"_code\", StringType(), True),\n",
        "                        StructField(\"_VALUE\", StringType(), True)                 # element text\n",
        "                    ])), True)\n",
        "                ]), True),\n",
        "\n",
        "                StructField(\"Packaging\", StructType([\n",
        "                    StructField(\"Boxes\", StructType([\n",
        "                        StructField(\"Box\", ArrayType(StructType([\n",
        "                            StructField(\"_id\", StringType(), True),\n",
        "                            StructField(\"Weight\", DecimalType(18, 2), True),\n",
        "                            StructField(\"Dimensions\", StructType([\n",
        "                                StructField(\"Length\", IntegerType(), True),\n",
        "                                StructField(\"Width\", IntegerType(), True),\n",
        "                                StructField(\"Height\", IntegerType(), True)\n",
        "                            ]), True),\n",
        "                            StructField(\"Labels\", StructType([\n",
        "                                StructField(\"Label\", ArrayType(StringType()), True)\n",
        "                            ]), True)\n",
        "                        ])), True)\n",
        "                    ]), True)\n",
        "                ]), True),\n",
        "\n",
        "                StructField(\"Serials\", StructType([\n",
        "                    StructField(\"Serial\", ArrayType(StringType()), True)\n",
        "                ]), True)\n",
        "            ]), True))\n",
        "        ]), True)\n",
        "    ]), True)\n",
        "])\n",
        "\n",
        "# 3) METADATA (JSON)  explode line items + taxes; select key columns\n",
        "#    (Exploding more arrays can produce a cross-product; this keeps it tidy.)\n",
        "METADATA_JSON_INVOICES = \"\"\"\n",
        "{\n",
        "  \"version\": \"1.0\",\n",
        "  \"explode\": [\n",
        "    { \"path\": \"Body.Items.LineItem\",            \"alias\": \"li\",  \"explode_mode\": \"outer\" },\n",
        "    { \"path\": \"Body.Items.LineItem.Taxes.Tax\",  \"alias\": \"tax\", \"explode_mode\": \"outer\" }\n",
        "  ],\n",
        "  \"select\": [\n",
        "    { \"source\": \"Header.InvoiceNumber\",          \"alias\": \"invoice_number\" },\n",
        "    { \"source\": \"Header.InvoiceDate\",            \"alias\": \"invoice_date\"   },\n",
        "    { \"source\": \"Header.Buyer.Name\",             \"alias\": \"buyer_name\"     },\n",
        "    { \"source\": \"Header.Buyer.Address.Line1\",    \"alias\": \"addr_line1\"     },\n",
        "    { \"source\": \"Header.Buyer.Address.City\",     \"alias\": \"addr_city\"      },\n",
        "    { \"source\": \"Header.Buyer.Address.State\",    \"alias\": \"addr_state\"     },\n",
        "    { \"source\": \"Header.Buyer.Address.Postcode\", \"alias\": \"addr_postcode\"  },\n",
        "    { \"source\": \"Header.Buyer.Address.Country\",  \"alias\": \"addr_country\"   },\n",
        "\n",
        "    { \"source\": \"li._id\",                        \"alias\": \"line_id\"        },\n",
        "    { \"source\": \"li.ItemId\",                     \"alias\": \"item_id\"        },\n",
        "    { \"source\": \"li.Description\",                \"alias\": \"description\"    },\n",
        "    { \"source\": \"li.Quantity\",                   \"alias\": \"qty\"            },\n",
        "    { \"source\": \"li.Price\",                      \"alias\": \"price\"          },\n",
        "\n",
        "    { \"source\": \"tax.Code\",                      \"alias\": \"tax_code\"       },\n",
        "    { \"source\": \"tax.Amount\",                    \"alias\": \"tax_amount\"     },\n",
        "\n",
        "    { \"source\": \"size(li.Serials.Serial)\",       \"alias\": \"serial_count\"   },\n",
        "    { \"source\": \"size(li.Packaging.Boxes.Box)\",  \"alias\": \"box_count\"      }\n",
        "  ],\n",
        "  \"evidence\": [\"Header.InvoiceNumber\"],\n",
        "  \"partitions\": [\"source\",\"year\",\"month\",\"day\"]\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# If you used my previous framework code as-is, it already normalizes ALL arrays from the target schema,\n",
        "# and validates/casts according to types above (including attributes & text nodes).\n",
        "\n",
        "# 4) Run with your existing framework helpers:\n",
        "METADATA_INVOICES = load_metadata(METADATA_JSON_INVOICES)\n",
        "\n",
        "# Small tip: for attributes/text nodes, spark-xml defaults usually match our schema,\n",
        "# but if you customized the reader, ensure attribute/value options are:\n",
        "#   .option(\"attributePrefix\", \"_\").option(\"valueTag\", \"_VALUE\")\n",
        "# In my framework, you can add them in the read() call if needed.\n",
        "\n",
        "valid_df, corrupt_df = process_xml(CONFIG_INVOICES, TARGET_SCHEMA_INVOICES, METADATA_INVOICES)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "demospark",
              "statement_id": 14,
              "statement_ids": [
                14
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "11",
              "normalized_state": "finished",
              "queued_time": "2025-09-07T12:43:30.5182148Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-07T12:43:30.5210557Z",
              "execution_finish_time": "2025-09-07T12:43:34.7161276Z",
              "parent_msg_id": "13bb4ef3-12bc-4d60-a884-7cd8db66de67"
            },
            "text/plain": "StatementMeta(demospark, 11, 14, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== VALID sample ===\n+--------------+------------+------------+-------------+---------+----------+-------------+------------+--------+---------+------------+---+-----+--------+----------+------------+---------+-----------+----+-----+---+--------------------------+-----------------------------------------------------------------------+\n|invoice_number|invoice_date|buyer_name  |addr_line1   |addr_city|addr_state|addr_postcode|addr_country|line_id |item_id  |description |qty|price|tax_code|tax_amount|serial_count|box_count|source     |year|month|day|ingestion_timestamp       |file_path                                                              |\n+--------------+------------+------------+-------------+---------+----------+-------------+------------+--------+---------+------------+---+-----+--------+----------+------------+---------+-----------+----+-----+---+--------------------------+-----------------------------------------------------------------------+\n|INV-1001      |2025-08-30  |Acme Pty Ltd|123 George St|Sydney   |NSW       |2000         |AU          |LI-001-A|SKU-100-A|Blue Widget |3  |19.99|GST     |1.999     |2           |2        |invoices_v1|2025|09   |07 |2025-09-07 12:43:32.571069|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/invoices.xml|\n|INV-1001      |2025-08-30  |Acme Pty Ltd|123 George St|Sydney   |NSW       |2000         |AU          |LI-001-A|SKU-100-A|Blue Widget |3  |19.99|ENV     |0.500     |2           |2        |invoices_v1|2025|09   |07 |2025-09-07 12:43:32.571069|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/invoices.xml|\n|INV-1001      |2025-08-30  |Acme Pty Ltd|123 George St|Sydney   |NSW       |2000         |AU          |LI-002-A|SKU-200-A|Green Widget|1  |49.50|GST     |4.95      |-1          |-1       |invoices_v1|2025|09   |07 |2025-09-07 12:43:32.571069|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/invoices.xml|\n|INV-1002      |2025-08-31  |Globex AU   |77 Market Rd |Melbourne|VIC       |3000         |AU          |LI-003-B|SKU-300-B|Red Widget  |5  |9.99 |GST     |4.995     |-1          |1        |invoices_v1|2025|09   |07 |2025-09-07 12:43:32.571069|abfss://ds-india@demodsindia.dfs.core.windows.net/SEMIDATA/invoices.xml|\n+--------------+------------+------------+-------------+---------+----------+-------------+------------+--------+---------+------------+---+-----+--------+----------+------------+---------+-----------+----+-----+---+--------------------------+-----------------------------------------------------------------------+\n\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "1d2e2246-07e0-43b7-8ff9-a13e5ebe6a93",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 1d2e2246-07e0-43b7-8ff9-a13e5ebe6a93)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CORRUPT sample ===\n+--------------+------------+----------+----------+---------+----------+-------------+------------+-------+-------+-----------+---+-----+--------+----------+------------+---------+------+----+-----+---+-------------------+---------+----------------+-------------------------+\n|invoice_number|invoice_date|buyer_name|addr_line1|addr_city|addr_state|addr_postcode|addr_country|line_id|item_id|description|qty|price|tax_code|tax_amount|serial_count|box_count|source|year|month|day|ingestion_timestamp|file_path|_corrupt_reasons|raw_Header__InvoiceNumber|\n+--------------+------------+----------+----------+---------+----------+-------------+------------+-------+-------+-----------+---+-----+--------+----------+------------+---------+------+----+-----+---+-------------------+---------+----------------+-------------------------+\n+--------------+------------+----------+----------+---------+----------+-------------+------------+-------+-------+-----------+---+-----+--------+----------+------------+---------+------+----+-----+---+-------------------+---------+----------------+-------------------------+\n\n"
          ]
        }
      ],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# ========================= XML Bronze Framework (no XSD) =========================\n",
        "# Requires cluster/pool with: com.databricks:spark-xml_2.12:0.18.0 (or compatible)\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from typing import Dict, List, Any, Tuple\n",
        "import json\n",
        "\n",
        "# ---------------------------- Utilities -----------------------------------\n",
        "\n",
        "def to_stringy_schema(dt: DataType) -> DataType:\n",
        "    \"\"\"Same shape as target, but every leaf is StringType (arrays preserved).\"\"\"\n",
        "    if isinstance(dt, StructType):\n",
        "        return StructType([StructField(f.name, to_stringy_schema(f.dataType), True) for f in dt.fields])\n",
        "    if isinstance(dt, ArrayType):\n",
        "        return ArrayType(to_stringy_schema(dt.elementType), True)\n",
        "    return StringType()\n",
        "\n",
        "def _sanitize(path: str) -> str:\n",
        "    return path.replace(\".\", \"__\").replace(\"[\", \"_\").replace(\"]\", \"_\")\n",
        "\n",
        "def ensure_array(df, path: str, array_dt: ArrayType, out_col: str):\n",
        "    \"\"\"\n",
        "    Branch-free normalization to ArrayType(element). Assumes source already array-shaped\n",
        "    (per reader schema). Avoids CASE branches that trigger type-analysis failures.\n",
        "    \"\"\"\n",
        "    empty_arr = F.expr(\"array()\").cast(array_dt)\n",
        "    norm = F.coalesce(F.col(path).cast(array_dt), empty_arr)\n",
        "    return df.withColumn(out_col, norm)\n",
        "\n",
        "def exists_invalid_primitive(col_arr, elem_type: DataType):\n",
        "    return F.exists(col_arr, lambda x: x.isNotNull() & x.cast(elem_type).isNull())\n",
        "\n",
        "def exists_invalid_struct_field(col_arr, field: StructField):\n",
        "    return F.exists(col_arr, lambda e: e[field.name].isNotNull() & e[field.name].cast(field.dataType).isNull())\n",
        "\n",
        "def get_dt(path: str, dt: DataType) -> DataType:\n",
        "    \"\"\"\n",
        "    Resolve a dot path on StructType/ArrayType. If current node is ArrayType,\n",
        "    step into elementType transparently and continue.\n",
        "    \"\"\"\n",
        "    if not path:\n",
        "        return dt\n",
        "    if isinstance(dt, ArrayType):\n",
        "        return get_dt(path, dt.elementType)\n",
        "    if not isinstance(dt, StructType):\n",
        "        raise KeyError(f\"Path '{path}' not found in schema.\")\n",
        "    if \".\" in path:\n",
        "        head, tail = path.split(\".\", 1)\n",
        "    else:\n",
        "        head, tail = path, \"\"\n",
        "    child = next((f for f in dt.fields if f.name == head), None)\n",
        "    if child is None:\n",
        "        raise KeyError(f\"Field '{head}' not found under path '{path}'.\")\n",
        "    return get_dt(tail, child.dataType)\n",
        "\n",
        "def list_array_paths(dt: DataType, base=\"\") -> List[Tuple[str, ArrayType]]:\n",
        "    \"\"\"List ALL array paths in the schema.\"\"\"\n",
        "    paths = []\n",
        "    if isinstance(dt, StructType):\n",
        "        for f in dt.fields:\n",
        "            sub = f\"{base}.{f.name}\" if base else f.name\n",
        "            paths += list_array_paths(f.dataType, sub)\n",
        "    elif isinstance(dt, ArrayType):\n",
        "        paths.append((base, dt))\n",
        "        paths += list_array_paths(dt.elementType, base)\n",
        "    return paths\n",
        "\n",
        "def top_level_array_paths(schema: StructType) -> List[str]:\n",
        "    \"\"\"Return array paths that are NOT nested under another array path.\"\"\"\n",
        "    all_arrays = [p for p, _ in list_array_paths(schema)]\n",
        "    tops = []\n",
        "    for p in all_arrays:\n",
        "        is_nested = any(parent != p and p.startswith(parent + \".\") for parent in all_arrays)\n",
        "        if not is_nested:\n",
        "            tops.append(p)\n",
        "    return tops\n",
        "\n",
        "def collect_validation_errors(df, target_schema: StructType, normalized_map: Dict[str, str]):\n",
        "    \"\"\"\n",
        "    Build array<string> of validation errors by comparing RAW strings to target types.\n",
        "    Uses normalized_map for pre-normalized arrays; falls back to raw columns otherwise.\n",
        "    \"\"\"\n",
        "    errs = [F.when(F.col(\"_corrupt_record\").isNotNull(), F.lit(\"STRUCTURAL_CORRUPT\"))]\n",
        "\n",
        "    def walk(path: str, dt: DataType):\n",
        "        nonlocal errs\n",
        "        if isinstance(dt, StructType):\n",
        "            for f in dt.fields:\n",
        "                walk(f\"{path}.{f.name}\" if path else f.name, f.dataType)\n",
        "        elif isinstance(dt, ArrayType):\n",
        "            use_col = F.col(normalized_map.get(path, path))\n",
        "            elem = dt.elementType\n",
        "            if isinstance(elem, StructType):\n",
        "                for ch in elem.fields:\n",
        "                    errs.append(F.when(\n",
        "                        exists_invalid_struct_field(use_col, ch),\n",
        "                        F.lit(f\"INVALID_ARRAY_ELEM_FIELD_{ch.dataType.simpleString()}:{path}.{ch.name}\")\n",
        "                    ))\n",
        "            else:\n",
        "                errs.append(F.when(\n",
        "                    exists_invalid_primitive(use_col, elem),\n",
        "                    F.lit(f\"INVALID_ARRAY_ELEMENT_{elem.simpleString()}:{path}\")\n",
        "                ))\n",
        "        else:\n",
        "            raw_col = F.col(path)\n",
        "            casted  = raw_col.cast(dt)\n",
        "            errs.append(F.when(raw_col.isNotNull() & casted.isNull(),\n",
        "                               F.lit(f\"INVALID_{dt.simpleString()}:{path}\")))\n",
        "    walk(\"\", target_schema)\n",
        "\n",
        "    return F.array_remove(F.array(*[e for e in errs if e is not None]), None)\n",
        "\n",
        "def typed_projection(path: str, dt: DataType, normalized_map: Dict[str, str]):\n",
        "    \"\"\"\n",
        "    Produce a typed Column (recursively cast). Uses normalized arrays when available,\n",
        "    otherwise raw arrays from the reader schema (which are already array-shaped).\n",
        "    \"\"\"\n",
        "    if isinstance(dt, StructType):\n",
        "        return F.struct(*[typed_projection(f\"{path}.{f.name}\", f.dataType, normalized_map).alias(f.name)\n",
        "                          for f in dt.fields])\n",
        "    if isinstance(dt, ArrayType):\n",
        "        col_ref = F.col(normalized_map.get(path, path))\n",
        "        elem = dt.elementType\n",
        "        if isinstance(elem, StructType):\n",
        "            return F.transform(col_ref, lambda e:\n",
        "                               F.struct(*[(e[ch.name].cast(ch.dataType)).alias(ch.name) for ch in elem.fields]))\n",
        "        else:\n",
        "            return F.transform(col_ref, lambda x: x.cast(elem))\n",
        "    return F.col(path).cast(dt)\n",
        "\n",
        "def explode_paths(df, explode_plan, normalized_map):\n",
        "    \"\"\"\n",
        "    Explode each configured array in order.\n",
        "    - If base path was pre-normalized (top-level), explode the normalized column.\n",
        "    - If the path is nested under a previously exploded alias, rewrite to that alias.\n",
        "    \"\"\"\n",
        "    alias_map = {}  # base path -> alias column name\n",
        "    for item in (explode_plan or []):\n",
        "        base_path = item[\"path\"]                      # e.g. Body.Items.LineItem.Taxes.Tax\n",
        "        alias     = item.get(\"alias\") or base_path.replace(\".\", \"__\")\n",
        "        mode      = (item.get(\"explode_mode\") or \"outer\").lower()\n",
        "\n",
        "        # Rewrite to use prior aliases when inside an already-exploded one\n",
        "        effective = base_path\n",
        "        for parent_path, parent_alias in alias_map.items():\n",
        "            if effective.startswith(parent_path + \".\"):\n",
        "                effective = parent_alias + effective[len(parent_path):]\n",
        "\n",
        "        # Pick the column to explode\n",
        "        if base_path in normalized_map:\n",
        "            col_to_explode = F.col(normalized_map[base_path])\n",
        "        else:\n",
        "            col_to_explode = F.col(effective)\n",
        "\n",
        "        df = df.withColumn(alias,\n",
        "                           F.explode(col_to_explode) if mode == \"inner\" else F.explode_outer(col_to_explode))\n",
        "\n",
        "        alias_map[base_path] = alias\n",
        "    return df\n",
        "\n",
        "def add_audit_cols(df, config: Dict[str, Any]):\n",
        "    return (df\n",
        "            .withColumn(\"source\", F.lit(config.get(\"source_name\", \"unknown\")))\n",
        "            .withColumn(\"year\",   F.lit(config[\"year\"]))\n",
        "            .withColumn(\"month\",  F.lit(config[\"month\"]))\n",
        "            .withColumn(\"day\",    F.lit(config[\"day\"]))\n",
        "            .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
        "            .withColumn(\"file_path\", F.input_file_name()))\n",
        "\n",
        "def load_metadata(json_str_or_dict):\n",
        "    meta = json.loads(json_str_or_dict) if isinstance(json_str_or_dict, str) else dict(json_str_or_dict)\n",
        "    # select\n",
        "    norm_select = []\n",
        "    for it in meta.get(\"select\", []):\n",
        "        expr  = it.get(\"source\") or it.get(\"expr\")\n",
        "        alias = it.get(\"alias\") or it.get(\"as\") or expr\n",
        "        if not expr:\n",
        "            raise ValueError(\"select item missing 'source'/'expr'\")\n",
        "        norm_select.append({\"expr\": expr, \"as\": alias})\n",
        "    meta[\"select\"] = norm_select\n",
        "    # explode\n",
        "    norm_explode = []\n",
        "    for it in meta.get(\"explode\", []):\n",
        "        path = it[\"path\"]\n",
        "        alias = it.get(\"alias\") or it.get(\"as\") or _sanitize(path)\n",
        "        norm_explode.append({\"path\": path, \"alias\": alias, \"explode_mode\": it.get(\"explode_mode\", \"outer\")})\n",
        "    meta[\"explode\"] = norm_explode\n",
        "    # evidence\n",
        "    evid = meta.get(\"evidence\", [])\n",
        "    meta[\"evidence\"] = evid if isinstance(evid, list) else []\n",
        "    # partitions\n",
        "    if \"partitions\" in meta and isinstance(meta[\"partitions\"], list):\n",
        "        meta[\"partitions\"] = meta[\"partitions\"]\n",
        "    meta[\"version\"] = meta.get(\"version\", \"1.0\")\n",
        "    return meta\n",
        "\n",
        "# ---------------------------- Main processing ---------------------------------\n",
        "\n",
        "def process_xml(config: Dict[str, Any],\n",
        "                target_schema: StructType,\n",
        "                metadata: Dict[str, Any]):\n",
        "\n",
        "    raw_schema = to_stringy_schema(target_schema)\n",
        "    landing = config[\"landing_path\"].format(**config)\n",
        "    read_mode = config.get(\"read_mode\", \"PERMISSIVE\")\n",
        "\n",
        "    # Reader: be explicit about attributes/text conventions\n",
        "    df_raw = (spark.read.format(\"com.databricks.spark.xml\")\n",
        "              .option(\"rowTag\", config[\"row_tag\"])\n",
        "              .option(\"mode\", read_mode)\n",
        "              .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
        "              .option(\"attributePrefix\", \"_\")\n",
        "              .option(\"valueTag\", \"_VALUE\")\n",
        "              .schema(raw_schema)\n",
        "              .load(landing))\n",
        "\n",
        "    if \"_corrupt_record\" not in df_raw.columns:\n",
        "        df_raw = df_raw.withColumn(\"_corrupt_record\", F.lit(None).cast(StringType()))\n",
        "\n",
        "    # Pre-normalize ONLY top-level arrays to avoid array-of-array cast issues\n",
        "    normalized_map = {}\n",
        "    for p in top_level_array_paths(target_schema):           # e.g., \"Body.Items.LineItem\"\n",
        "        arr_dt = get_dt(p, target_schema)                    # ArrayType(...)\n",
        "        stringy_arr_dt = ArrayType(to_stringy_schema(arr_dt.elementType), True)\n",
        "        out_col = f\"__norm__{_sanitize(p)}\"\n",
        "        df_raw = ensure_array(df_raw, p, stringy_arr_dt, out_col)\n",
        "        normalized_map[p] = out_col\n",
        "\n",
        "    # Evidence (raw) fields to carry into corrupt output\n",
        "    evidence_map = {}\n",
        "    for p in (metadata.get(\"evidence\") or []):\n",
        "        ev_col = f\"__raw__{_sanitize(p)}\"\n",
        "        df_raw = df_raw.withColumn(ev_col, F.col(p))\n",
        "        evidence_map[p] = ev_col\n",
        "\n",
        "    # Validation on RAW vs target types\n",
        "    errors_col = collect_validation_errors(df_raw, target_schema, normalized_map)\n",
        "    df_checked = (df_raw\n",
        "                  .withColumn(\"_validation_errors\", errors_col)\n",
        "                  .withColumn(\"_has_validation_errors\", F.size(errors_col) > 0))\n",
        "\n",
        "    # Typed projection (from RAW + normalized arrays)\n",
        "    typed_cols = [typed_projection(f.name, f.dataType, normalized_map).alias(f.name)\n",
        "                  for f in target_schema.fields]\n",
        "    keep_cols = list(normalized_map.values()) + list(evidence_map.values()) + [\"_validation_errors\", \"_has_validation_errors\"]\n",
        "    df_typed = df_checked.select(*typed_cols, *[F.col(c) for c in keep_cols])\n",
        "\n",
        "    # Explode per metadata (alias-aware)\n",
        "    df_exp = explode_paths(df_typed, metadata.get(\"explode\"), normalized_map)\n",
        "\n",
        "    # Final projection\n",
        "    select_items = metadata.get(\"select\") or [{\"expr\": f.name, \"as\": f.name} for f in target_schema.fields]\n",
        "    proj_cols = [F.expr(item[\"expr\"]).alias(item.get(\"as\", item[\"expr\"])) for item in select_items]\n",
        "    df_out = df_exp.select(*proj_cols, \"_validation_errors\", \"_has_validation_errors\",\n",
        "                           *[F.col(c) for c in evidence_map.values()])\n",
        "    df_out = add_audit_cols(df_out, config)\n",
        "\n",
        "    # Split valid vs corrupt\n",
        "    is_corrupt = F.col(\"_has_validation_errors\")\n",
        "    corrupt_df = (df_out.where(is_corrupt)\n",
        "                  .withColumn(\"_corrupt_reasons\",\n",
        "                              F.array_distinct(F.coalesce(F.col(\"_validation_errors\"), F.array())))\n",
        "                  .drop(\"_validation_errors\", \"_has_validation_errors\"))\n",
        "    for p, raw_col in evidence_map.items():\n",
        "        corrupt_df = corrupt_df.withColumn(f\"raw_{_sanitize(p)}\", F.col(raw_col)).drop(raw_col)\n",
        "\n",
        "    valid_df = df_out.where(~is_corrupt).drop(\"_validation_errors\", \"_has_validation_errors\", *evidence_map.values())\n",
        "\n",
        "    # Writes\n",
        "    writer_fmt = config.get(\"write_format\", \"parquet\")\n",
        "    partitions = metadata.get(\"partitions\") or config.get(\"partitions\") or [\"source\",\"year\",\"month\",\"day\"]\n",
        "    out_struct  = config[\"xml_structured_path\"].format(**config)\n",
        "    out_corrupt = config[\"corrupt_records_path\"].format(**config)\n",
        "\n",
        "    if writer_fmt == \"delta\":\n",
        "        (valid_df.write.format(\"delta\").mode(\"append\").partitionBy(*partitions).save(out_struct))\n",
        "        if not corrupt_df.rdd.isEmpty():\n",
        "            (corrupt_df.write.format(\"delta\").mode(\"append\").partitionBy(*partitions).save(out_corrupt))\n",
        "    else:\n",
        "        (valid_df.write.mode(\"append\").partitionBy(*partitions).parquet(out_struct))\n",
        "        if not corrupt_df.rdd.isEmpty():\n",
        "            (corrupt_df.write.mode(\"append\").partitionBy(*partitions).parquet(out_corrupt))\n",
        "\n",
        "    # Preview\n",
        "    print(\"=== VALID sample ===\")\n",
        "    valid_df.show(10, truncate=False)\n",
        "    display(valid_df)\n",
        "    print(\"=== CORRUPT sample ===\")\n",
        "    corrupt_df.show(10, truncate=False)\n",
        "\n",
        "    return valid_df, corrupt_df\n",
        "\n",
        "# ====================== Invoices XML: CONFIG, TARGET_SCHEMA, METADATA ======================\n",
        "\n",
        "CONFIG_INVOICES = {\n",
        "    \"container\": \"ds-india\",\n",
        "    \"storage_account\": \"demodsindia\",\n",
        "    \"source_name\": \"invoices_v1\",\n",
        "    \"year\": \"2025\", \"month\": \"09\", \"day\": \"07\",\n",
        "    # Point to your XML file location:\n",
        "    \"landing_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/invoices.xml\",\n",
        "    \"xml_structured_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/bronze/history/xml_structured/\",\n",
        "    \"corrupt_records_path\": \"abfss://{container}@{storage_account}.dfs.core.windows.net/SEMIDATA/bronze/corrupt_records/source={source_name}/\",\n",
        "    \"row_tag\": \"Invoice\",\n",
        "    \"read_mode\": \"PERMISSIVE\",\n",
        "    \"write_format\": \"parquet\",\n",
        "    \"partitions\": [\"source\",\"year\",\"month\",\"day\"]\n",
        "}\n",
        "\n",
        "TARGET_SCHEMA_INVOICES = StructType([\n",
        "    StructField(\"Header\", StructType([\n",
        "        StructField(\"InvoiceNumber\", StringType(), True),\n",
        "        StructField(\"InvoiceDate\", DateType(), True),\n",
        "        StructField(\"Buyer\", StructType([\n",
        "            StructField(\"Name\", StringType(), True),\n",
        "            StructField(\"Address\", StructType([\n",
        "                StructField(\"Line1\", StringType(), True),\n",
        "                StructField(\"City\", StringType(), True),\n",
        "                StructField(\"State\", StringType(), True),\n",
        "                StructField(\"Postcode\", StringType(), True),\n",
        "                StructField(\"Country\", StringType(), True)\n",
        "            ]), True)\n",
        "        ]), True)\n",
        "    ]), True),\n",
        "\n",
        "    StructField(\"Body\", StructType([\n",
        "        StructField(\"Items\", StructType([\n",
        "            StructField(\"LineItem\", ArrayType(StructType([\n",
        "                StructField(\"_id\", StringType(), True),                 # <LineItem id=\"...\">\n",
        "                StructField(\"ItemId\", StringType(), True),\n",
        "                StructField(\"Description\", StringType(), True),\n",
        "                StructField(\"Quantity\", IntegerType(), True),\n",
        "                StructField(\"Price\", DecimalType(18, 2), True),\n",
        "\n",
        "                StructField(\"Meta\", StructType([\n",
        "                    StructField(\"CreatedAt\", TimestampType(), True),\n",
        "                    StructField(\"UpdatedAt\", TimestampType(), True),\n",
        "                    StructField(\"Flags\", StructType([\n",
        "                        StructField(\"Flag\", ArrayType(StructType([      # <Flag name=\"...\" value=\"...\"/>\n",
        "                            StructField(\"_name\", StringType(), True),\n",
        "                            StructField(\"_value\", BooleanType(), True)\n",
        "                        ])), True)\n",
        "                    ]), True)\n",
        "                ]), True),\n",
        "\n",
        "                StructField(\"Taxes\", StructType([\n",
        "                    StructField(\"Tax\", ArrayType(StructType([\n",
        "                        StructField(\"Code\", StringType(), True),\n",
        "                        StructField(\"Amount\", DecimalType(18, 3), True),\n",
        "                        StructField(\"TaxBreakup\", StructType([\n",
        "                            StructField(\"Break\", ArrayType(StructType([\n",
        "                                StructField(\"Type\", StringType(), True),\n",
        "                                StructField(\"Rate\", DecimalType(18, 3), True)\n",
        "                            ])), True)\n",
        "                        ]), True)\n",
        "                    ])), True)\n",
        "                ]), True),\n",
        "\n",
        "                StructField(\"Attributes\", StructType([\n",
        "                    StructField(\"Attribute\", ArrayType(StructType([\n",
        "                        StructField(\"_name\", StringType(), True),\n",
        "                        StructField(\"_code\", StringType(), True),\n",
        "                        StructField(\"_VALUE\", StringType(), True)\n",
        "                    ])), True)\n",
        "                ]), True),\n",
        "\n",
        "                StructField(\"Packaging\", StructType([\n",
        "                    StructField(\"Boxes\", StructType([\n",
        "                        StructField(\"Box\", ArrayType(StructType([\n",
        "                            StructField(\"_id\", StringType(), True),\n",
        "                            StructField(\"Weight\", DecimalType(18, 2), True),\n",
        "                            StructField(\"Dimensions\", StructType([\n",
        "                                StructField(\"Length\", IntegerType(), True),\n",
        "                                StructField(\"Width\", IntegerType(), True),\n",
        "                                StructField(\"Height\", IntegerType(), True)\n",
        "                            ]), True),\n",
        "                            StructField(\"Labels\", StructType([\n",
        "                                StructField(\"Label\", ArrayType(StringType()), True)\n",
        "                            ]), True)\n",
        "                        ])), True)\n",
        "                    ]), True)\n",
        "                ]), True),\n",
        "\n",
        "                StructField(\"Serials\", StructType([\n",
        "                    StructField(\"Serial\", ArrayType(StringType()), True)\n",
        "                ]), True)\n",
        "            ]), True))\n",
        "        ]), True)\n",
        "    ]), True)\n",
        "])\n",
        "\n",
        "METADATA_JSON_INVOICES = \"\"\"\n",
        "{\n",
        "  \"version\": \"1.0\",\n",
        "  \"explode\": [\n",
        "    { \"path\": \"Body.Items.LineItem\",            \"alias\": \"li\",  \"explode_mode\": \"outer\" },\n",
        "    { \"path\": \"Body.Items.LineItem.Taxes.Tax\",  \"alias\": \"tax\", \"explode_mode\": \"outer\" }\n",
        "  ],\n",
        "  \"select\": [\n",
        "    { \"source\": \"Header.InvoiceNumber\",          \"alias\": \"invoice_number\" },\n",
        "    { \"source\": \"Header.InvoiceDate\",            \"alias\": \"invoice_date\"   },\n",
        "    { \"source\": \"Header.Buyer.Name\",             \"alias\": \"buyer_name\"     },\n",
        "    { \"source\": \"Header.Buyer.Address.Line1\",    \"alias\": \"addr_line1\"     },\n",
        "    { \"source\": \"Header.Buyer.Address.City\",     \"alias\": \"addr_city\"      },\n",
        "    { \"source\": \"Header.Buyer.Address.State\",    \"alias\": \"addr_state\"     },\n",
        "    { \"source\": \"Header.Buyer.Address.Postcode\", \"alias\": \"addr_postcode\"  },\n",
        "    { \"source\": \"Header.Buyer.Address.Country\",  \"alias\": \"addr_country\"   },\n",
        "\n",
        "    { \"source\": \"li._id\",                        \"alias\": \"line_id\"        },\n",
        "    { \"source\": \"li.ItemId\",                     \"alias\": \"item_id\"        },\n",
        "    { \"source\": \"li.Description\",                \"alias\": \"description\"    },\n",
        "    { \"source\": \"li.Quantity\",                   \"alias\": \"qty\"            },\n",
        "    { \"source\": \"li.Price\",                      \"alias\": \"price\"          },\n",
        "\n",
        "    { \"source\": \"tax.Code\",                      \"alias\": \"tax_code\"       },\n",
        "    { \"source\": \"tax.Amount\",                    \"alias\": \"tax_amount\"     },\n",
        "\n",
        "    { \"source\": \"size(li.Serials.Serial)\",       \"alias\": \"serial_count\"   },\n",
        "    { \"source\": \"size(li.Packaging.Boxes.Box)\",  \"alias\": \"box_count\"      }\n",
        "  ],\n",
        "  \"evidence\": [\"Header.InvoiceNumber\"],\n",
        "  \"partitions\": [\"source\",\"year\",\"month\",\"day\"]\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "METADATA_INVOICES = load_metadata(METADATA_JSON_INVOICES)\n",
        "\n",
        "# (Optional) Spark tuning\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"16\")\n",
        "\n",
        "# Run\n",
        "valid_df, corrupt_df = process_xml(CONFIG_INVOICES, TARGET_SCHEMA_INVOICES, METADATA_INVOICES)\n",
        ""
      ]
    }
  ]
}